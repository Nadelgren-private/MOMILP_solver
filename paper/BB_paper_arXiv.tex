\documentclass[11.5pt]{article}
%\documentclass[smallextended]{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended,natbib]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
%\smartqed
\usepackage{amsmath,amssymb,amsfonts,amsthm} 
\usepackage[top=1in,bottom=1.4in,right=1in,left=1in]{geometry}
%\geometry{a4paper}
\usepackage[numbers,sort]{natbib}
%\usepackage[maxbibnames=99,style=alphabetic,backend=biber,sorting=ynt,sortcites,natbib=true]{biblatex}

%\usepackage{authblk}
\usepackage{fullpage}
%\usepackage{listings}
\usepackage{graphicx}
%\usepackage[colorlinks=true,citecolor=blue]{hyperref}
\usepackage{booktabs} 
\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{pdfpages}
\usepackage{cancel}
\usepackage{hyperref}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{array}
%\usepackage{relsize}
%\usepackage[numbers,sort&compress,sectionbib]{natbib}
%\usepackage{amsaddr}
\usepackage{subcaption}
%\usepackage[center]{caption}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage{tkz-graph}
\usepackage{tikz-3dplot}
\usetikzlibrary{arrows,positioning,automata}
\usepackage{mathrsfs}
\usepackage{ wasysym }
\usepackage{multicol}
\usepackage{longtable}
\usepackage{lscape}
\usepackage[normalem]{ulem}
\usepackage{color}

%\setlength{\captionmargin}{1pt}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\newtheorem{prop}{Proposition}
\newtheorem{obs}{Observation}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}

%\DeclareMathOperator{\maxde}{max\_ depth}
%\DeclareMathOperator{\minde}{min\_ depth}
%\DeclareMathOperator{\childtype}{child\_ type}
%\DeclareMathOperator{\subts}{subtree\_ size}
%\DeclareMathOperator{\idptl}{ideal\_ point\_ left}
%\DeclareMathOperator{\idptr}{ideal\_ point\_ right}
%%\DeclareMathOperator{\bcup}{\biggcup}
%\DeclareMathOperator{\proj}{Proj}

\newcommand{\bb}{BB}
\newcommand*\anotherif[2]{\State \textbf{if} #1 \textbf{then} #2}
\newcommand*\spacedif[3]{\State \hspace*{#1} \textbf{if} #2 \textbf{then} #3}
\newcommand*\anotherelse[1]{\State \textbf{else} #1}
\newcommand*\spacedelse[2]{\State \hspace*{#1} \textbf{else} #2}
\newcommand{\li}[2][1]{\ensuremath{\displaystyle{\lim_{#1 \rightarrow #2}}}}
\newcommand{\su}[2][1]{\ensuremath{\displaystyle{\sum_{#1}^{#2}}}}
%\renewcommand{\arraystretch}{1.1}
\newcommand*\Let[2]{\State #1 $\gets$ #2}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\T}{\mathbb{T}}
\renewcommand{\O}{\mathbb{O}}
\newcommand{\rgn}{\textsf{Rgn}}
\newcommand{\pnt}{\textsf{Pnt}}
\newcommand{\sgmt}{\textsf{Sgmt}}
\newcommand{\rt}{\textsf{Root}}
\newcommand{\lft}{\textsf{Left}}
\newcommand{\rght}{\textsf{Right}}
\newcommand{\maxd}{\maxde}
\newcommand{\mind}{\minde}
\newcommand{\ipl}{\idptl}
\newcommand{\ipr}{\idptr}
\newcommand{\ct}{\childtype}
%\newcommand{\biggcup}{\mathlarger{\mathlarger{\mathlarger{\cup}}}}
\newcommand{\lcup}{\cup} %{\mathlarger{\mathlarger{\cup}}}
\newcommand{\xbar}{\overline{x}}
\newcommand{\ybar}{\overline{y}}
\newcommand{\xp}{x^{\prime}}
\newcommand{\yp}{y^{\prime}}
\newcommand{\sts}{\subts}
\newcommand{\dom}{\succ}
\newcommand{\pdom}{\succ_p}
\renewcommand{\P}{\mathbb{P}}
\renewcommand{\xi}{f}
\newcommand{\U}{\mathcal{U}}
\renewcommand{\L}{\mathcal{L}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\OS}{\mathcal{OS}}
\renewcommand{\top}{}
\newcommand{\penta}{\scalebox{1.3}{\pentagon}}
\newcommand{\hexa}{\scalebox{1.3}{\hexagon}}
\newcommand{\bul}{\scalebox{.45}{\ensuremath{\bullet}}}
\newcommand*{\tran}{^{\mkern-1.5mu\mathsf{T}}}
\renewcommand{\top}{\tran}
\newcommand{\argmin}{\text{argmin}}
\newcommand\Tstrut{\rule{0pt}{3ex}} 
\newcommand\Tstrutt{\rule{0pt}{5ex}} 
\newcommand\Tstruttt{\rule{0pt}{12ex}}

%\renewcommand{\ell}{l}
\newcommand{\objf}{\boldsymbol{f}}
\newcommand{\nonneg}{\R^{2}_{\ge 0}}
\newcommand{\nd}[1]{\mathcal{ND}(#1)}
\newcommand{\comment}[1]{{\color{red} #1}}

%\addbibresource{biobjBB.bib}


\newcommand{\keywords}[1]{\medskip\\
\noindent\textbf{Keywords.} #1}
\newcommand{\subclass}[1]{\smallskip\\
\noindent\textbf{AMS subject classification.} #1}

\title{Branch-and-bound for biobjective mixed integer programming}
\author{Nathan Adelgren\thanks{Research supported in part by ONR grant  N00014-16-1-2725.} \textsuperscript{,}\footnote{Department of Mathematics and Computer Science, Edinboro University. \emph{Email}: \texttt{nadelgren@edinboro.edu}.} \and Akshay Gupte\footnotemark[1] \textsuperscript{,}\footnote{Department of Mathematical Sciences, Clemson University. \emph{Email}: \texttt{agupte@clemson.edu}.}}
%\author[1]{Nathan Adelgren}
%\author[2]{Akshay Gupte}
%\affil[1]{\footnote{Department of Mathematics and Computer Science, Edinboro University}}%, Edinboro, PA 16444}
%\affil[2]{\footnote{Department of Mathematical Sciences, Clemson University}}%, Clemson, SC 29634}
\date{October 20, 2016}

%Part of this research was carried out under support from ONR grant N00014-16-1-2725.

\begin{document}

\maketitle
{
\newcommand{\sep}{$\cdot$ }
\begin{abstract}
We present a generic branch-and-bound method for finding all the Pareto solutions of a biobjective mixed integer program. Our main contribution is new algorithms for obtaining dual bounds at a node, for checking node fathoming, presolve and duality gap measurement. Our various procedures are implemented and empirically validated on instances from literature and a new set of hard instances. We also perform comparisons against the triangle splitting method of \citeauthor{boland2015acriterion} [\emph{INFORMS Journal on Computing}, \textbf{27} (4), 2015], which is a objective space search algorithm as opposed to our variable space search algorithm. On each of the literature instances, our branch-and-bound is able to compute the entire Pareto set in significantly lesser time. Most of the instances of the harder problem set were not solved by either algorithm  in a reasonable time limit, but our algorithm performs better on average on the instances that were solved.
\keywords{Branch-and-bound \sep Mixed-integer \sep Multiobjective \sep Pareto solutions}
\subclass{90C11 \sep 90C57 \sep 90C29 \sep 90-04 \sep 90-08}
\end{abstract}
}
%

\section{Introduction}
Multiobjective mixed-integer linear programs belong to the class of multicriteria optimization \citep{ehrgott2005multicriteria} and are an extension of the single objective mixed-integer linear program (MILP) that has been studied for decades. These problems mathematically model discrete decisions to be made in the presence of multiple objective functions that must be considered simultaneously. Many real-life decision-making problems are characterized by multiple and conflicting criteria such as cost, performance, reliability, safety, productivity, and  affordability that drive the decision process. Hence the applications of multiobjective optimization are vast across engineering, management science, energy systems, finance, etc. \citep{HM2000,fugenschuh2006multicriteria,ehrgott2000survey,Jozefowiez2008293,hirschberger2013computing}. The multiobjective model requires more algorithmic and computational power than its single objective counterpart and offers broader perspective to practitioners and presents the user a choice amongst many solutions.  

A multiobjective problem is considered solved when the entire set of so-called Pareto optimal solutions has been discovered (cf. \textsection\ref{sec:defn} for definitions). A common approach to find these Pareto points has been to scalarize the vector objective \citep{ehrgott2006discussion} either by aggregating all objectives into one or by moving all but one objective to the constraints, but doing so does not generate all the Pareto points and supplies a very small part of the optimality information that can otherwise be supplied by the original multiobjective problem. Indeed, it is easy to construct examples of biobjective MILPs where many  Pareto solutions are located in the interior of the convex hull of the feasible set, a phenomenon that is impossible with optimal solutions of MILPs. The set of Pareto solutions of a mixed-integer multiobjective problem with a bounded feasible region is equal to the union of the set of Pareto solutions from each slice problem, where the union is taken over the set of integer feasible values and a slice problem is a continuous multiobjective program obtained by fixing the integer variables to some feasible values. In general, there are exponentially many Pareto solutions. Enumeration of the Pareto set for a pure integer problem has received considerable attention, including iterative approaches  \citep{lokman,ozpeynirci2010exact} and lower and upper bounds on the number of Pareto solutions \citep{bazgan2013number,stanojevic2013cardinality} under certain assumptions. \citet{de2009pareto} gave an algorithm that uses rational generating functions to enumerate all the Pareto optima in polynomial-time for fixed size of the decision space and fixed number of objectives. Later, \citet{blanco2012complexity} eliminated the dependence on the number of objectives. There also have been many efforts at finding good approximations of the Pareto set \citep{bazgan2015approximate,grandoni2014new,ruzika2005approximation,sayin2000measuring,sayin2003procedure}.



In this paper, we present an algorithm that computes the entire Pareto set of a biobjective mixed-integer linear program (BOMILP), formulated as
\begin{equation}\label{BOMILP}
\min_{x} \left\{\begin{array}{c} f_1(x) := {c^{1}}^{\top}x \\ f_2(x) := {c^{2}}^{\top}x \end{array}\right\} \quad \text{s.t.} \quad  x \in X_I := \left\{x\in \Z^{n}_+ \times \R^{p}_{+} \colon Ax\leq b,\; l_i \leq x_i \leq u_i \ \forall i\right\}.
\end{equation}
The only assumption we make on the above model is a mild and standard one: that $X_{I}\neq\emptyset$ and $-\infty < l_{i} < u_{i} < +\infty$ for all $i$, in order to have a bounded feasible problem. Pure integer multiobjective problems have been studied extensively in literature, both in full generality for 2 or more objectives \citep{kiziltan1983algorithm,KLEIN1982378,ralphs2006improved,parragh2015boip,jozefowiez2012generic,boland2016quadrant,boland2015shape,mavrotas2013improved,ehrgott2008improved}  and also for specific classes of biobjective combinatorial problems \citep{przybylski2010149,jozefowiez2012generic,sourd2008multiobjective,leitner2014computational,royset2007solving,biobjkp1998,berube2009exact}. The  algorithms specialized for the pure integer case do not extend to the mixed-integer case primarily because of the way they certify Pareto optimality. Note that the Pareto set of a mixed-integer multiobjective problem is a finite union of graphs of piecewise linear functions, whereas that for a pure integer problem is a finite set of points, and hence Pareto computation and certification of Pareto optimality of a given subset is far more  complicated in the former case.  So much so, that mixed-integer problems can benefit immensely from sophisticated data structures for storing Pareto sets; see the recent work of \citet{treestructure}. The mixed-integer case has been dealt in literature only for biobjective problems, with two schools of thought emerging on devising exact algorithms. The first one \citep{belotti2012biobjective} works in the $x$-space in $\R^{n+p}$ by  modifying the classical branch-and-bound method \bb{}  \citep{land1960automatic} devised for mixed-integer linear programs (MILPs), and solves linear programs (LPs) at each node. The correctness of \bb{}-type methods is guaranteed via correct node fathoming rules. Although procedures for implementing node fathoming and checking Pareto optimality have been proposed \citep{belotti2016fathoming}, the \bb{} algorithm has not been fully implemented and extensively tested. There is also some prior work on \bb{} specifically for mixed 0-1 biobjective problems \citep{vincent2013biobjective,mavrotas2005multi,stidsen2014branch}, but these algorithms do not contain a comprehensive study of all components of \bb{}. The second school of thought, wherein algorithms are devised separately for pure integer \citep{boland2015bcriterion} and mixed-integer \citep{boland2013criterion,boland2015acriterion} problems, is a objective space search method that iteratively partitions the $f$-space in $\R^{2}$ into smaller search regions, each of which is either rectangular or triangular in shape, and searches each region for Pareto optima by solving either MILPs or scalarized versions of biobjective LPs.   

%Although our algorithm works for both pure integer ($p=0$) and mixed-integer ($p\ge 1$) problems, we experiment only with instances of the latter. 




%Our emphasis on BOMILPs as opposed to the general multiobjective case is motivated by the fact that most of the work on solving BOMILP exactly is devoted to specific cases, such as pure integer , mixed 0-1  , or specific classes of 



  

%\begin{figure}[h!]
%\begin{center}
%\begin{subfigure}[b]{.32\textwidth}
%\centering
%	\begin{tikzpicture}
%\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=.6\textwidth]{figure1}};
%\node[align=center,yshift=1.1cm,xshift=-0.45cm] at (image.center) { $p^1_s$};
%\node[align=center,yshift=-.3cm,xshift=1.1cm] at (image.center) { $p^2_s$ };
%\node[align=center,yshift=-.2cm,xshift=-.4cm] at (image.center) { $p^\lambda_s$ };
%\end{tikzpicture}
%\caption*{\scriptsize (1a) Impact of $p^\lambda_s$.}
%\label{fig1}
%\end{subfigure}
%\begin{subfigure}[b]{.32\textwidth}
%\centering
%\begin{tikzpicture}
%\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=.6\textwidth]{figure2}};
%\node[align=center,yshift=.85cm,xshift=-.8cm] at (image.center) { $p^1_{s}$};
%\node[align=center,yshift=1.15cm,xshift=-0.35cm] at (image.center) { $p^1_{c}$};
%\node[align=center,yshift=-.45cm,xshift=1.25cm] at (image.center) { $p^2_{s,c}$ };
%%\node[align=center,yshift=-.15cm,xshift=1.55cm] at (image.center) { $p^2_c$ };
%\node[align=center,yshift=-.4cm,xshift=-.35cm] at (image.center) { $p^\lambda_{s,c}$ };
%\end{tikzpicture}
%\caption*{\scriptsize (1b) Branching on variable from $y^1_s$
%}
%\label{fig2}
%\end{subfigure}
%\begin{subfigure}[b]{.32\textwidth}
%\centering
%\begin{tikzpicture}
%\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=.6\textwidth]{figure3}};
%\node[align=center,yshift=.9cm,xshift=-.4cm] at (image.center) { $p^1_{s,c}$};
%\node[align=center,yshift=-.4cm,xshift=1.2cm] at (image.center) { $p^2_{s,c}$ };
%\node[align=center,yshift=-.25cm,xshift=-.45cm] at (image.center) { $p^\lambda_{s}$ };
%\node[align=center,yshift=.45cm,xshift=.3cm] at (image.center) { $p^\lambda_{c}$ };
%\end{tikzpicture}
%\caption*{\scriptsize (1c) Branching on variable from $y^\lambda_s$}
%\label{fig3}
%\end{subfigure}
%%\caption{Visualizations for Node Selection and Branching}
%\end{center}
%\end{figure}  \vspace*{-3mm}

%\pagebreak
Our exact algorithm for general BOMILP is based on the \bb{} method. Although there is certainly merit in studying and developing objective space search methods for solving BOMILP, our choice is motivated by the recognition that there is still much work that can be done to exploit the structure of Pareto points in biobjective problems to improve \bb{} techniques for BOMILP. That is indeed the main contribution of this paper --- an exhaustive computational study of ideas that specifically address the biobjective nature of \eqref{BOMILP}. Besides the fact that \bb{} operates mainly in the $x$-space and objective space search, as the name suggests, operates solely in the $f$-space, another point of distinction between the two is that the MILPs we consider at each node of the \bb{} tree do not have to be solved to optimality whereas the correctness of the latter depends on MILPs being solved to optimality. Of course, it is to be expected that solving MILPs for a longer time will lead to better convergence results for our \bb{}. Implementing our \bb{} through the callback interface of a MILP solver allows us to utilize the huge computational progress made in different components of \bb{} for MILP (see for example \cite{achterberg2013mixed,morrison2016branch}). 

The main components of any \bb{} for MILP include presolve, preprocessing, primal heuristics, dual bounding via cutting planes, node processing, and branching. We present new algorithms to adapt and extend each of these components to the biobjective case. We begin with presolve; since primal presolve techniques work solely on the feasible region, their implementations in state-of-the-art MILP solvers can be directly used for a BOMILP. However, dual presolve utilizes information from the objective function and hence cannot be used directly for a BOMILP. We are the first to discuss (\textsection\ref{sec:presolve}) and implement an extension of a variety of dual presolve techniques to the multiobjective setting. Additionally, we show that using one of the primal presolve techniques --- probing on integer variables (\textsection\ref{sec:probe}), alongside branching reduces the overall computational time. Two different preprocessing algorithms (\textsection\ref{sec:preprocessing}) are proposed for generating good primal bounds. Our main body of work is in developing new node processing techniques (\textsection\ref{sec:np}) for BOMILP.  The node processing component takes increased importance for BOMILP since bound sets for a multiobjective problem are much more complicated than those for a single objective problem (cf. \textsection\ref{sec:bounds}), meaning that generation of valid dual bounds and fathoming of a node is not as straightforward as that for MILPs. At each node, we describe procedures to generate valid dual bounds while accounting for the challenges of biobjective problems and strengthen these bounds through the use of locally valid cutting planes and the solution of single objective MILPs. Our bounds are tighter than what has previously been proposed. To guarantee correctness of our \bb{}, we develop new fathoming rules and delineate their difference to the recent work of \citet{belotti2016fathoming} in \textsection\ref{sec:compare}. A branching scheme is presented in \textsection\ref{sec:branch} and a method for exploiting distances between Pareto points in the objective space is discussed in \textsection\ref{sec:osgap}. Finally, our \bb{} also incorporates an early termination feature that allows it to terminate after a prescribed gap has been attained. In the MILP case, gap computation is trivial to implement because primal and dual bounds for MILPs are scalars. However for BOMILPs, since these bounds are subsets of $\R^{2}$ as explained in \textsection\ref{sec:bounds}, computation of optimality gap requires the use of error measures that are nontrivial to compute. To aid quicker computation, we propose in \textsection\ref{sec:gap} an approximated version of the Hausdorff metric and computationally compare it in our experiments to the hypervolume gap measure from literature.

An extensive computational analysis is carried out in \textsection\ref{sec:compute} to test the performance of our \bb{} method. Our first test set consists of randomly generated instances from literature. Recognizing the need for new and challenging set of large-scale instances, we generate new test instances for BOMILP using some of the single objective instances available in the MIPLIB 2010 library \citep{koch2011miplib}. Our initial set of tests is designed to evaluate the utility of a variety of techniques that we propose for use within the \bb{} framework. The first of these experiments evaluates our three dual presolve techniques %: duality fixing, exploitation of singleton columns, and exploitation of dominating columns. 
and the results show that duality fixing is the most useful of the three for reducing CPU time. In our second experiment, we demonstrate that preprocessing methods utilizing $\epsilon$-constraint scalarization techniques typically yield better primal bounds at the start of \bb{} than  weighted sum scalarization techniques. Next, we evaluate the performance of various procedures, such as probing, objective-space fathoming, a variety of cut generation techniques, and some minor improvements to our proposed fathoming rules, that we propose in this paper for improving the overall performance of \bb{}. These tests indicated that probing prior to each branching decision and objective space fathoming are very useful for decreasing the total solution time. 
%decreases \bb{} CPU time considerably (approximately 65\%) on the largest set of instances, and  objective space fathoming results in approximately a 28\% decrease in CPU time for these same instances. 
The local cuts that we added were not as useful. After evaluating the performance of these aspects of \bb{}, we compared the performance of our \bb{} with that of the triangle splitting method \citep{boland2015acriterion} for our test set of literature instances. %all instances of BOMILP previously considered in the literature. 
This test shows that our \bb{} uses less CPU time %\comment{by how much? put a \% figure. On all instances?} 
to compute the complete Pareto sets of these instances than the triangle splitting method. The performance profile in Figure~\ref{prof} shows the dominance of our \bb{} method. We also tested the performance of our \bb{} on large-scale instances developed from the MIPLIB 2010 library. Of the 115 instance considered in this test, 34 were solved by our original \bb{} implementation, 43 by a variant of our \bb{} that we propose in \textsection\ref{sec:osgap}, and 40 by the triangle splitting method. There were 17 instances which were solved by at least one variation of our \bb{}, but not by the triangle splitting method, and 10 instances solved by the triangle splitting method, but not by either of our \bb{} procedures. The performance profile in Figure~\ref{MIPLIB_perf_prof} compares the two methods. %\comment{Can we quote some \% numbers about the gaps for these instances? On the ones we didn't solve, were our average gaps small? On the ones we did solve, how much faster was \bb{}?} 
Overall, the computational experiments show that our \bb{} method is superior than the triangle splitting method on literature instances and performs comparably, and sometimes much better, on new challenging large-scale instances.
%We implement our method for the case $p=2$ since for $p\ge 3$ there are many obstacles primarily related to the storage and domination comparisons of the primal and dual bound sets during the fathoming procedures. %Though LP separation techniques can be utilized to determine whether or not a given set dominates another \citep{belotti2012biobjective}, in the case of partial dominance it is extremely difficult to determine precisely the subset of one set which is not dominated by another.

%\comment{Is the Jozefowiez paper for multiobjective?}
%{\color{cyan} Yes, but the Jozefowiez paper is also only for pure problems. }
%\comment{How is it different than our BB?}
%{\color{cyan} Well, the general framework is the same, except they use a B\&C method, so there is a constraint generation step. However, since our BB is a mixed-integer algorithm, we spend much time generating full bound sets (piece-wise linear) and comparing bound sets (determining dominance) is a far more tedious process. }
%\comment{What are challenges of general integer over 0-1 besides branching?}
%{\color{cyan} From what I can see, the branching strategies really are the main difference. It seems that algorithms designed for 0-1 problems use a depth-first approach. They select a variable, set it to 0, generate a bound, and check for dominance. If non-dominated, they select a new variable and repeat. Otherwise, they set the variable 1 and re-start the process.}
%\comment{How is our BB different?}
%{\color{cyan} In comparison to Dr. Belotti's BB, ours is different in the way in which we implement fathoming. That, and the use of our data structure to store the primal bound are the major differences.}
%\comment{What about the two-phase and $\varepsilon$-constraint method?}
%{\color{cyan} These methods are only for the pure case, so it doesn't make sense to compare them to our BB.}

We conclude this paper with a few remarks in \textsection\ref{sec:conclude}. %First,  we comment on the generation of strong cutting planes that can further enhance the proposed \bb{} method and transform it into a truly branch-and-cut method for BOMILPs. Second, 
We observe that a majority of the algorithms proposed in this paper can be extended naturally to the multiobjective case. The main challenge in developing a fully implementable and efficient \bb{} algorithm for multiobjective MILP is in carrying out the bound domination step. We present some directions for future research on this topic.

\section{Preliminaries}\label{sec:prelim}

\subsection{Definitions and Notation}\label{sec:defn}
The idea of optimality for single objective optimization is replaced with the idea of \emph{efficiency} in multiobjective problems. Consider BOMILP \eqref{BOMILP}. Denote $\objf(x) := (f_{1}(x),f_{2}(x))$. Given distinct $x,x^{\prime} \in X_I$, we say that $y=\objf(x)$ \emph{dominates} $y^{\prime} = \objf(x^{\prime})$ if $y \le y^{\prime}$, or equivalently $y^{\prime}\in y + \nonneg$. We denote this relationship as $y \dom y^{\prime}$. 
%This dominance is \emph{strong} if $f(\xbar,\ybar) < f(\xp,\yp)$; otherwise it is \emph{weak}. 
We then say that $x\in X_I$ is \emph{efficient} if there is no $x' \in X_I$ such that $\objf(x') \dom \objf(x)$. The set of efficient solutions is denoted by $X_E$. Let $Y_I = \{\objf(x) \colon x\in X_I\}$ be the image of integer feasible solutions. Then $y \in Y_I$ is called \emph{Pareto optimal} if its pre-image $\objf^{-1}(y)$ belongs to $X_{E}$. The set of Pareto optimal points is $Y_{N}$. The nondominated subset of any $S\subset\R^{2}$ is defined as $\nd{S} := \{y\in S\colon \nexists\, y^{\prime}\in S \text{ s.t. } y^{\prime}\dom y \}$. Thus $Y_{N} = \nd{Y_{I}}$.

For $k=1,2$, let $f^{\ast}_{k} := \min\{f_{k}(x)\colon x\in X_{I}\}$ be the optimal value of objective $k$ for the single objective problem. Denote
\begin{equation}
Y^{k}_I := \left\{y\in \R^{2} \colon y_{i} = f^{\ast}_{i} \ \, i\neq k,\, y_k = \min_{x\in X_I} \left\{ f_k(x) \colon f_i(x) = f^{\ast}_{i} \ \,  i\neq k\right\} \right\} \quad k=1,2.
\end{equation}
We have $Y^{k}_{I}\subset Y_{N}$. For each of $X_I$, $Y_I$, and $Y^k_I$, dropping the $I$ subscript indicates the continuous relaxation of the set. Also, if we add a subscript $s$, then it means that the set is associated with node $s$ of the BB tree. We use $\OS$ to denote the \emph{objective space}, i.e., the smallest rectangle in $\R^2$ that contains $Y$. Given $S\ \subseteq \OS \subseteq \R^2$, the \emph{ideal point} of $S$, denoted $S^{ideal}$, is the point $y \in \R^2$ with $y_k = \min_{y \in S}\{y_k\}$ for $k =1,2$. % \comment{What is the relation between Pareto optima and non dominated? Don't we use them interchangeably?}
%\begin{itemize}
%\item $\textbf{f}(x)$ vector valued objective function (assume $p$ objectives).
%\item $X_E$ -- The set of efficient $x$ solutions. 
%\item ${X} := \left\{x\in \R_+^{m+n}: Ax \leq b, l_i \leq x \leq u_i\right\}$ -- The continuous relaxation of $X_I$.
%\item $\mathcal{OS}$ -- The objective space.
%%\item $\S \in \OS$ -- A subset of the objective space made up of either a line segment or a singleton.
%\item $y := \textbf{f}(x)$ -- The mapping of $x\in X$ (or $x\in \tilde{X}$) into $\mathcal{OS}$.
%\item $Y_I := \textbf{f}(X_I) = \{y: y = \textbf{f}(x), x \in X_I\}$ -- The mapping of $X_I$ into $\mathcal{OS}$.
%\item ${Y} := \textbf{f}({X}) = \{y: y = \textbf{f}(x), x \in {X}\}$ -- The mapping of ${X}$ into $\mathcal{OS}$.
%\item $Y_N := \textbf{f}(X) = \{y: y = \textbf{f}(x), x \in X_E\}$ -- The mapping of $X_E$ into $\mathcal{OS}$ (this is the nondominated subset of $Y$).
%
%\item $x^I \in \Z_+^m$ -- The integer subvector of $x$.
%\item $x^C \in \R_+^n$ -- The continuous subvector of $x$.
%\item $y_i$ -- The $f_i$ value of any $y \in \mathcal{OS}$.
%\item $Y^{k}_I = \left\{y\in Y_I: y_k = \min_{x\in X_I} \left\{ f_k(x): f_i(x) \leq f_i(\hat{x}) \text{ for all } i\neq k,\,\, \hat{x}\in X_I \right\} \right\}$.
%\item $y^{k}_I$ -- any arbitrary element of $Y^{k}_I$.
%%\item $\mathcal{MR} := \max\{y^{se}_1 - y^{nw}_1, y^{nw}_2 - y^{se}_2\}$.
%\item $y' \dom y''$ -- $y'$ dominates $y''$
%\item $\P(\hat{x}) := \min_{x\in X_I} \left\{\textbf{f}(x): x^I = \hat{x}^I \right\}$
%\item Describe $\epsilon$-constraint method.
%\item Describe weighted-sum method.
%\item $\R^p_{\geq} = \{z \in \R^p: z_i \geq 0 \text{ for all } i\in\{1,\dots,p\}\}$
%\item $\R_{++} = \{p \in \R^2: p_1 \geq 0, p_2 \geq 0 \}$
%\item $\R_{+-} = \{p \in \R^2: p_1 \geq 0, p_2 \leq 0 \}$
%\item $\R_{-+} = \{p \in \R^2: p_1 \leq 0, p_2 \geq 0 \}$
%\item $\R_{--} = \{p \in \R^2: p_1 \leq 0, p_2 \leq 0 \}$
%%\item Each $x_i$ is assumed to have bounds $l_i$ and $u_i$, not necessarily finite.\\
%%\item $(X_s)_I$ -- feasible set associated with node $s$ of the BB tree
%%\item $\tilde{X}_s$ -- continuous relaxation of $X_s$
%%\item $Y^k_s = \left\{y\in Y: y_k = \min_{x\in X_s} \left\{ f_k(x): f_i(x) \leq f_i(\hat{x}) \text{ for all } i\neq k,\,\, \hat{x}\in X_s \right\} \right\}$.
%%\item $y^k_s$ -- an arbitrary element of $Y^k_s$.
%%\item $\tilde{Y}^k_s = \left\{y\in Y: y_k = \min_{x\in X_s} \left\{ f_k(x): f_i(x) \leq f_i(\hat{x}) \text{ for all } i\neq k,\,\, \hat{x}\in \tilde{X}_s \right\} \right\}$.
%%\item $\tilde{y}^k_s$ -- an arbitrary element of $\tilde{Y}^k_s$.
%\item Given a subset $S$ of $\OS \subset \R^p$, the \emph{ideal point} of $S$, denoted $S^{ideal}$, is the point $y \in \R^p$ for which $y_k = \min_{y \in S}\{y_k\}$ for each $k \in \{1,\dots,p\}$.
%\end{itemize}
%
%Note that for any of the sets above, if we drop the ``$I$'' subscript we are indicating the continuous relaxation of the given set. Also, if we add a subscript ``$s$'' to any of the above sets, then it means that the set is associated with node $s$ of the BB tree.
% to indicate that the feasible set used to generate

%\subsection{Single objective BB}
%
%Many of the prevalent techniques for solving BOMILP are based on the branch-and-bound (BB) method, which has been well established in the single objective case. A typical single objective BB procedure consists of the following steps:\\~\\
%{\bf{Presolve:}} Primal and dual information is used to simplify an instance of MILP in such a way that the optimal value is maintained. This can be done by removing variables which can be fixed to either their upper or lower bounds, removing  redundant constraints, tightening variable bounds or the right-hand-sides of constraints, and a variety of other procedures. For many instances of MILP, the use of presolve can significantly reduce solution time.\\~\\
%{\bf Node processing:} During an iteration of BB, one solves an LP subproblem at a node $s$ selected from a list $\mathscr{L}$ of open subproblems in the BB enumeration tree. As each node is processed, the optimal LP solutions are used to determine bounds on the optimal MILP solution. In particular, the objective value of the best known integer-feasible solution serves as an upper bound (also known as the primal bound) on the objective value of the optimal MILP solution. Similarly, the optimal LP solution at node $s$ serves as a lower bound (or dual bound) on the objective value of the optimal MILP solution. If after solving the LP at node $s$, the dual bound is found to be no less than the primal bound, node $s$ can be \emph{fathomed} or \emph{pruned} from the search tree. If, on the other hand, the dual bound at $s$ is found to be lower than the primal bound, work is often done to attempt to strengthen the dual bound at $s$. One example of a way in which this is done is by adding globally valid cutting planes to the subproblem associated with $s$.\\~\\
%{\bf Branching:} If after processing node $s$ it is not fathomed, there must exist some $x^I_{i}$ which takes a fractional value $\gamma_{i}$ in the LP optimal solution. In this case, a process referred to as \emph{branching} is performed in which two new subproblems are created by adding the constraints $x^I_i \geq \lceil \gamma_i \rceil$ and $x^I_{i} \le \lfloor \gamma_{i}\rfloor$, respectively, to the LP subproblem at $s$. These two subproblems are then added to $\mathscr{L}$ and a new iteration is begun by selecting a new node $s^{\prime}\in\mathscr{L}$ to explore and subdivide, if necessary. 

%By continually selecting subproblems to explore and subdivide, a BB tree of subproblems is formed. In the single objective case, a fractional solution to the LP at node $\eta$ provides a valid lower bound for all subproblems of $\eta$, while the objective function value associated with any integer feasible solution is an upper bound for every subproblem of the BB tree. These BB methods are effective because by comparing this global upper bound with the lower bound of a particular subtree, one is often able to prove that the subtree cannot provide a better integer feasible solution. Once this has been done, it is said that this subtree has been \emph{pruned} or \emph{fathomed}. Thus, in most cases the entire BB tree does not need to be explored and the best integer feasible solution can be found relatively quickly.
%
%In the biobjective case, the bound sets used for fathoming are no longer singletons in $\R$, instead they are subsets of $\R^{2}$ formed by taking unions of finitely many continuous convex piecewise linear functions \citep{ehrgott2007bound}. During each iteration $s$ of BB, a node $\eta_s$ of the BB tree is considered. The set of Pareto solutions to the LP relaxation of \eqref{BOMILP} associated with $\eta_s$ provides a lower bound set $\mathcal{L}_{s}$. The upper bound set $\mathcal{U}_G$, on the other hand, is globally valid to all nodes of the BB tree, although it is generally not known in its entirety until completion of the BB. Therefore, at each iteration $s$ of BB, since $\mathcal{U}_G$ is unknown and cannot be used for fathoming, another set $\mathcal{U}_s = \vartheta(\mathcal{N}_s)$ is used instead, where $\mathcal{N}_s \subset \Omega$ is a set containing no dominated points at iteration $s$. Now, in order to describe the mapping $\vartheta(\cdot)$, we introduce several definitions.
%%\begin{definition} ~
%A point $(\kappa_1,\kappa_2):=\kappa \in S \subset \R^2$ is said to be \emph{isolated} in $S$ if $\exists\, \epsilon>0$ for which $B_\epsilon(\kappa):=\{\hat{\kappa}\in S : \Vert \kappa - \hat{\kappa} \Vert_2 < \epsilon\}$ is empty.
%%\end{definition}
%%\begin{definition} ~
%Given distinct $\overline{\kappa},\kappa' \in S$ such that $\overline{\kappa}_1 < \kappa'_1$ and $\overline{\kappa}_2 > \kappa'_2$, the point $\kappa^n = (\kappa'_1,\overline{\kappa}_2)$ is called the \emph{local nadir point} with respect to $\overline{\kappa}$ and $\kappa'$ (note that the above inequalities are strict so that $\kappa^n\neq\kappa'$ and $\kappa^n\neq\overline{\kappa}$).
%%\end{definition}
%%\begin{definition} ~
%Given a line segment containing points in $S$, the segment itself is referred to as a \emph{local nadir set}.
%%\end{definition}
%We now describe the mapping $\vartheta(\cdot)$, which can be used to construct $\mathcal{U}_s$ given $\mathcal{N}_s$. Notice that if a line segment contains no dominated points, then it must have a negative slope. We use the notation $[\kappa^{nw},\kappa^{se}]$ to denote any such segment, where $\kappa^{nw}$ and $\kappa^{se}$ are the segment's north-west and south-east endpoints, respectively. Now, since at any iteration $s$ of BB $\mathcal{N}_s\subset \Omega$ contains no dominated points, each of its elements must be either an isolated point or a line segment with a negative slope. For each point $\kappa \in \mathcal{N}_s$ consider $\kappa_1$, and for each segment $[\kappa^{nw},\kappa^{se}]\in\mathcal{N}$ consider $\kappa^{nw}_1$. Arrange the elements of $\mathcal{N}_s$ in increasing order of these values. Then for each pair of adjacent elements $(\varepsilon_1,\varepsilon_2)\in\mathcal{N}_s$, if the south-east-most point of $\varepsilon_1$ is not equal to the north-west-most point of $\varepsilon_2$, calculate the local nadir point with respect to these two points and add it to a set $\mathcal{N}_s'$. Note that if $\varepsilon_i$ for $i\in\{1,2\}$ is a point and not a segment, then its north-west-most and south-west-most points are simply $\varepsilon_i$ itself. Now let $\mathcal{N}_s''$ be the set of local nadir sets in $\mathcal{N}_s$. Then $\vartheta(\mathcal{N}_s):=\mathcal{N}_s' \cup \mathcal{N}_s''$ and thus $\mathcal{U}_s=\mathcal{N}_s' \cup \mathcal{N}_s''$. Figure \ref{bound_sets} illustrates the relationship between $\mathcal{N}_s$ and $\mathcal{U}_s$.
%%$\proj_{y}P_{I}$ and the mapping $\vartheta(\cdot)$ depends on $\cup_{y\in\mathcal{N}}\S(y)$ and is described in \citep{ehrgott2007bound}. 
%
%One of the fathoming rules presented by \citet{belotti2012biobjective} states that at iteration $s$ of BB a node $\eta_s$ can be fathomed if $\mathcal{L}_s$ is separable from $\mathcal{U}_s$, i.e., $\mathcal{L}_s \cap (\mathcal{U}_s-\R^2_+) = \emptyset$. This is essentially the extension of the well known ``fathoming by bound dominance'' rule for single objective problems to the biobjective case. Figure \ref{separability} shows examples of lower bound sets $\mathcal{L}_{s_1}$ and $\mathcal{L}_{s_2}$. Notice that the locations of these sets show that $\eta_{s_1}$ cannot be fathomed but $\eta_{s_2}$ can. Clearly, efficient fathoming depends on the choice of $\mathcal{N}_s$ used to construct $\mathcal{U}_s$ since good approximations of $\mathcal{U}_G$ at each iteration of BB can help fathom a large number of nodes. 

We assume background in branch-and-cut algorithms for single objective problems; see for example \citet{martin2001general} for a survey. One of the key differences and challenging aspects of BOMILP versus MILP is the concept of primal and dual bound sets, which we explain next.

{
\newcommand{\Pareto}[1]{\mathcal{P}(#1)}
\subsection{Bound sets for BOMILP}\label{sec:bounds}
Similar to the single objective case, correct fathoming rules are essential for any \bb{} algorithm to solve BOMILP to Pareto optimality. Primal and dual bounds in a single objective \bb{} are scalars, making it easy to compare them and fathom a node by bound dominance. In biobjective BB, these bounds are subsets of $\R^{2}$. Bound sets were first discussed by \citet{ehrgott2007bound}. The manner in which these bound sets are generated within a \bb{} is conceptually similar to the single objective case and we explain this next. Note that our forthcoming explanation trivially extends to the multiobjective case. 

Suppose that we are currently at node $s$ of the \bb{} tree. The primal bound sets are constructed from the set of integer feasible solutions, denoted by $T_{s}\subset\Z^{n}$, found so far by the \bb{}. %$\Omega_{explored}$ denote the set of \bb{} nodes explored so far. Then 
For every $\tilde{x}\in T_{s}$, the BOLP obtained by fixing $x_{i}=\tilde{x}_{i}$ for $i=1,\dots,n$ in BOMILP \eqref{BOMILP} is called the \emph{slice problem}. The Pareto curve for this slice problem is $\nd{\objf(X(\tilde{x}))}$, where $X(\tilde{x})$ denotes the feasible set of the slice problem, and this curve is convex (because it is minimization) piecewise linear. Then $\N_{s} := \displaystyle\nd{\cup_{\tilde{x}\in T_{s}}\nd{\objf(X(\tilde{x}))}}$ is the globally valid primal bound calculated at node $s$. For the dual bound set, we consider BOLPs obtained by relaxing integrality on variables. Since $X_{s}$ denotes the relaxed feasible set at node $s$ and $Y_{s}=\objf(X_{s})$, the local dual bound is $\L_{s} := \nd{Y_{s}}$ and is convex piecewise linear. The global dual bound $\L^{global}_{s}$ is obtained by considering the local dual bounds for all the open nodes in the \bb{} tree, i.e., $\L^{global}_{s} = \nd{\cup_{s^{\prime}\in\Omega_{s}}\L_{s^{\prime}}}$ where $\Omega_{s}$ is the set of unexplored nodes so far, and this bound is a union of convex piecewise linear curves.

For multiobjective \bb{}, node $s$ is allowed to be fathomed by bound dominance if and only if $\L_{s}$ is dominated by $\N_{s}$, i.e., for every $y^{\prime}\in\L_{s}$ there exists a $y\in\N_{s}$ such that $y \dom y^{\prime}$. Equivalently, due to translation invariance of $\dom$, we have that node $s$ can be fathomed by bound dominance if and only if $\L_{s}+\nonneg \subset \N_{s}+\nonneg$. For this reason, henceforth for convenience, we consider our local dual bound to be $\L_{s}=\nd{Y_{s}}+\nonneg$ and the current primal bound to be $\U_{s}:=\N_{s}+\nonneg$. Thus the dual bound set is a polyhedron whereas the primal bound is a finite union of polyhedra. Although this deviates from the traditional view of bound sets, which defines them in the previous paragraph in terms of the boundary of these polyhedra, it is clear that there is a one-to-one correspondence between fathoming rules for the two alternate representations of bound sets.

Figure~\ref{bound_sets} illustrates the concept of bound sets. Here, $s_2$ can be fathomed because $\L_{s_{2}}\subset\U_{s}$ but we cannot say anything about fathoming node $s_1$ since $\L_{s_{1}}\nsubseteq\U_{s}$. As can be imagined from Figure~\ref{bound_sets}, fathoming is even more crucial and computationally expensive for BOMILPs since it involves checking inclusion and intersection of polyhedral sets as opposed to comparing scalar values in the MILP case. Thus, the majority of the computational effort in multiobjective BB is spent processing a node $s$ of the BB tree, in particular checking various fathoming rules.

%We treat the dual bound set as a single polyhedron in $\R^{2}$ and the primal bound set as a finite union of polyhedra in $\R^{2}$. Note that this deviates from the traditional view of bound sets which defines them in terms of the boundary of these polyhedra. \sout{However it is straightforward to see that equivalent fathoming rules exist for each definition.} \comment{YOU HAVE NOT DISCUSSED FATHOMING YET. STATE THE EQUIVALENCE WITHOUT INVOKING FATHOMING RULES.} {\color{cyan} The bound sets are themselves not equivalent. The main purpose of bound sets is to be used in conjunction with fathoming, and there are implementations of fathoming that will result in the same outcome regardless of which interpretation of bound sets is used. I think we still need to mention fathoming, but maybe we can mention dominance first and say something like, ``Recognize, however, that each of these definitions has an equivalent interpretation of dominance. If bound sets are interpreted as boundaries of a union of polyhedra, the dual bound is dominated by the primal bound if the dual bound is contained within the set resulting from adding the positive orthant to the primal bound. Alternatively, if these sets are interpreted as unions of polyhedra, the dual bound is dominated by the primal bound if the dual bound is a subset of the primal bound. As will be seen later in this work, this results in equivalent implementations of fathoming, regardless of the interpretation of bound sets used.''}

\begin{figure}[h]%[H]
\centering
\begin{tikzpicture}
\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.3\textwidth]{bounds_new2.jpg}};
\node[align=center,yshift=0.35cm,xshift=-.75cm] at (image.center) {\large $\U_s$ };
\node[align=center,yshift=1.25cm,xshift=-.8cm] at (image.center) {\large $\L_{s_1}$ };
\node[align=center,yshift=0.15cm,xshift=0.95cm] at (image.center) {\large $\L_{s_2}$ };
\node[align=center,yshift=-1.05cm,xshift=-1.75cm] at (image.center) {\large $\N_s$ };
\end{tikzpicture}
\caption{Primal $(\U)$ and dual $(\L)$ bound sets for BOMILP}
\label{bound_sets}
\end{figure}


%We use $\L_{s}$ to denote the locally valid dual bound set generated from Pareto solutions of the BOLP relaxation at this node; this dual bound can be calculated as $\L_{s} = Y_{s} + \R^{2}_{\ge}$. Let $\N_{s}$ denote the current nondominated set of  solutions in $\OS$ that correspond to some feasible integer solutions in $X_{I}$; this set $\N_{s}$ is the nondominated subset of $\cup_{\text{nodes $s^{\prime}$ explored so far}}(Y_{s^{\prime}})_{I}$. The globally valid primal bound generated from the solutions in $\N_{s}$ is $\U_s := \N_s + \nonneg$. Using these sets, the most basic idea of fathoming for BOMILP is: node $s$ can be fathomed if $\L_{s}\subseteq\U_{s}$. %if $\L_s$ is separable from $\U_s$, 
%%Notice that $\U_s$ is obtained from $\N_s$ by adding a local nadir point between each adjacent pair of solutions in $\N_s$.  
%%Examples of $\L_s$ can be seen in Figure \ref{bound_sets_2}. 
%%This figure also depicts the most basic idea of fathoming for BOMILP: if $\L_s \subset \U_s$, node $s$ can be fathomed. 



}


%
%
%\begin{figure}[H]
%\begin{subfigure}[h]{.5\textwidth}
%\centering
%\begin{tikzpicture}
%\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=.83\textwidth]{ideal_pts_sgmnt5}};
%\node[align=center,yshift=2.1cm,xshift=-0.95cm] at (image.center) {%\large 
%$y^1_s$ };
%\node[align=center,yshift=0.0cm,xshift=2cm] at (image.center) {%\large 
%$y^\lambda_s$ };
%\node[align=center,yshift=-0.9cm,xshift=3.15cm] at (image.center) {%\large
% $y^2_s$ };
%\node[align=center,yshift=-1.85cm,xshift=-.6cm] at (image.center) {%\large
% LP ideal };
% \node[align=center,yshift=-2.2cm,xshift=-.6cm] at (image.center) {%\large
% points };
%\node[align=center,yshift=.35cm,xshift=-1.7cm] at (image.center) {%\large 
%LP} ;
%\node[align=center,yshift=-.00cm,xshift=-1.7cm] at (image.center) {%\large 
%ideal} ;
%\node[align=center,yshift=-.35cm,xshift=-1.7cm] at (image.center) {%\large 
%segment};
%%\node[align=center,yshift=-1.75cm,xshift=.55cm] at (image.center) {%\large
%% MILP} ;
%%\node[align=center,yshift=-2.1cm,xshift=.55cm] at (image.center) {%\large 
%%ideal pts.} ;
%%\node[align=center,yshift=.4cm,xshift=.65cm] at (image.center) {%\large 
%%pts.} ;
%%\node[align=center,yshift=2.05cm,xshift=1.25cm] at (image.center) {%\large 
%%$(y^1_s)_I$ };
%%\node[align=center,yshift=0.25cm,xshift=1.9cm] at (image.center) {%\large 
%%$(y^\lambda_s)_I$ };
%%\node[align=center,yshift=-0.05cm,xshift=2.9cm] at (image.center) {%\large 
%%$(y^2_s)_I$ };
%%\node[align=center,yshift=1.5cm,xshift=2.25cm] at (image.center) {%\large 
%%MILP} ;
%%\node[align=center,yshift=1.15cm,xshift=2.25cm] at (image.center) {%\large 
%%ideal seg.} ;
%\end{tikzpicture}
%\caption{LP ideal points and segment}
%\label{ideal_pts_sgmnt}
%\end{subfigure}%
%\begin{subfigure}[h]{.5\textwidth}
%\centering
%\centering
%\begin{tikzpicture}
%\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=.83\textwidth]{ideal_pts_sgmnt6}};
%\node[align=center,yshift=2.1cm,xshift=-0.95cm] at (image.center) {%\large 
%$y^1_s$ };
%\node[align=center,yshift=-1.0cm,xshift=2.5cm] at (image.center) {%\large 
%$y^\lambda_s$ };
%\node[align=center,yshift=-0.9cm,xshift=3.15cm] at (image.center) {%\large
% $y^2_s$ };
%%\node[align=center,yshift=-1.55cm,xshift=-1.9cm] at (image.center) {%\large
%% LP ideal };
%% \node[align=center,yshift=-1.9cm,xshift=-1.9cm] at (image.center) {%\large
%% points };
%%\node[align=center,yshift=.35cm,xshift=-1.7cm] at (image.center) {%\large 
%%LP} ;
%%\node[align=center,yshift=-.00cm,xshift=-1.7cm] at (image.center) {%\large 
%%ideal} ;
%%\node[align=center,yshift=-.35cm,xshift=-1.7cm] at (image.center) {%\large 
%%segment};
%\node[align=center,yshift=-.75cm,xshift=-.3cm] at (image.center) {%\large
% MILP} ;
%\node[align=center,yshift=-1.1cm,xshift=-.3cm] at (image.center) {%\large 
%ideal pts.} ;
%%\node[align=center,yshift=.4cm,xshift=.65cm] at (image.center) {%\large 
%%pts.} ;
%\node[align=center,yshift=2.05cm,xshift=1.25cm] at (image.center) {%\large 
%$(y^1_s)_I$ };
%\node[align=center,yshift=0.25cm,xshift=1.9cm] at (image.center) {%\large 
%$(y^\lambda_s)_I$ };
%\node[align=center,yshift=-0.05cm,xshift=2.9cm] at (image.center) {%\large 
%$(y^2_s)_I$ };
%\node[align=center,yshift=1.35cm,xshift=2.05cm] at (image.center) {%\large 
%MILP} ;
%\node[align=center,yshift=1cm,xshift=2.05cm] at (image.center) {%\large 
%ideal seg.} ;
%\end{tikzpicture}
%\caption{MILP ideal points and segment}
%\label{ideal_pts_sgmnt}
%\end{subfigure}
%\caption{Fathoming Rules}
%\label{fath_rules_fig}
%\end{figure}

%We utilize the data structure presented in \citet{adelgren2014} to store and dynamically update $\N_s$. We use $\Pi$ to denote this data structure and represent each of its elements as $\pi \in \Pi$. Specifically, $\Pi$ is designed to store (i) line segments and (ii) isolated singletons (i.e., singletons which are not part of any stored line segment) in $\R^2$, and thus $\N_s$ is stored as a combination of these. Each time an $x\in X$ is found throughout the course of our BB scheme, $y = \textbf{f}(x)$ is inserted into the data structure. If there is no $\pi \in \Pi$ such that $\pi \dom y$ then the Pareto set of $\P(x^I)$ is generated, and all singletons and/or segments making up this set are also inserted into the structure. Throughout this work, when we say that a solution is added to $\N_s$, we are referring to the process of inserting that solution (either a line segment or singleton) into the data structure. We will also use the notation $\N_s^2 := \{\S \in \N_s : \S \text{ is a line segment} \}$ and $N_s^1 = \N_s \setminus \N_s^2$, as well as analogous notations $\L_s^1$, $\L_s^2$, $\U_s^1$ and $\U_s^2$. Note that through the use of this data structure, we rarely need to explicitly construct the set $\U_s$ since each $\L_s$ is separable from $\U_s$ if and only if $\N_s \dom \S$ for all $\S \in \L_s$. Since $\N_s$ is completely stored in $\Pi$, we are able to check this criterion through the use of a function \textsc{CheckDominance}($\cdot$) that takes $\S \in \OS$ as input, and returns 1 if $\N_s \dom S$, and 0 otherwise. This function has worst-case complexity of $O(\log(t))$, where $t$ represents the number of solutions stored in $\Pi$.

\section{Presolve and Preprocessing}\label{sec:pre}
Examining the structure of an instance of single objective MILP prior to solving it, and utilizing information found during this examination to simplify the structure of the instance often has had a significant impact on the time and effort needed to solve that instance. %(\emph{include references here.}) 
It has also been shown that knowledge of feasible solutions for an instance of MILP can have a significant impact on solution time. %(\emph{include references here.}) 
Hence, it seems natural as a first step to extend the techniques used in these procedures to the biobjective case. For the discussion that follows we distinguish the idea of simplifying an instance of BOMILP based on its problem structure from the idea of determining a set of initial integer feasible solutions. We refer to the first as \emph{presolve} and the latter as \emph{preprocessing}. %We propose a procedure which is carried out in three phases: (i) Presolve phase 1, (ii) Preprocessing and (iii) Probing on variables.\\ %Presolve Phase 2. 

\subsection{Presolve} \label{sec:presolve}
Presolve for MILP uses both primal and dual information. The primal information of a BOMILP instance is no different than its single objective counterpart and thus primal presolve techniques can be applied directly to it. However, due to the presence of an additional objective, one must take care while utilizing dual information for a biobjective problem. 
%Though there are a plethora of presolve techniques available for single objective BB, our goal in this work is not to develop a commercial-scale BB procedure for BOMILP, but to highlight many different tools that could be employed in such a procedure. Hence, here 
We extend a few single objective dual presolve techniques to BOMILP (their extension to three or more  objectives is immediate and omitted here). In particular, we discuss duality fixing \citep{martin2001general} and the exploitation of singleton and dominating columns \citep{gamrath2015}. The proofs are given in Appendix \ref{app:proofs} since they are straightforward generalizations of those for MILPs. %We first review these procedures and then present propositions outlining how each procedure can be extended for use with multiple objectives. 
%For this discussion we assume that a single objective problem is given in the form
%\begin{equation}\label{single_ob_MILP}
%\min_x \left\{\su[j=1]{m+n} c_j x_j\right\}\,\,\, s.t.\,\, x \in X_I'
%\end{equation}
%where $ X_I':= \left\{x \in \R^m\times \Z^n: \su[j=1]{m+n} a_{i j} x_j \leq b_i \text{ for all } i, \ell_j \leq x_j \leq u_j \text{ for all } j\right\}$. We also assume that there exists a multiobjective counterpart to this problem for which objective $k$ has the form $f_k(x) = \sum_{j=1}^{m+n} c^k_j x_j$. 
%\paragraph{Duality fixing.}
%Suppose there exists a column $j$ with $c_j \geq 0$ and $a_{ij} \geq 0$ for all $i$. If $\ell_j > -\infty$, $x_j$ can be fixed to $\ell_j$. Otherwise, the problem is unbounded or infeasible. Similarly, suppose there exists a column $j$ with $c_j \leq 0$ and $a_{ij} \leq 0$ for all $i$. If $u_j < \infty$, $x_j$ can be fixed to $u_j$. Otherwise, the problem is unbounded or infeasible. Utilizing these ideas is known as duality fixing, which is discussed, for example, in \citep{martin2001general}. The following proposition holds for the multiobjective problem.

Let $a_{rj}$ denote the element of matrix $A$ in row $r$ and column $j$ and $c^{k}_{j}$ be the $j^{th}$ entry of $k^{th}$ objective. 

\begin{prop}[Duality fixing]\label{duality_fixing}
Suppose there exists a $j$ with $c^k_j \geq 0$ and $a_{ij} \geq 0$ for all $k,i$. Then $X_{E}\subseteq \{x\colon x_j = l_j\}$. Similarly, if there exists a $j$ with $c^k_j \leq 0$ and $a_{ij} \leq 0$ for all $k,i$, then $X_{E}\subseteq \{x\colon x_j = u_j\}$.
\end{prop}



\begin{prop}[Singleton Columns]\label{singleton_columns}
\renewcommand{\gamma}{t}
For every row $r$ in the system $Ax\le b$, define ${J}(r) := \{j \in \{1,\dots,m\}: a_{rj} > 0, c_j^k < 0 \ \forall k, a_{ij} = 0 \ \forall i\neq r\}$ and \[{U}_r := \sum_{j\in {J}(r)} a_{rj}l_j + \sum_{j\not\in {J}(r), a_{rj} > 0} a_{rj}u_j + \sum_{j\not\in {J}(r), a_{rj} < 0} a_{rj}l_j.\] Suppose there exists some $s\in {J}(r)$ such that $c_s^k/a_{rs} \leq c_\gamma^k/a_{r\gamma}$ for all $\gamma \in {J}(r)\setminus\{s\}$. If $a_{rs}(u_s-l_s) \leq b_r - {U}_r$, then {$X_{E}\subseteq\{x\colon x_{s}=u_{s}\}$.}
%for any efficient solution $x$, there exists an alternative efficient solution $x^*$ such that $f_k(x^*) \leq f_k(x)$ for all $k$ and  $x^*_s = u_s$.
\end{prop}

Note that a similar procedure can be followed for $a_{rj} < 0$, $c_j^k > 0$ for all $k$, thereby fixing $x_{s}=l_{s}$. Now, given two variables $x_{i}$ and $x_{j}$, either both integer or both continuous, we say that $x_j$ \emph{dominates} $x_i$ if (i) $c_j^k \leq c^k_i$ for all $k$, and (ii) $a_{r j} \leq a_{ri}$ for every $r$. This variable domination has no relationship with the idea of domination between bound sets. %Observe the following lemma, which is an obvious extension from Lemma 1 of \citep{gamrath2015}.


\begin{prop}[Dominating columns]\label{dominating_col_disjunction}
Suppose that $x_j$ dominates $x_i$ in the BOMILP. Then {$X_{E}\subseteq\{x\colon x_{j}=u_{j}\} \cup \{ x\colon x_{i}=l_{i}\}$.}
%for any efficient solution $x$ there exists an alternative efficient solution $x^*$ such that $f_k(x^*) \leq f_k(x)$ for all $k$ and either $x^*_j = u_j$ or $x^*_j = \ell_j$.
\end{prop}

One may use the disjunction resulting from Proposition \ref{dominating_col_disjunction} to generate valid cutting planes for $X_I$ prior to branching. Additionally, there are also ways to further utilize the structure of dominating columns in order to strengthen variable bounds as described in \citep[Theorem 3, Corollary 1 and 2]{gamrath2015}. These methods for strengthening bounds also extend to the multiobjective case. However, we did not find these methods to be advantageous in practice. Thus, since the description of these additional strategies is quite lengthy, we omit them from this work.\\ %methodology for doing so.

%We now recall several quantities defined in \citep{gamrath2015}. Let a MILP of the form \eqref{single_ob_MILP} be given, together with a row index $r$ and variables $x_s$ and $x_t$. Then define:
%\begin{equation}
%U_r^t(x_t) = \su[j\neq t, a_{rj} > 0]{}a_{rj}u_j + \su[j\neq t, a_{rj} < 0]{}a_{rj}\ell_j + a_{rt}x_t,
%\end{equation}
%\begin{equation}
%L_r^t(x_t) = \su[j\neq t, a_{rj} > 0]{}a_{rj}\ell_j + \su[j\neq t, a_{rj} < 0]{}a_{rj}u_j + a_{rt}x_t,
%\end{equation}
%\begin{equation}
%MAXL_s^t(x_t) = \max_\gamma\left\{\frac{b_\gamma-L_\gamma^t(x_t)+a_{\gamma s}u_s}{a_{\gamma s}}: a_{\gamma s}, a_{\gamma t} < 0 \right\},
%\end{equation}
%\begin{equation}
%MAXU_s^t(x_t) = \max_\gamma\left\{\frac{b_\gamma-U_\gamma^t(x_t)+a_{\gamma s}\ell_s}{a_{\gamma s}}: a_{\gamma s}, a_{\gamma t} < 0 \right\},
%\end{equation}
%\begin{equation}
%MINL_s^t(x_t) = \min_\gamma\left\{\frac{b_\gamma-L_\gamma^t(x_t)+a_{\gamma s}\ell_s}{a_{\gamma s}}: a_{\gamma s}, a_{\gamma t} > 0 \right\},
%\end{equation}
%and
%\begin{equation}
%MINU_s^t(x_t) = \min_\gamma\left\{\frac{b_\gamma-U_\gamma^t(x_t)+a_{\gamma s}u_s}{a_{\gamma s}}: a_{\gamma s}, a_{\gamma t} > 0 \right\}.
%\end{equation}
%
%The following proposition shows ways in which column dominance can be exploited in order to strengthen variable bounds.
%
%\begin{prop}\label{dominating_cols_strength}
%For a given multiobjective MILP assume $x_j \dom x_i$, then for any efficient solution $x$ there exists an alternative efficient solution $x^*$ such that $f_k(x^*) \leq f_k(x)$ for all $k$ and the following hold:
%\begin{enumerate}
%\item $x^*_j \leq MINL^i_j(\ell_i)$
%\item $x^*_i \geq MAXL_i^j(u_j)$
%\item $x^*_j \geq \min\{u_j,MAXL_j^i(\ell_i)\}$
%\item $x^*_i \leq \max\{\ell_i,MINL_i^j(u_j)\}$
%\item If $c_j^k \leq 0$ for all $k$, then $x_j^* \geq \min\{u_j, MINU_j^i(\ell_i)\}$
%\item If $c_i^k \geq 0$ for all $k$, then $x_i^* \leq \max\{\ell_i, MAXU^j_i(u_j)\}$
%\end{enumerate}
%\end{prop} 

%Examples of presolve techniques which can be extended using Proposition \ref{prop1} are duality fixing \citep{martin2001general} and the exploitation of singleton and dominating columns \citep{gamrath2015}. Hence, Presolve Phase 1 consists of two straightforward steps. We first employ CPLEX primal presolve and then the biobjective extensions of duality fixing and exploiting singleton and dominating columns. Note that these presolve techniques are not the only ones for which this extension is applicable. However, our goal in this work is not to develop a commercial-level biobjective BB tool, but simply to demonstrate the strengths of biobjective BB and highlight some of what can be done.

\subsection{Preprocessing} \label{sec:preprocessing}  As in the single objective case, the efficiency of BB can be significantly improved if good-quality primal feasible solutions can be generated prior to the start of BB. This can be accomplished by a heuristic method, such as \citep{soylu2015heuristic,leitner2016ilp}. We utilize two different preprocessing techniques, both of which solve single objective MILPs subject to a certain time limitation --- the first uses the $\epsilon$-constraint method, and the second uses the weighted-sum approach. We briefly discuss the benefits and drawbacks of using either the $\epsilon$-constraint or weighted-sum approaches (see \citep{ehrgott2005multicriteria} for background on scalarization methods).

%We, subject to a specified time restriction $\mathscr{T}$, and also use some simple heuristics in order to obtain a subset of $Y_I$ prior to beginning the main step of the BB procedure. For solving MILPs technique 1 employs the $\epsilon$-constraint method, while technique 2 employs the weighted-sum method. 


\paragraph{\textbf{$\epsilon$-constraint:}} It is well known that for a BOMILP every $y \in Y_N$ can be obtained using the $\epsilon$-constraint method. Unfortunately though, when a MILP formulated using the $\epsilon$-constraint method is not solved to optimality, there are two major drawbacks: (i) each $y\in Y_I$ discovered while processing the MILP must lie within a restricted region of $\OS$, and (ii) the information associated with the best dual bound cannot be utilized. 

\paragraph{\textbf{weighted-sum:}} The major drawback of the weighted sum method is that when a MILP is formulated using this method, only \emph{supported} Pareto solutions can be found, i.e., those lying on the boundary of the convex hull of $Y_N$. There are, however, the following two benefits: (i) $y \in Y_I$ discovered during the MILP solve are not restricted to any particular region of $\OS$, and (ii) the best dual bound is valid for all $y \in Y_I$ and can therefore be used to create a cutting plane in $\OS$. \\

As can be seen, there is a certain level of trade-off present between the $\epsilon$-constraint method and the weighted sum method. The pros and cons of each technique are illustrated in Figures \ref{preprocessing_epsilon} and \ref{preprocessing_weighted}. For each of these figures, we have the following: (i) $Y_N$, which we assume to be unknown, is shown in grey, (ii) the optimal solution, which we assume is not known at termination of the MILP solve, is depicted as a yellow star, (iii) the best known solution at termination is shown as a blue square, and (iv) the level curve associated with the best known dual bound at termination is shown as a dotted red line. Note that for Figure \ref{preprocessing_epsilon}, we assume that $\epsilon$ is defined so that the feasible region is restricted to the light blue box. 

\begin{figure}
\begin{subfigure}[h]{.48\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{preprocessing_1}
\caption{$\epsilon$-constraint method}
\label{preprocessing_epsilon}
\end{subfigure} 
\begin{subfigure}[h]{.48\textwidth}
\centering
\includegraphics[width=.8\textwidth]{preprocessing_2}
\caption{Weighted sum method}
\label{preprocessing_weighted}
\end{subfigure}
\caption{Bound information when a single objective MILP terminates early}
\label{preprocessing_fig}
\end{figure}~\\

We now present Algorithms \ref{alg5} and \ref{alg6} in which we describe our proposed $\epsilon$-constraint and weighted sum based preprocessing procedures.
\begin{algorithm}[h!] 
%\small
  \caption{Preprocessing based on the $\epsilon$-constraint method.\\
  \underline{Input}: $y^1_I$, $y^2_I$ and a nonnegative value for parameter $\rho$.\\
  \underline{Output}: An initialized set of Pareto solutions $\protect\N_0 \subseteq Y_N$.}
  \label{alg5}
  \begin{algorithmic}[1]
     \Function{PreprocessingMethod1}{$y^1_I,y^2_I,\rho$} 
    	\State Let $\N_0 = \emptyset$.
    	\State Solve the MILP $\min\{f_\lambda(x): x\in {X}_I\}$ to obtain ${y}^\lambda_I \in {Y}_I$.
    	\State Add a cutting plane to $X$ lying on the level curve of $f_\lambda$ associated with the best dual solution.
    	\State Set $h_1 = \frac{(y^2_I)_1 - (y^\lambda_I)_1}{60}$, $\epsilon_1 = (y^\lambda_I)_1 + h_1$, $h_2 = \frac{(y^1_I)_2 - (y^\lambda_I)_2}{60}$ and $\epsilon_2 = (y^\lambda_I)_2 + h_2$.
    	\For{$k \in \{1,2\}$}{}
    		\While{$\epsilon_k > (y^k_I)_k$}{ solve the MILP $P_k(\epsilon_k):=\min\{f_{\{1,2\}\setminus\{k\}}(x): x\in {X}_I, f_k(x) \leq \epsilon_k\}$ to obtain $y^* \in Y_N$.}
    			\If{$\N_0 \not\dom y^*$}{ set $h_k = \frac{h_k}{1+\rho}$.}
    			\Else{ set $h_k = \max(5-\rho,1) h_k$.} 
    			\EndIf
    			\For{each $x \in X_I$ found while solving $P_k(\epsilon_k)$}{ let $N = $ \textsc{GenerateDualBd}$(s(x))$ and set $\N_0 = \nd{\N_0 \cup N}$.}
    			\EndFor
    			\State Set $\epsilon_k = \epsilon_k + h_k$.
    		\EndWhile
    	\EndFor
    	\State Return $\N_0$.
    \EndFunction
  \end{algorithmic}
\end{algorithm} 
On line 3 of Algorithm \ref{alg5} we solve the MILP associated with $f_\lambda$. Recall that $\lambda$ is computed so that the level curves of $f_\lambda$ have the same slope as the line segment joining $y_I^1$ and $y_I^2$. On line 5 we then use the solution of this MILP to compute horizontal and vertical step sizes, $h_1$ and $h_2$. These step sizes are then used to sequentially increase the values of $\epsilon_1$ and $\epsilon_2$ which are used on line 7 to construct new MILPs, using the $\epsilon$-constraint problem, which may yield new, undiscovered Pareto solutions. On lines 8 and 9 we modify the step sizes $h_1$ and $h_2$. If the MILP solved on line 7 yields a new, previously undiscovered Pareto solution, we decrease the step size. Otherwise we increase it. This allows us the continue searching for additional new solutions in locations of $\OS$ which are near previously discovered solutions, and to cease searching in areas in which new solutions are not being generated. Note that the amount in which the step sizes are increased or decreased depends on the value of the parameter $\rho$. Also note that each time we solve a MILP, we utilize its solution to update $\N_s$. 

\begin{algorithm}[h!] 
%\small
  \caption{Preprocessing based on the weighted-sum method.\\
  \underline{Input}: A nonnegative value for parameter $\rho$.\\
  \underline{Output}: An initialized set of Pareto solutions $\protect\N_0 \subseteq Y_N$.}
  \label{alg6}
  \begin{algorithmic}[1]
    \Function{PreprocessingMethod2}{$\rho$} 
    	\State Let $\N_0 = \emptyset$.
    	%\State Solve the MILP $\min\{f_\lambda(x): x\in {X}_I\}$ to obtain ${y}^\lambda_I \in {Y}^k_I$.
%    	\State Add a cutting plane to $X$ which lies on the level curve of $f_\lambda$ associated with the best found dual solution.
    	\State Set $\Lambda = \{\lambda\}$, $\Lambda' = \{0,1\}$ and $t=0$.
    	\While{$t \leq \rho$}{}
    		\State Set $\tau = 0$ and $\sigma = |\Lambda|$.
    		\For{$\lambda' \in \Lambda$}{ remove $\lambda'$ from $\Lambda$ and add it to $\Lambda'$.\hfill(Assume $\Lambda'$ is always sorted in increasing order.)}
    			\State Solve the MILP $P(\lambda'):=\min\{f_{\lambda'}(x): x\in {X}_I\}$ to obtain ${y}^{\lambda'} \in {Y}_I$.
    			\State Add a cutting plane to $X$ lying on the level curve of $f_{\lambda'}$ associated with the best dual solution.
    			\If{$\N_0 \not\dom {y}^{\lambda'}$}{ set $\tau = \tau + 1$.}
    			\EndIf
    			\For{each $x \in X_I$ found while solving $P(\lambda')$}{ let $N = $ \textsc{GenerateDualBd}$(s(x))$ and set $\N_0 = \nd{\N_0 \cup N}$.}
    			\EndFor
    		\EndFor
    		\For{each adjacent pair $(\lambda_1,\lambda_2)\in\Lambda'$}{ add $\frac{\lambda_1+\lambda_2}{2}$ to $\Lambda$.}
    		\EndFor
    		\If{$\tau < \frac{\sigma}{5}$}{ set $t = t+1$.}
    		\EndIf
    	\EndWhile
    	\State Return $\N_0$.
    \EndFunction
  \end{algorithmic}
\end{algorithm} 

In Algorithm \ref{alg6} we compute several sets of weights which we utilize in the weighted-sum approach to generate Pareto solutions. We initialize the set of weights $\Lambda$ on line 3 with the weight $\lambda$ for which the level curves of $f_\lambda$ have the same slope as the line segment joining $y_I^1$ and $y_I^2$. We use $\sigma$ to represent the number of weights for which MILPs will be solved in a given iteration. We deem an iteration successful if at least a fifth of the solved MILPs reveal previously undiscovered Pareto solutions. We use $\tau$ to count the number of unsuccessful iterations. On line 11 we increase the number of weights that will be used in the next iteration by computing the next set of weights so that it contains the midpoint of each pair of adjacent weights in the set $\Lambda'$, which is the set of previously used weights together with 0 and 1. The process then terminates when the number of unsuccessful iterations exceeds the value of the parameter $\rho$. As we did with Algorithm \ref{alg5}, we also utilize the solution of each MILP we solve in this procedure to update $\N_s$. 
%We now discuss the details of Preprocessing techniques 1 and 2. Both techniques begin with the same three steps. First $y^{se}$ is discovered by solving a single objective MILP with a weighted sum objective $f_\lambda:=\lambda f_1 + (1-\lambda) f_2$, where the value of $\lambda\in(0,1)$ is chosen sufficiently close to 1. Next $y^{nw}$ is discovered in a similar fashion, using $\lambda$ sufficiently close to 0. Note that when solving these two MILPs, no maximum time limit is used. In the third step, another single objective MILP is solved using $f_\lambda$ with $\lambda$ determined based on the slope of the segment connecting $y^{nw}$ and $y^{se}$. When solving this MILP, though, an upper limit is set on the solution time. We denote its solution as $y^0$. If this third MILP is able to be solved to optimality in the given time, preprocessing is continued by employing technique 1, otherwise technique 2 is used. 
%
%In Preprocessing technique 1, $y^0$ is used in create two subregions of $\OS$ that ought to be searched for Pareto optimal solutions. Specifically, subregion 1 is $y^0 + \R_{-+}$ and subregion 2 is $y^0 + \R_{+-}$. In each subregion the $\epsilon$-constraint method is used to create a sequence of single objective MILPs that are solved in order to reveal Pareto solutions. Note that when technique 1 is utilized, all MILPs are solved to optimality. %After each MILP is solved, two tasks are performed: (i) if the optimal solution of the MILP is $(x^*,y^*)$, then the Pareto set of $\P(y^*)$ is generated and added to $\N_0$, and (ii) a subset of previously discovered integer-feasible solutions are provided to a genetic algorithm which is used to attempt to find new integer-feasible solutions whose associated points in the objective space are not dominated by $\N_0$. 
%For subregion 1, the objective used for the single objective MILPs is $f_1$ and a restriction is added so that $f_2 \leq \epsilon$ for some $\epsilon \in \R$. The opposite convention is taken in subregion 2. For each subregion initial values of $\epsilon$ and initial step sizes, which we denote, respectively, as $\epsilon_0^i$ and $\triangle \epsilon^i$, for $i=1,2$, are chosen so that if for the $k^{\text{th}}$ MILP solved in subregion $i$, $\epsilon = \epsilon_0^i + k\triangle \epsilon^i$, exactly 60 MILPs are solved within each subregion. After each MILP is solved, if no new integer feasible solution has been discovered, and the step size is less than some upper limit $\overline{\triangle \epsilon^i}$, the step size is increased by a factor of two. Alternatively, if a new integer feasible solution is discovered, and the step size is greater than some lower limit $\underline{\triangle \epsilon^i}$, the step size is decreased by a factor of three.
%
%In Preprocessing technique 2, single objective MILPs are solved using objective $f_\lambda$ for a variety of values of $\lambda$. These MILPs are each allowed to run for 1 minute, and then terminated. A list is used to store values of $\lambda$ which may yield new supported Pareto solutions. Along with each stored value of $\lambda$ we associate upper and lower bounds, $\lambda^u$ and $\lambda^l$. Then, after the MILP is solved using $f_\lambda$, $\lambda$ is replaced in the list with two new values, $\lambda_{-} = \frac{\lambda+\lambda^l}{2}$ and $\lambda_+=\frac{\lambda+\lambda^u}{2}$. We then set $\lambda_{-}^l = \lambda^l$, $\lambda_{-}^u = \lambda_{+}^l = \lambda$ and $\lambda_{+}^u = \lambda^u$. After each MILP is solved we use the level curve in $\OS$ associated with best discovered dual bound to generate a globally valid cutting plane and add it CPLEX's list of cuts. Technique 2 is terminated when a maximum of 40 MILPs are solved, or if approximately one third of the MILPs that have been solved have not produced new Pareto solutions.
%
%We note that throughout both technique 1 and 2 we do the following: 
%\begin{enumerate}
%\item We save the CPLEX MILP starts associated with each discovered $y \in Y$ and use them for every subsequent MILP solved throughout the rest of Preprocessing as well as the entire biobjective BB procedure.
%\item After each MILP is solved, a subset of the most recently discovered solutions are passed to a genetic algorithm which is used to attempt to find yet undiscovered $y\in Y$ which are not dominated by any $\S \in \N_0$.
%%\item Every time a new solution, say $(x^*,y^*)$, is discovered, whether as a member of CPLEX's solution pool after solving a MILP or by the genetic algorithm, if $f(x^*,y^*)$ is not dominated by $\N_0$ we generate the set of Pareto solutions for $\P(y^*)$ (slice problem) and add it to $\N_0$.
%\end{enumerate} 
%
%Note that prior to the start of Preprocessing two single objective MILPs are solved in order to generate $y^1$ and $y^2$. We measure the time taken to solve these MILPs and if the solution time for each is less than $\mathscr{T}$ we employ Preprocessing technique 1 since it is likely that the solution time for each $\epsilon$-constraint MILP will be less than $\mathscr{T}$. Otherwise we use Preprocessing technique 2 so that best-bound information can be used for MILPs not solved to completion.
%
%%\subsubsection{Presolve Phase 2} 
%The solutions obtained during Preprocessing then form $\N_0$ which we use to conduct probing, as outlined in Proposition \ref{probe}.% we use the solutions stored in $\N_0$ and the probing technique outlined in Proposition \ref{probe} to strengthen the bounds on $x^I$. 
% We utilize this probing technique for each integer $x_i $ fixed to $l_i$, repeating it until $l_i$ has been increased as much as possible before selecting a new variable. Furthermore, after utilizing the probing technique for each $x_i$ fixed to $l_i$ we use the analogous procedure with each $x_i$ fixed to $u_i$. 

\subsection{Probing} \label{sec:probe}
After Preprocessing, a probing technique can be used to strengthen the bounds on each integer variable, as stated below. %This probing technique is described in the following proposition.

\begin{prop}[Probing on $x_{i}$]\label{probe}
Let $x_{i}$ be an integer variable. Fix $x_i = l_i$, relax integrality on other integer variables and solve the BOLP relaxation to obtain its Pareto set $\mathcal{L}_{l_i}$. If $\U_0 \dom \mathcal{L}_{l_i}$ then $X_{E}\subseteq\{x\colon x_{i}\ge l_{i}+1 \}$. %for any efficient solution $x^*$, $x^*_i \geq l_i+1$.
\end{prop}
\begin{proof}
Recognize that $\mathcal{L}_{l_i}$ dominates every $y \in Y_I$ where $y = \objf(x)$ with $x_i = \l_i$. The desired result follows from $\U_0 \dom \mathcal{L}_{l_i}$.
\end{proof}

This probing procedure can be repeated multiple times for a given integer $x_i$ and then iterated over each additional integer variable $x_j$. Furthermore, a similar procedure to that of Proposition \ref{probe} exists for tightening the upper bound. We point out that there are likely many more tasks that could be performed during Presolve and/or Preprocessing that could further impact the performance of BB. However, our goal here is not to develop extensive procedures for these tasks, but to put together an initial implementation that highlights some of what can be done.\\ %At this point we finish our discussion on Presolve/Preprocessing and move on the other aspects of BB, beginning with node processing.



\section{Node processing} \label{sec:np}
Processing a node consists of three basic steps: (i) Generate a valid dual bound; (ii) Check a fathoming rule to determine whether or not $s$ can be eliminated from the search tree; (iii) Optionally, if $s$ is not fathomed in (ii), generate a tighter dual bound and repeat (ii). Figure \ref{fath_rules_fig} provides a visual example of how one might carry out these three steps. Most of the fathoming rules for biobjective BB are designed to check whether or not $\U_s$ dominates $(Y_s)_I$ by exploiting the transitivity of dominance. First, a set $\T$ is generated such that $\T \dom (Y_s)_I$. Then if $\U_s \dom \T$, $\U_s \dom (Y_s)_I$ and $s$ can be fathomed. Otherwise, a tighter bound on $(Y_s)_I$ is needed. The first bound we use is a set of two ideal points which we obtain by solving three single objective LPs; one for each $f_k$ and an one with a weighted sum objective $f_\lambda$ in which the weights, denoted $\lambda^s$, are given by the normal vector of the line segment $H_s$ passing through $y^1_s$ and $y^2_s$. We begin with these points because it is straightforward to determine whether or not $\U_s$ dominates a singleton. In Figure \ref{ideal_pts_sgmnt} these points are labelled ``LP ideal points.'' Notice that they are not dominated. Consider the intersection of $(Y_s)^{ideal} + \nonneg$ and the line with normal vector $\lambda^s$ passing through $y^\lambda_s$. Recognize that this intersection, which we denote $H_s^\lambda$, is also a valid dual bound. In Figure \ref{ideal_pts_sgmnt} the resulting line segment is labelled ``LP ideal segment,'' but is not dominated. A tighter bound can next be found by explicitly generating $\L_s$. In Figure \ref{ideal_pts_sgmnt} this is the set indicated by the red points, which is again not dominated. After generating $\L_s$, one cannot hope to find a tighter bound on $(Y_s)_I$ resulting from LP solutions. Instead, one can solve single objective MILPs to generate elements of $(Y_s)_I$ and use these elements to form a valid dual bound. We first generate ideal points in the same way as before, but use single objective MILPs rather than LPs. In Figure \ref{ideal_pts_sgmnt2} these points are labelled ``MILP ideal points.'' Yet again they are not dominated. We can then consider the intersection of $((Y_s)_I)^{ideal} + \nonneg$ and the line with normal vector $\lambda^s$ passing through $(y^\lambda_s)_I$, which we denote $\tilde{H}_s^\lambda$. This intersection forms another valid dual bound. In Figure \ref{ideal_pts_sgmnt2} the resulting line segment is labelled ``MILP ideal segment'' and is dominated. Hence, $s$ can be fathomed in this example.

\begin{figure}%[H]
\begin{subfigure}[h]{.5\textwidth}
\centering
\begin{tikzpicture}
\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=.83\textwidth]{ideal_pts_sgmnt5}};
\node[align=center,yshift=2.1cm,xshift=-0.95cm] at (image.center) {%\large 
$y^1_s$ };
\node[align=center,yshift=0.0cm,xshift=2cm] at (image.center) {%\large 
$y^\lambda_s$ };
\node[align=center,yshift=-0.9cm,xshift=3.15cm] at (image.center) {%\large
 $y^2_s$ };
\node[align=center,yshift=-1.85cm,xshift=-.6cm] at (image.center) {%\large
 LP ideal };
 \node[align=center,yshift=-2.2cm,xshift=-.6cm] at (image.center) {%\large
 points };
\node[align=center,yshift=.35cm,xshift=-1.7cm] at (image.center) {%\large 
LP} ;
\node[align=center,yshift=-.00cm,xshift=-1.7cm] at (image.center) {%\large 
ideal} ;
\node[align=center,yshift=-.35cm,xshift=-1.7cm] at (image.center) {%\large 
segment};
%\node[align=center,yshift=-1.75cm,xshift=.55cm] at (image.center) {%\large
% MILP} ;
%\node[align=center,yshift=-2.1cm,xshift=.55cm] at (image.center) {%\large 
%ideal pts.} ;
%\node[align=center,yshift=.4cm,xshift=.65cm] at (image.center) {%\large 
%pts.} ;
%\node[align=center,yshift=2.05cm,xshift=1.25cm] at (image.center) {%\large 
%$(y^1_s)_I$ };
%\node[align=center,yshift=0.25cm,xshift=1.9cm] at (image.center) {%\large 
%$(y^\lambda_s)_I$ };
%\node[align=center,yshift=-0.05cm,xshift=2.9cm] at (image.center) {%\large 
%$(y^2_s)_I$ };
%\node[align=center,yshift=1.5cm,xshift=2.25cm] at (image.center) {%\large 
%MILP} ;
%\node[align=center,yshift=1.15cm,xshift=2.25cm] at (image.center) {%\large 
%ideal seg.} ;
\end{tikzpicture}
\caption{LP ideal points and segment}
\label{ideal_pts_sgmnt}
\end{subfigure}%
\begin{subfigure}[h]{.5\textwidth}
\centering
\centering
\begin{tikzpicture}
\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=.83\textwidth]{ideal_pts_sgmnt6}};
\node[align=center,yshift=2.1cm,xshift=-0.95cm] at (image.center) {%\large 
$y^1_s$ };
\node[align=center,yshift=-1.0cm,xshift=2.5cm] at (image.center) {%\large 
$y^\lambda_s$ };
\node[align=center,yshift=-0.9cm,xshift=3.15cm] at (image.center) {%\large
 $y^2_s$ };
%\node[align=center,yshift=-1.55cm,xshift=-1.9cm] at (image.center) {%\large
% LP ideal };
% \node[align=center,yshift=-1.9cm,xshift=-1.9cm] at (image.center) {%\large
% points };
%\node[align=center,yshift=.35cm,xshift=-1.7cm] at (image.center) {%\large 
%LP} ;
%\node[align=center,yshift=-.00cm,xshift=-1.7cm] at (image.center) {%\large 
%ideal} ;
%\node[align=center,yshift=-.35cm,xshift=-1.7cm] at (image.center) {%\large 
%segment};
\node[align=center,yshift=-.75cm,xshift=-.3cm] at (image.center) {%\large
 MILP} ;
\node[align=center,yshift=-1.1cm,xshift=-.3cm] at (image.center) {%\large 
ideal pts.} ;
%\node[align=center,yshift=.4cm,xshift=.65cm] at (image.center) {%\large 
%pts.} ;
\node[align=center,yshift=2.05cm,xshift=1.25cm] at (image.center) {%\large 
$(y^1_s)_I$ };
\node[align=center,yshift=0.25cm,xshift=1.9cm] at (image.center) {%\large 
$(y^\lambda_s)_I$ };
\node[align=center,yshift=-0.05cm,xshift=2.9cm] at (image.center) {%\large 
$(y^2_s)_I$ };
\node[align=center,yshift=1.35cm,xshift=2.05cm] at (image.center) {%\large 
MILP} ;
\node[align=center,yshift=1cm,xshift=2.05cm] at (image.center) {%\large 
ideal seg.} ;
\end{tikzpicture}
\caption{MILP ideal points and segment}
\label{ideal_pts_sgmnt2}
\end{subfigure}
\caption{Fathoming in biobjective BB}
\label{fath_rules_fig}
\end{figure}

%Then it is also true that $\cup_{A \in P_s} (A)^{ideal} \dom Y_s \dom (Y_s)_I$. Notice that generating $(Y_s)^{ideal}$ requires solving $p$ LPs, one for each objective  $f^k$ with $k \in \{1,\dots,p\}$ (and the solution of each is $y^k_s$). However, by generating the normal vector $\lambda^s$ for the hyperplane containing each $y^k_s$ and solving one additional LP with objective $f_{\lambda^s} := \sum_{i=1}^{p} \lambda^s_i f_i$ (with solution $y^\lambda_s$) we can generate $\cup_{A \in P_s} (A)^{ideal}$ for the following partition of $Y_s$: $P_s = \cup_{i=1}^{p} A_i$ with $A_i = \{y\in Y_s: y_j \leq y^\lambda_{s,j} \text{ for all }j \leq i, y_r > y^\lambda_{s,r} \text{ for all } r < i\}$. Since it only requires one additional LP to generate the $p$ ideal points for this partition, we begin by generating these points and checking to see if each is dominated by $\U_s$. This is fathoming rule 1a. If this rule fails, we can generate the hyperplane having normal vector $\lambda^s$ and passing through $y^\lambda_s$. Clearly the intersection between $((Y_s)^{ideal} + \R^p_{\geq})$ and this hyperplane dominates $Y_s$ and therefore, if $\U_s$ dominates this intersection, $\U_s \dom Y_s$ and so node $s$ can be fathomed. This is fathoming rule 2a. If fathoming rules 1a and 2a fail, we resort to explicitly generating $Y_s$ and then if $\U_s \dom Y_s$ we fathom. This is fathoming rule 3. If these 3 fathoming rules fail we resort to finding $\mathbb{T}$ such that $\mathbb{T} \dom (Y_s)_I$ but $\mathbb{T} \not\dom Y_s$. Fathoming rule 1b is to generate a series of ideal points coming from either MILP solutions or LP solutions such that the union of these ideal points dominates $(Y_s)_I$ and determining whether or not $\U_s$ dominates the union of these ideal points. How one should select these solutions (as in LP solutions vs MILP solutions) is not necessarily clear in the multiobjective case, but in the biobjective case it is clear. Without loss of generality, assume that in the biobjective case the ideal point resulting from $y^1_s$ and $y^\lambda_s$ is not dominated by $\U_s$, then the MILPs associated with $f_1$ and $f_\lambda$ should be solved to generate a better ideal point. This only requires solving 2 MILPs to increase the quality of the ideal point. However, in the multiobjective case increasing the quality of a single ideal point would require solving $p$ MILPs. Fathoming rule 2b begins by generating the hyperplane with normal vector $\lambda^s$ passing through $(y^\lambda_s)_I$. The intersection between $((Y_s)_I^{ideal} + \R^p_{\geq})$ and this hyperplane dominates $(Y_s)_I$ and therefore, if $\U_s$ dominates this intersection, $\U_s \dom (Y_s)_I$ and so node $s$ can be fathomed.

%Note that methods for strengthening $\L_s$ may vary greatly or be highly dependent on the number of objectives. However, one technique which can be used in any case is the addition of globally and/or locally valid cutting planes to the subproblem associated with $s$.\\~\\
We now formally outline the fathoming rules employed in this work. Some additional notation will be useful. For $k \in \{1,2\}$, define 
\begin{equation}
\mathcal{P}^k_s := \left(\cup_{i \neq k} {y}^i_s\right) \cup {y}^\lambda_s,
\end{equation} 
and let 
\begin{equation}
\mathcal{P}_s := (\mathcal{P}^1_s)^{ideal} \cup (\mathcal{P}^2_s)^{ideal}.
\end{equation} 
Additionally, for any $\mathcal{I} \subset \{1,2,\lambda\}$, define 
\begin{equation}
D^\mathcal{I}_s:=\cup_{k=1}^{2} \left(\left(\mathcal{P}^k_s\setminus \cup_{i\in \mathcal{I}}\, {y}^i_s \right) \cup \cup_{i \in \mathcal{I}\setminus\{k\}}\, ({y}^i_s)_I\right)^{ideal}.
\end{equation}
$\mathcal{P}_s$ represents the sets of ideal points obtained from LP solutions, while  $D^\mathcal{I}_s$ represents a set of ideal points obtained from a mixture of LP and MILP solutions. 
%Observe the following propositions which provide fathoming rules for node $s$.

\begin{prop}[Fathoming Rules]\label{fath0} 
Node $s$ can be fathomed if:
\begin{enumerate}
\item[0.] $\L_s \subset (Y_s)_I$
\item[1a.] $\U_s \dom \mathcal{P}_s$
\item[2a.] $\U_s \dom {H}_s^\lambda$
\item[1b.] $\U_s \dom D^\mathcal{I}_s$ for some $\mathcal{I} \subset \{1,2,\lambda\}$
\item[2b.] $\U_s \dom \tilde{H}_s^\lambda$.
\item[3.] $\L_s \subseteq \U_s$
\end{enumerate}
\end{prop}
\begin{proof}
Rule 0 is due to integer feasibility of $\L_s$. Rule 1a holds since by construction $\mathcal{P}_s \dom \L_s$, and so $\U_s \dom \L_s$. Rule 2a holds since by construction $\tilde{H}_s^\lambda \dom \L_s$, and so $\U_s \dom \L_s$. For Rule 1b, note that by construction, for any $\mathcal{I} \subset \{1,2,\lambda\}$, $D^\mathcal{I}_s \dom (y_s)_I$ for every $(y_{s})_{I} \in (Y_s)_I$ and thus $D^\mathcal{I}_s$ is a valid dual bound at node $s$. For Rule 2b, note that by construction ${H}_s^\lambda \dom (y_s)_I$ for every $(y_s)_I \in (Y_s)_I$ and thus ${H}_s^\lambda$ is a valid dual bound at node $s$. Rule 3 is obvious. 
\end{proof}

Proposition \ref{fath0} outlines five fathoming rules. Rule 0 expresses the idea of fathoming due to optimality, while the remainder of the rules indicate situations in which $s$ can be fathomed due to bound dominance.

Before we outline the process we use for processing a node $s$, we briefly discuss another important task that ought to be carried out while processing node $s$: Updating $\N_s$. We do this in two ways: (i) add each integer-feasible line segment discovered while checking Fathoming Rule 0 to $\N_s$, and (ii) for each discovered $x^* \in X_I$, generate the nondominated subset of 
\begin{equation}
\mathcal{Y}(x^*) := \{y = \objf(x): x \in X, x_i = x^*_i \text{ for all } i \in \{m+1,\dots, m+n\}\}
\end{equation}
and add each defining line segment of this set to $\N_s$. Consider the latter of these strategies. Observe that the feasible set of $\mathcal{Y}(x^*)$ can be interpreted as a leaf node of the BB tree, which we denote $s(x^*)$. Hence, the $\mathcal{Y}(x^*) + \nonneg = \L_{s(x^*)}$. This leads to a need for generating the nondominated subset of $\L_{s}$, i.e. $\nd{\L_s}$. Typical techniques for generating $\nd{\L_{s}}$ include the multiobjective simplex method and the parametric simplex algorithm (PSA) \citep{ehrgott2005multicriteria}. However, the multiobjective simplex method is far more robust than is necessary for biobjective problems. Also, we found in practice that using the PSA often resulted in many basis changes yielding the same extreme point of $\L_s$ in $\OS$. Since much work is done during the PSA to determine the entering and exiting variables, we found that generating $\nd{\L_s}$ using the PSA required a significant amount of computational effort. We decided to use an alternative method for generating $\nd{\L_s}$ which relies on sensitivity analysis. We first solve the single objective LP using objective $f_2$ to obtain $y^2_s$. Next we create the LP 
\begin{equation}
\mathscr{P}_s(\alpha):=\min\{ f_1(x) + \alpha f_2(x): x \in X_s\}
\end{equation}
and then carry out the procedure outlined in Algorithm \ref{alg2}.

\begin{algorithm}[h] 
%\small
  \caption{Generate $\nd{\L_s}$\\
  \underline{Input}: Node $s$.\\
  \underline{Output}: A set $\mathcal{B}$ containing all defining line segments of $\nd{\L_s}$.}
  \label{alg2}
  \begin{algorithmic}[1]
    \Function{GenerateDualBd}{$s$} 
        \State Set $\mathcal{B} = \emptyset$.
        \State Solve the LP $\min\{f_2(x): x \in X_s\}$ to obtain $y^2_s$.
    	\State Solve $\mathscr{P}_s(0)$ to obtain solution $x^*$ and set $y = \objf(x^*)$.
    	\While{$y \neq y^2_s$}{}
    		\State Use sensitivity analysis to obtain an interval $[\alpha',\alpha'']$ such that $x^*$ is optimal to $\mathscr{P}_s(\alpha)$ for all $\alpha \in [\alpha',\alpha'']$.
    		\State Let $\alpha^*$ be the negative reciprocal of the slope of the line segment connecting $y$ and $y^2_s$.
    		\State Set $x^* = \argmin\{\mathscr{P}_s(\alpha'' + \epsilon)\}$ for sufficiently small $\epsilon \in (0,\alpha^*-\alpha'']$.
    		\If{$\objf(x^*) \neq y$}
    			\State {Add the line segment connecting $\objf(x^*)$ and $y$ to $\mathcal{B}$. Update $y$ to be $\objf(x^*)$.}
    		\EndIf
    	\EndWhile
    	\State Return $\mathcal{B}$.
    \EndFunction
  \end{algorithmic}
\end{algorithm} 

In lines 3 and 4 of Algorithm \ref{alg2} we compute the south-east and north-west most extreme points of $\nd{\L_s}$, respectively. The while loop beginning on line 5 is then used to sequentially compute adjacent extreme points of $\nd{\L_s}$ in a west to east pattern, until the south-east most extreme point is rediscovered. Each line segment joining a pair of adjacent extreme points of $\nd{\L_s}$ is stored and the set of all computed segments is returned at the end of the procedure. Note that the correctness of the algorithm relies on an appropriately small choice for $\epsilon$ on line 8. As we have discussed, there are other methods which can be used here that do not rely on $\epsilon$, such as the PSA or the first phase of the two-phase method for solving biobjective combinatorial problems \citep{ehrgott2005multicriteria}. We have already discussed the difficulties we encountered with the PSA. The difficulty with the first phase of the two-phase method is that, although it generates the extreme supported Pareto solutions of a BOLP, it does not generate them in order from left to right. Thus, when using a simplex-style solution method for each single objective LP, each iteration can require a significant number of basis changes. Our method generates these extreme points in order from left to right, and as a result, warm-starting each iteration by reusing the basis information from the previous iteration reduces the overall number of required basis changes.

Recognize from Proposition \ref{fath0} that Fathoming Rules 0 and 3 each impose a condition on $\L_s$ and therefore require knowledge of $\nd{\L_s}$ in order to be employed. We note, however, that for each of these rules it is often unnecessary to generate $\nd{\L_s}$ entirely. In particular, the generation of $\nd{\L_s}$ should cease if: (i) one is checking Fathoming Rule 0 and a defining line segment of $\nd{\L_s}$ is generated that is not integer feasible, or (ii) one is checking Fathoming Rule 3 and a defining line segment of $\nd{\L_s}$ is generated that is not contained in $\U_s$. Hence, the procedures in Algorithm \ref{alg2} can be modified in order to develop strategies for checking Fathoming Rules 0 and 3. These strategies are outlined in Algorithms \ref{alg4} and \ref{alg3}, respectively. 

\begin{algorithm}%[h] 
%\small
  \caption{Fathoming Rule 0\\
  \underline{Input}: Node $s$ and solutions $y^1_s$ and $y^2_s$.\\
  \underline{Output}: 1 if node $s$ should be fathomed, 0 otherwise.}
  \label{alg4}
  \begin{algorithmic}[1]
    \Function{FR\_0}{$s, y^1_s, y^2_s$} 
    	\State $y^1_s$ is the solution to $\mathscr{P}_s(0)$. Let $x^*$ represent the preimage of $y^1_s$. Set $y = y^1_s$. %and set $y = \objf(x^*)$.
    	\If{$y = y^2_s$}{ return 1}%STOP! Fathom node $s$.}
    	\Else
    		
    	\While{$y \neq y^2_s$}{}
    		\State Use sensitivity analysis to obtain an interval $[\alpha',\alpha'']$ such that $x^*$ is optimal to $\mathscr{P}_s(\alpha)$ for all $\alpha \in [\alpha',\alpha'']$.
    		\State Let $\alpha^*$ be the negative reciprocal of the slope of the line segment connecting $y$ and $y^2_s$.
    		\State Set $x^* = \argmin\{\mathscr{P}_s(\alpha'' + \epsilon)\}$ for sufficiently small $\epsilon \in (0,\alpha^*-\alpha'']$.
    		\If{$\objf(x^*) \neq y$}
    			\State {Let $\mathbb{S}$ represent the line segment connecting $\objf(x^*)$ and $y$.}
    			\If{$\mathbb{S} \not\subset (Y_s)_I$}{ return 0}%STOP! Do not fathom node $s$.}
    			\Else{ Update $y$ to be $\objf(x^*)$.}
    			\EndIf
    		\EndIf
    	\EndWhile
    	\State{return 1}%Fathom node $s$.}
    	\EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm} 

Algorithm \ref{alg4} follows almost the same procedure as Algorithm \ref{alg2}, except it terminates prematurely on line 10 if a line segment is computed that is not integer feasible.
\begin{algorithm}%[h] 
%\small
  \caption{Fathoming Rule 3\\
  \underline{Input}: Node $s$ and solutions $y^1_s$ and $y^2_s$.\\
  \underline{Output}: 1 if node $s$ should be fathomed, 0 otherwise.}
  \label{alg3}
  \begin{algorithmic}[1]
    \Function{FR\_3}{$s, y^1_s, y^2_s$} 
    	\State $y^1_s$ is the solution to $\mathscr{P}_s(0)$. Let $x^*$ represent the preimage of $y^1_s$. Set $y = y^1_s$. %and set $y = \objf(x^*)$.
    	\If{$y = y^2_s$}
    		\If{$\U_s \dom y$}{ return 1}%STOP! Fathom node $s$.}
    		\Else{ return 0}
    		\EndIf
    	\Else
    		
    	\While{$y \neq y^2_s$}{}
    		\State Use sensitivity analysis to obtain an interval $[\alpha',\alpha'']$ such that $x^*$ is optimal to $\mathscr{P}_s(\alpha)$ for all $\alpha \in [\alpha',\alpha'']$.
    		\State Let $\alpha^*$ be the negative reciprocal of the slope of the line segment connecting $y$ and $y^2_s$.
    		\State Set $x^* = \argmin\{\mathscr{P}_s(\alpha'' + \epsilon)\}$ for sufficiently small $\epsilon \in (0,\alpha^*-\alpha'']$.
    		\If{$\objf(x^*) \neq y$}
    			\State {Let $\mathbb{S}$ represent the line segment connecting $\objf(x^*)$ and $y$.}
    			\If{$\U_s \not\dom \mathbb{S}$}{ return 0}%STOP! Do not fathom node $s$.}
    			\Else{ Update $y$ to be $\objf(x^*)$.}
    			\EndIf
    		\EndIf
    	\EndWhile
    	\State{return 1}%Fathom node $s$.}
    	\EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm} 
Algorithm \ref{alg3} also follows almost the same procedure as Algorithm \ref{alg2}. However, this procedure terminates prematurely on line 5 or 12 if a point or line segment is computed that is not dominated by $\U_s$. We have now built the tools necessary to present our proposed procedure for processing a node $s$. We do so in Algorithm~\ref{alg1}.

\begin{algorithm}%[h] 
%\small
  \caption{Process node $s$}
  \label{alg1}
  \begin{algorithmic}[1]
    \Function{ProcessNode}{$s$} 
    	\State Compute valid cutting planes for $(X_s)_I$ and add them to the description of $X_s$.
    	\For{$k \in \{1,2\}$}{ Solve $\min\{f_k(x): x\in {X}_s\}$ to find optimal solution $\bar{x}^k$ and generate ${y}^k_s \in {Y}^k_s$.}
    		\If{$y^k_s \in (Y_s)_I$}{ let $N = $ \textsc{GenerateDualBd}$(s(\bar{x}^k))$ and set $\N_s = \nd{\N_s \cup N}$.}
    		\EndIf
    	\EndFor
    	\If{$y^1_s,y^2_s \in (Y_s)_I$}
    		\If{\textsc{FR\_0}$(s,y^1_s,y^2_s)$ = 1}{ Fathom $s$, STOP!\hfill(Fathoming Rule 0)}
    		\EndIf
    	\EndIf
    	\State{Calculate $H_s$ and $\lambda^s$ using ${y}^1_s$ and $y^2_s$. Solve $\min\{f_\lambda(x): x\in {X}_s\}$ to find optimal solution $\bar{x}^\lambda$ and generate ${y}^\lambda_s \in {Y}^\lambda_s$.}
    	\If{$y^\lambda_s \in (Y_s)_I$}{ let $N = $ \textsc{GenerateDualBd}$(s(\bar{x}^\lambda))$ and set $\N_s = \nd{\N_s \cup N}$.}
    	\EndIf
    	\If{$\U_s \dom {y}^1_s$, $\U_s \dom {y}^2_s$ and $\U_s \dom {y}^\lambda_s$}
    		\If{$\U_s \dom \mathcal{P}_s$}{ Fathom $s$, STOP! \hfill(Fathoming Rule 1a)}
    		\Else
    			\State{Calculate $\tilde{H}_s^\lambda$.}
    			\If{$\U_s \dom \tilde{H}_s^\lambda$}{ Fathom $s$, STOP! \hfill(Fathoming Rule 2a)}
    			\Else
%    				\State{Define $\mathscr{D} = \emptyset$.}
%    				\While{$\mathscr{D} \neq \L_s$}{}
%    					\State{Generate a defining simplex $\mathcal{S}$ of $\L_s$ where $\mathcal{S} \not\in \mathscr{D}$; add $\mathcal{S}$ to $\mathscr{D}$.}
%    					\If{$\U_s \not\dom \mathcal{S}$}{ break}
%%    						\If{No new defining simplices of $\L_s$ remain}{ Fathom $s$, STOP! \hfill(Fathoming Rule 3)}
%%    						\EndIf
%    					\EndIf
%    				\EndWhile
%    				\If{$\mathscr{D} = \L_s$}{ Fathom $s$, STOP!\hfill(Fathoming Rule 3)}
%    				\EndIf
					\If{\textsc{FR\_3}$(s,y^1_s,y^2_s)$ = 1}{ Fathom $s$, STOP!\hfill(Fathoming Rule 3)}
    				\EndIf
    			\EndIf
    		\EndIf
    	\Else
    		\State Define the set $\mathcal{I} = \emptyset$.
    		\For{$k \in \{1,2\}$}
    			\If{$\U_s \not\dom (\mathcal{P}^k_s)^{ideal}$}{ add $(\{1,2\}\setminus\{k\})\cup\{\lambda\}$ to $\mathcal{I}$}
    			\EndIf
    		\EndFor
    		%\State Select a set $\mathcal{I} \subset \{1,2,\lambda\}$.
    		\For{each $k \in \mathcal{I}$}{ solve the MILP $\min\{f_k(x): x\in ({X}_s)_I\}$ to find optimal solution $\hat{x}^k$ and obtain $({y}^k_s)_I \in ({Y}^k_s)_I$.}
    			\State Add a local cut to $X_s$ lying on the level curve of $f_k$ associated with the best dual solution.
    			\State Let $N = $ \textsc{GenerateDualBd}$(s(\hat{x}^k))$ and set $\N_s = \nd{\N_s \cup N}$.
    		\EndFor
    		%\State{Generate $D^\mathcal{I}_s=\cup_{k=1}^{p} \left(\left(\mathcal{P}^k_s\setminus \{\cup_{i\in \mathcal{I}}\, {y}^i_s\}\right) \cup \{\cup_{i \in \mathcal{I}}\, ({y}^i_s)_I\}\right)^{ideal}$}
			\If{$\U_s \dom D^\mathcal{I}_s$}{ Fathom $s$, STOP! \hfill(Fathoming Rule 1b)}
			\ElsIf{$\lambda \in \mathcal{I}$}
				\State{Calculate ${H}_s^\lambda$.}
    			\If{$\U_s \dom {H}_s^\lambda$}{ Fathom $s$, STOP! \hfill(Fathoming Rule 2b)}
    			\EndIf
			\EndIf    		
    	\EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm} 

Line 2 of Algorithm~\ref{alg1} is an optional procedure in which we can generate locally valid cutting planes to strengthen the representation of $X_s$ if so desired. We then compute $y_s^1$ and $y_s^2$ on line 3. We then check to see if either of these solutions are integer feasible, and if they are, we generate the dual bound associated with the integer solution in order to update $\N_s$. Furthermore, if both solutions are integer feasible, we check Fathoming Rule 0 on line 6. On line 7 we compute the value $\lambda_s$, the value of the weights on the objectives so that the level curves of $f_\lambda$ have the same slope as the line segment joining $y_s^1$ and $y_s^2$. We then solve the LP associated with $f_\lambda$. If the solution is integer feasible, we again update $\N_s$ as before. On line 9 we check whether or not $y_s^1,y_s^2$ and $y_s^\lambda$ are dominated by $\U_s$. If they are, we proceed to check Fathoming Rules 1a, 2a, and 3. Otherwise, we solve the MILP associated with $f_\lambda$ and $f_k$ for each $k\in\{1,2\}$ such that the ideal point $(\mathcal{P}_s^k)^{ideal}$ is not dominated by $\U_s$. On lines 21 and 22 we utilize the solutions of each MILP to (optionally) add local cuts to $X_s$ and update $\N_s$. Finally, we check Fathoming Rules 1b and 2b. 
%Recognize the use of arbitrary functions \textsc{FR\_0} and \textsc{FR\_3} on lines 5 and 13 of Algorithm \ref{alg1}, respectively. We use \textsc{FR\_0} and \textsc{FR\_3} to represent implementations of Fathoming Rules 0 and 3, respectively. We outline possible implementations of each of these rules in Algorithms \ref{alg4} and \ref{alg3}. However, before introducing these algorithms we briefly disuss another important task that ought to be carried out while processing node $s$: Updating $\N_s$. We do this in two ways: (i) add each integer-feasible line segment discovered while checking Fathoming Rule 0 to $\N_s$, and (ii) for each discovered $x^* \in X_I$, generate the nondominated subset of $\mathcal{Y}(x^*) := \{y = \objf(x): x \in X, x_i = x^*_i \text{ for all } i \in \{m+1,\dots, m+n\}\}$ and add each defining line segment of this set to $\N_s$. Consider the latter of these strategies. Observe that the feasible set of $\mathcal{Y}(x^*)$ can be interpreted as a leaf node of the BB tree, which we denote $s(x^*)$. Hence, the non-dominated subset of $\mathcal{Y}(x^*)$ is precisely $\L_{s(x^*)}$. This leads to a need for generating $\L_{s}$ for a given node $s$. Typical techniques for generating $\L_{s}$ include the multiobjective simplex method and the parametric simplex algorithm (PSA) \citep{ehrgott2005multicriteria}. However, the multiobjective simplex method is far more robust than is necessary for biobjective problems. Also, we found in practice that using the PSA often resulted in many basis changes yielding the same extreme point of $\L_s$ in $\OS$. Since much work is done during the PSA to determine the entering and exiting variables, we found that generating $\L_s$ using the PSA required a significant amount of computational effort. We decided to use an alternative method for generating $\L_s$ which relies on sensitivity analysis. We first solve the single objective LP using objective $f_2$ to obtain $y^2_s$. Next we create the LP $\mathscr{P}_s(\alpha):=\min\{ f_1(x) + \alpha f_2(x): x \in X_s\}$ and then carry out the procedure outlined in Algorithm \ref{alg2}.
%
%\begin{algorithm}[h] 
%%\small
%  \caption{Generate $\L_s$}
%  \label{alg2}
%  \begin{algorithmic}[1]
%    \Function{GenerateDualBd}{$s$} 
%        \State Set $\mathcal{B} = \emptyset$.
%    	\State Solve $\mathscr{P}_s(0)$ to obtain solution $x^*$ and set $y = \objf(x^*)$.
%    	\While{$y \neq y^2_s$}{}
%    		\State Use sensitivity analysis to obtain an interval $[\alpha',\alpha'']$ such that $x^*$ is optimal to $\mathscr{P}_s(\alpha)$ for all $\alpha \in [\alpha',\alpha'']$.
%    		\State Update $x^*$ to be the solution of $\mathscr{P}_s(\alpha'' + \epsilon)$ for an arbitrarily small $\epsilon > 0$.
%    		\If{$\objf(x^*) \neq y$}
%    			\State {Add the line segment connecting $\objf(x^*)$ and $y$ to $\mathcal{B}$. Update $y$ to be $\objf(x^*)$.}
%    		\EndIf
%    	\EndWhile
%    	\State Return $\mathcal{B}$.
%    \EndFunction
%  \end{algorithmic}
%\end{algorithm} 
%
%Recognize that upon termination of Algorithm \ref{alg2}, the set $\mathcal{B}$ contains all defining segments of $\L_s$. We now consider the functions \textsc{FR\_0} and \textsc{FR\_3}, and therefore Fathoming Rules 0 and 3. Note that both of these rules require the generation of a subset of $\L_s$ for some node $s$ and thus the procedure outlined in Algorithm \ref{alg2} serves as a good foundation for these methods. However, for each of these rules it is often unnecessary to generate $\L_s$ entirely. The generation of $\L_s$ should cease if: (i) one is checking Fathoming Rule 0 and a defining line segment of $\L_s$ is generated that is not integer feasible, or (ii) one is checking Fathoming Rule 3 and a defining line segment of $\L_s$ is generated that is not dominated by $\U_s$. Hence, the procedures in Algorithm \ref{alg2} can be modified in order to develop strategies for checking Fathoming Rules 0 and 3. These strategies are outlined in Algorithms \ref{alg4} and \ref{alg3}, respectively.
%
%\begin{algorithm}%[h] 
%%\small
%  \caption{Fathoming Rule 0\\
%  \underline{Input}: Node $s$ and solutions $y^1_s$ and $y^2_s$.\\
%  \underline{Output}: 1 if node $s$ should be fathomed, 0 otherwise.}
%  \label{alg4}
%  \begin{algorithmic}[1]
%    \Function{FR\_0}{$s, y^1_s, y^2_s$} 
%    	\State $y^1_s$ is the solution to $\mathscr{P}_s(0)$. Let $x^*$ represent the preimage of $y^1_s$. Set $y = y^1_s$. %and set $y = \objf(x^*)$.
%    	\If{$y = y^2_s$}{ return 1}%STOP! Fathom node $s$.}
%    	\Else
%    		
%    	\While{$y \neq y^2_s$}{}
%    		\State Use sensitivity analysis to obtain an interval $[\alpha',\alpha'']$ such that $x^*$ is optimal to $\mathscr{P}_s(\alpha)$ for all $\alpha \in [\alpha',\alpha'']$.
%    		\State Update $x^*$ to be the solution of $\mathscr{P}_s(\alpha'' + \epsilon)$ for an arbitrarily small $\epsilon > 0$.
%    		\If{$\objf(x^*) \neq y$}
%    			\State {Let $\mathbb{S}$ represent the line segment connecting $\objf(x^*)$ and $y$.}
%    			\If{$\mathbb{S} \not\subset (Y_s)_I$}{ return 0}%STOP! Do not fathom node $s$.}
%    			\Else{ Update $y$ to be $\objf(x^*)$.}
%    			\EndIf
%    		\EndIf
%    	\EndWhile
%    	\State{return 1}%Fathom node $s$.}
%    	\EndIf
%    \EndFunction
%  \end{algorithmic}
%\end{algorithm} 
%
%\begin{algorithm}%[h] 
%%\small
%  \caption{Fathoming Rule 3\\
%  \underline{Input}: Node $s$ and solutions $y^1_s$ and $y^2_s$.\\
%  \underline{Output}: 1 if node $s$ should be fathomed, 0 otherwise.}
%  \label{alg3}
%  \begin{algorithmic}[1]
%    \Function{FR\_3}{$s, \cup_{k=1}^{2}y^k_p$} 
%    	\State $y^1_s$ is the solution to $\mathscr{P}_s(0)$. Let $x^*$ represent the preimage of $y^1_s$. Set $y = y^1_s$. %and set $y = \objf(x^*)$.
%    	\If{$y = y^2_s$}
%    		\If{$\U_s \dom y$}{ return 1}%STOP! Fathom node $s$.}
%    		\Else{ return 0}
%    		\EndIf
%    	\Else
%    		
%    	\While{$y \neq y^2_s$}{}
%    		\State Use sensitivity analysis to obtain an interval $[\alpha',\alpha'']$ such that $x^*$ is optimal to $\mathscr{P}_s(\alpha)$ for all $\alpha \in [\alpha',\alpha'']$.
%    		\State Update $x^*$ to be the solution of $\mathscr{P}_s(\alpha'' + \epsilon)$ for an arbitrarily small $\epsilon > 0$.
%    		\If{$\objf(x^*) \neq y$}
%    			\State {Let $\mathbb{S}$ represent the line segment connecting $\objf(x^*)$ and $y$.}
%    			\If{$\U_s \not\dom \mathbb{S}$}{ return 0}%STOP! Do not fathom node $s$.}
%    			\Else{ Update $y$ to be $\objf(x^*)$.}
%    			\EndIf
%    		\EndIf
%    	\EndWhile
%    	\State{return 1}%Fathom node $s$.}
%    	\EndIf
%    \EndFunction
%  \end{algorithmic}
%\end{algorithm} 

Two additional tasks are performed while processing each node. 

\subsection{Objective space fathoming}
After processing each node, we perform an additional type of fathoming which we refer to as \emph{objective-space fathoming}. After updating $\N_s$, we impose bounds on $f_1$ and $f_2$ which ``cut off'' portions of $\OS$ in which we have discovered that $\U_s \dom (Y_s)_I$. In certain cases the remaining subset of $\OS$ %in which we have \emph{not} discovered that $\U_s \dom (Y_s)_I$ 
consists of disjoint regions. When this is the case, we implement objective-space fathoming by branching on $f_1$ and $f_2$ bounds which generate the desired disjunctions in $\OS$. In these cases, objective-space fathoming resembles the ``Pareto branching'' of \citep{stidsen2014branch} and ``objective branching'' of \citep{parragh2015boip}. 

\subsection{Bound tightening}\label{sec:tight}
In order to increase the likelihood of fathoming, we utilize a few different strategies for tightening the bound $\L_s$. The first strategy we use is the generation of locally valid cutting planes. We do this in two ways: (i) we generate discjuntive cuts based on disjunctions observed in $\OS$ when performing $\OS$ fathoming, and (ii) we convert the BOLP relaxation associated with $s$ to the BOMILP $\min\{f_\lambda(x): x\in ({X}_s)_I\}$, allow the MILP solver to process its root node, and add all cuts generated by this solver as local cuts to $s$ as local cuts. It is widely accepted that for single objective MILPs, locally valid cutting planes are not particularly helpful for improving the performance of BB. However, locally valid cutting planes can have a significantly greater impact on BOMILPs. To see this, observe Figure \ref{local_cuts}. Assume that Figure \ref{local_cuts_1} displays an instance of BOMILP for which the $(f_1,f_2)$-space and the $X$-space are one and the same, i.e., this instance contains only two variables $y_1$ and $y_2$, both integer, and $f_1 = y_1$ and $f_2 = y_2$. The constraints of this instance yield the blue polytope, and the integer lattice is indicated by the black dots. The red dots represent the Pareto-optimal solutions. Suppose that branching is performed as shown in Figure \ref{local_cuts_2}. Notice that all Pareto optimal solutions in the left branch can be revealed by a single locally valid cutting plane, as shown by the red dashed line in Figure \ref{local_cuts_3}. Also notice that this could never be accomplished through the use of globally valid cuts.

\begin{figure}
\begin{subfigure}[h]{.32\textwidth}
\centering
\includegraphics[width=0.97\textwidth]{local_cuts_1}
\caption{Example instance of BOMILP}
\label{local_cuts_1}
\end{subfigure} 
\begin{subfigure}[h]{.33\textwidth}
\centering
\includegraphics[width=.97\textwidth]{local_cuts_2}
\caption{After branching}
\label{local_cuts_2}
\end{subfigure}
\begin{subfigure}[h]{.32\textwidth}
\centering
\includegraphics[width=.97\textwidth]{local_cuts_3}
\caption{Locally valid cut}
\label{local_cuts_3}
\end{subfigure}
\caption{An example showing the usefulness of locally valid cuts for BOMILP}
\label{local_cuts}
\end{figure}



\subsection{Comparison with another \bb{} algorithm} \label{sec:compare}
We highlight some key differences regarding the node processing step between our \bb{} and that of \citet{belotti2012biobjective,belotti2016fathoming}, which is the only other \bb{} method for general BOMILP. There are also differences in the other components of \bb{}, %way presolve and preprocessing (\textsection\ref{sec:pre}), branching, objective space splitting, and gap measurement (\textsection\ref{sec:multi_BB}) are carried out, 
but that is not of concern here.

The two methods differ in the way fathoming rules are implemented. Firstly, we utilize the data structure of \citet{treestructure} to store and dynamically update the set $\N_s$ throughout the BB process. In \citep{belotti2012biobjective,belotti2016fathoming}, fathoming rules are checked at a node $s$ of the BB tree by: 
\begin{enumerate}
\item using $\N_s$ to generate $\U_s$ by adding a set of local nadir points to $\N_s$, 
\item selecting the  subset $\mathcal{R} := \U_{s}\cap ((Y_s)^{ideal} + \nonneg)$,
\item solving auxiliary LPs to determine whether $\mathcal{R}$ and $\L_{s}$ can be separated by a hyperplane.
\end{enumerate}
Node $s$ is then fathomed if $\mathcal{R} = \emptyset$ or if a separating hyperplane is found. Note that these procedures amount to comparing each element of the primal bound with the dual bound as a whole by solving at most one LP for each element of the primal bound.

In this paper, we utilize the opposite approach to fathoming. Rather than comparing each element of the primal bound with the dual bound as a whole, we compare each element of the dual bound with the primal bound as a whole. Additionally, instead of making these comparisons by solving LPs, we exploit the following guarantee of the data structure of \citep{treestructure}: a point or line segment inserted to the structure is added to the structure if and only if the point or segment is not dominated by the data already stored in the structure. Hence, we implement an extra function \textsc{IsDominated}($\cdot$) alongside this data structure which returns 1 if the input is dominated by $\N_s$ and 0 otherwise. We then implement our fathoming rules 1-3 by passing the appropriate sets ($\mathcal{P}_s, H_s^\lambda, D_s^\mathcal{I}, \tilde{H}_s^\lambda$ and $\L_s$) to \textsc{IsDominated}. If a 1 is returned for any of these sets, we fathom, otherwise we do not. It is difficult to comment on whether solving LPs or utilizing a function call to a data structure is more efficient for checking fathoming. However, we have found in practice that for a particular node $s$ of the BB tree, the primal bound $\U_s$ typically contains far more points and segments than the dual bound $\L_s$. Thus, comparing each element of the dual bound with the primal bound as a set seems to be a more efficient procedure than doing it the opposite way.

%We note that in preparing the work \citet{treestructure}, we performed some initial tests in which we compared the performance of our BB with that of \citet{belotti2012biobjective}. We found that the implementation of \citet{belotti2012biobjective} was somewhat incomplete and for this reason the CPU times we obtained using our BB were significantly lower than those of \citet{belotti2012biobjective}. For this reason, we do not perform computational tests comparing our BB with that of \citet{belotti2012biobjective} in this work.

We now discuss the extension of the remaining major aspects of \bb{} to the biobjective setting.

\section{Biobjective BB}\label{sec:multi_BB}
%In this work we present a BB procedure designed for solving instances of BOMILP. Throughout the next several sections 
In this section we discuss the specifics of how the different components of single objective BB ---  presolve/preprocessing, node processing, and branching, can each be extended to the biobjective setting. We then briefly discuss optional additions to our basic biobjective BB procedure. %We will now briefly discuss properties of each of these which must be modified for a multiobjective problem.\\~\\

%\subsection{Presolve}\label{presolve}
%Although primal information is not modified by the addition of additional objective functions, dual information is. For this reason, much care must be taken during presolve to ensure that no Pareto solution is lost due to any simplifications made to an instance of BOMILP.


%\subsection{Additional Notes on Node processing}\label{more_np} 
%
%Recall that we discussed the major aspects of node processing in Section \ref{np}. Here we discuss a few additional, though non-essential, tasks that we perform while processing a node.
%
%\subsubsection{Objective Space Fathoming} After processing a node, we perform an additional type of fathoming which we refer to as \emph{objective-space fathoming}. After updating $\N_s$, we impose bounds on $f_1$ and $f_2$ which ``cut off'' portions of $\OS$ in which we have discovered that $\U_s \dom (Y_s)_I$. In certain cases the remaining subset of $\OS$ %in which we have \emph{not} discovered that $\U_s \dom (Y_s)_I$ 
%consists of disjoint regions. When this is the case, we implement objective-space fathoming by branching on $f_1$ and $f_2$ bounds which generate the desired disjunctions in $\OS$. In these cases, objective-space fathoming resembles the ``Pareto branching'' of \citep{stidsen2014branch} and ``objective branching'' of \citep{parragh2015branch}. 
%
%%We now discuss the implementation of objective space fathoming. If node $s$ cannot be fathomed, then the following procedure is repeated for each $y \in \{y^{s,\, nw},y^{s,\,se},y^{s,\,\lambda}\}$ for which $\N_s \dom y$. Sensitivity analysis on $\lambda$ is used to pivot to the basis or bases which yield the extreme points of $\L_s$ which are adjacent to $y$. This reveals some $\S \in \L_s$. If $\N_s \dom \S$, the portion of $\OS$ dominated by $\S$ is removed, and sensitivity analysis is used again to generate the next $\S \in \L_s$. Once $\S \in \L_s$ is discovered for which $\N_s \not \dom \S$, the process ceases.
%
%\subsubsection{Bound Tightening} \label{bound_tightening}
%
%In order to increase the likelihood of fathoming, we utilize a few different strategies for tightening the bound $\L_s$. The first strategy we use is the generation of locally valid cutting planes. We do this in two ways: (i) we generate discjuntive cuts based on disjunctions observed in $\OS$ when performing $\OS$ fathoming, and (ii) we convert the BOLP relaxation associated with $s$ to the BOMILP $\min\{f_\lambda(x): x\in ({X}_s)_I\}$, allow CPLEX to process its root node, and add all cuts generated by CPLEX for this BOMILP to $s$ as local cuts. 
%
%It is widely accepted that for single objective MILPs, locally valid cutting planes are not particularly helpful for improving the performance of BB. However, locally valid cutting planes can have a significantly greater impact on BOMILPs. To see this, observe Figure \ref{local_cuts}. Assume that Figure \ref{local_cuts_1} displays an instance of BOMILP for which the $(f_1,f_2)$-space and the $X$-space are one and the same, i.e., this instance contains only two variables $y_1$ and $y_2$, both integer, and $f_1 = y_1$ and $f_2 = y_2$. The constraints of this instance yield the blue polytope, and the integer lattice is indicated by the black dots. The red dots represent the Pareto-optimal solutions. Suppose that branching is performed as shown in Figure \ref{local_cuts_2}. Notice that all Pareto optimal solutions in the left branch can be revealed by a single locally valid cutting plane, as shown by the red dashed line in Figure \ref{local_cuts_3}. Also notice that this could never be accomplished through the use of globally valid cuts.
%%Figure (2b) provides an illustration of the usefulness of local cuts. Global cuts cannot exist north-west of the dotted black line, but a possible local cut is indicated by the green line. Although this line does not cut off $p^1_s$ or $p^2_s$, it does cut off a large portion of $\L_s$.
%
%\begin{figure}
%\begin{subfigure}[h]{.32\textwidth}
%\centering
%\includegraphics[width=0.97\textwidth]{local_cuts_1}
%\caption{Example instance of BOMILP}
%\label{local_cuts_1}
%\end{subfigure} 
%\begin{subfigure}[h]{.33\textwidth}
%\centering
%\includegraphics[width=.97\textwidth]{local_cuts_2}
%\caption{After branching}
%\label{local_cuts_2}
%\end{subfigure}
%\begin{subfigure}[h]{.32\textwidth}
%\centering
%\includegraphics[width=.97\textwidth]{local_cuts_3}
%\caption{Locally valid cut}
%\label{local_cuts_3}
%\end{subfigure}
%\caption{An example showing the usefulness of locally valid cuts for BOMILP}
%\label{local_cuts}
%\end{figure}

%Another way we attempt to increase the likelihood of fathoming is by occasionally solving single objective MILPs in place of LPs, as shown in line 17 of Algorithm \ref{alg1}. %In cases in which a node $s$ cannot be fathomed, each of the single objective LPs solved to generate our initial approximation of $\L_s$ can be replaced by single objective MILPs. After solving these MILPs, improved ideal points and an improved ideal segment can be generated which must dominate all integer-feasible solutions that are discoverable at any node which is a child of $s$. Therefore if these points, or the segment, are dominated by $\N_s$, node $s$ can be fathomed. 
%Although solving these MILPs can be quite an expensive endeavour, we always utilize all CPLEX MILP starts associated with every $x\in X_I$ found throughout BB. Additionally, we always keep track of the children of $s$ at which these single objective MILP solutions remain feasible. In this way, MILPs do not need solved at every node. In order to minimize any negative affect of solving MILPs at a large number of nodes, we also set a solution time limit of one minute on each MILP. Additionally, after each MILP is solved we generate the level curve of the objective function used which is associated with the best known dual bound, and add a locally valid cutting plane passing through this level curve. Thus, information found during each MILP solve is useful even if the MILP is not solved to optimality. 

%Processing a node $s$ of the BB tree consists of three basic steps: 
%\begin{enumerate}
%\item Generate (or approximate) the dual bound $\L_s$.
%\item Compare $\L_s$ (or its approximation) with $\U_s$ to determine if node $s$ can be fathomed.
%\item If $s$ is not fathomed in Step 2, strengthen $\L_s$ and repeat Step 2.
%\end{enumerate}
%
%%Note that methods for strengthening $\L_s$ may vary greatly or be highly dependent on the number of objectives. However, one technique which can be used in any case is the addition of globally and/or locally valid cutting planes to the subproblem associated with $s$.\\~\\
%Though there may be a variety of methods for accomplishing the tasks outlined above, we propose one possibility here. In order to establish this method, though, we first introduce several new definitions and some propositions. Define the following: 
%\begin{itemize}
%\item $H_s$ -- the hyperplane in $\OS$ containing ${y}^k_s \in Y^k_s$ for each $k \in \{1,\dots,p\}$
%\item $\lambda^s$ -- the normal vector for $H_s$ having all nonnegative components
%\item  $f_{\lambda^s}(x) = \sum_{i=1}^{p}{\lambda^s_i} f_i(x)$ 
%\item $(Y^\lambda_{s})_I = \{y\in Y_I:y={\bf{f}}(x),x=\argmin\{f_{\lambda^s}(x): x\in (X_s)_I\}\}$
%\item $(y^\lambda_s)_I$ -- an arbitrary element of $(Y^\lambda_s)_I$
%%\item For each $k \in \{1,\dots,p\}$, let $\mathcal{P}^k_s = \left(\cup_{i \neq k} y^i_s\right) \cup y^\lambda_s$
%%\item $\tilde{Y}^\lambda_s = \{y\in \tilde{Y}:y={\bf{f}}(x),x=\argmin_{x\in \tilde{X}_s}\{f_{\lambda^s}(x)\}\}$
%%\item $\tilde{y}^\lambda_s$ -- an arbitrary element of $\tilde{Y}^\lambda_s$
%\item For each $k \in \{1,\dots,p\}$, let $\mathcal{P}^k_s = \left(\cup_{i \neq k} \tilde{y}^i_s\right) \cup \tilde{y}^\lambda_s$
%\item $H_s^\lambda$ -- the intersection between $((\L_s)^{ideal} + \R^p_{\geq})$ and the hyperplane with normal vector $\lambda^s$, passing through $(y^\lambda_s)_I$
%\item $\tilde{H}_s^\lambda$ -- the intersection between $((\L_s)^{ideal} + \R^p_{\geq})$ and the hyperplane with normal vector $\lambda^s$, passing through ${y}^\lambda_s$
%\item Given any $\mathcal{I} \subset \{1,\dots,p,\lambda\}$, define $D^\mathcal{I}_s:=\cup_{k=1}^{p} \left(\left(\mathcal{P}^k_s\setminus \{\cup_{i\in \mathcal{I}}\, {y}^i_s\}\right) \cup \{\cup_{i \in \mathcal{I}\setminus\{k\}}\, ({y}^i_s)_I\}\right)^{ideal}$
%\end{itemize}
%
%Observe the following propositions which provide fathoming rules for node $s$.
%
%\begin{prop}\label{fath0} (Fathoming Rule 0) --
%If $\L_s \subset (Y_s)_I$ node $s$ can be fathomed.
%\end{prop}
%
%Proposition \ref{fath0} expresses the idea of fathoming due to optimality.
%
%\begin{prop}\label{fath1}
%Node $s$ of the BB tree can be fathomed if:
%\[\left(\U_s \dom \cup_{k=1}^{p} \left(\mathcal{P}^k_s\right)^{ideal}\right) \bigvee \left(\U_s \dom \tilde{H}_s^\lambda\right) \bigvee \left(\U_s \dom D^\mathcal{I}_s \text{ for } \mathcal{I} \subset \{1,\dots,p,\lambda\}\right) \bigvee \left(\U_s \dom {H}_s^\lambda\right)\]
%\end{prop}
%
%Notice that Proposition \ref{fath1} specifies four additional cases in which a node $s$ can be fathomed, listed from easiest to most difficult to verify. Hence Proposition \ref{fath1} outlines four fathoming rules, which we refer to as \emph{Fathoming Rules 1a, 2a, 1b}, and \emph{2b}, respectively. There is also one additional fathoming rule which is described in the next proposition.
%
%%\begin{prop} (Fathoming Rule 1a) --
%%For any node $s$ of the BB tree, $\cup_{k=1}^{p} \left(\mathcal{P}^k_s\right)^{ideal}$ dominates $\L_s$. Hence, if $\U_s$ dominates $\cup_{k=1}^{p} \left(\mathcal{P}^k_s\right)^{ideal}$, node $s$ can be fathomed.
%%\end{prop}
%%
%%\begin{prop} (Fathoming Rule 1b) --
%%For any node $s$ of the BB tree and any $I \subset \{1,\dots,p,\lambda\}$ the set $D^I_s:=\cup_{k=1}^{p} \left(\left(\mathcal{P}^k_s\setminus \{\cup_{i\in I} \tilde{y}^i_s\}\right) \cup \{\cup_{i \in I} {y}^i_s\}\right)^{ideal}$ forms a valid dual bound for node $s$. Hence, if $\U_s$ dominates $D^I_s$, node $s$ can be fathomed.
%%\end{prop}
%%
%%\begin{prop} (Fathoming Rule 2a) --
%%For any node $s$ of the BB tree, $\tilde{H}_s^\lambda$ dominates $\L_s$. Hence, if $\U_s$ dominates $\tilde{H}_s^\lambda$, node $s$ can be fathomed.
%%\end{prop}
%%
%%\begin{prop} (Fathoming Rule 2b) --
%%For any node $s$ of the BB tree ${H}_s^\lambda$ forms a valid dual bound for node $s$. Hence, if $\U_s$ dominates ${H}_s^\lambda$, node $s$ can be fathomed.
%%\end{prop}
%
%\begin{prop}\label{fath3} (Fathoming Rule 3) --
%If $\U_s$ dominates each defining simplex of $\L_s$, then $\U_s$ dominates $\L_s$ and consequently node $s$ can be fathomed.
%\end{prop}
%
%We now introduce Algorithm \ref{alg1} which can be used to process node $s$. %Note that at any point during Algorithm \ref{alg1} one could add valid cutting planes for $X_s$ to $\tilde{X}_s$ in order to increase the likelihood of fathoming.
%
%\begin{algorithm}[h] 
%%\small
%  \caption{Process node $s$}
%  \label{alg1}
%  \begin{algorithmic}[1]
%    \Function{ProcessNode}{$s$} 
%    	\State Compute valid cutting planes for $(X_s)_I$ and add them to the description of $X_s$.
%    	\For{$k \in \{1,\dots,p\}$}{ Solve $\min\{f_k(x): x\in {X}_s\}$ to generate ${y}^k_s \in {Y}^k_s$.}
%    	\EndFor
%    	\If{$y^k_s \in (Y_s)_I$ for all $k \in \{1,\dots, p\}$}
%    		\If{\textsc{FR\_0}$(s, \cup_{k=1}^{p} y^k_s)$ = 1}{ Fathom $s$, STOP!\hfill(Fathoming Rule 0)}
%    		\EndIf
%    	\EndIf
%    	\State{Calculate $H_s$ and $\lambda^s$ using ${y}^k_s$ for all $k \in \{1,\dots,p\}$. Solve $\min\{f_\lambda(x): x\in {X}_s\}$ to generate ${y}^\lambda_s \in {Y}^\lambda_s$.}
%    	\If{$\U_s \dom {y}^k_s$ for each $k$ and $\U_s \dom {y}^\lambda_s$}
%    		\If{$\U_s \dom \cup_{k=1}^{p} \left(\mathcal{P}^k_s\right)^{ideal}$}{ Fathom $s$, STOP! \hfill(Fathoming Rule 1a)}
%    		\Else
%    			\State{Calculate $\tilde{H}_s^\lambda$.}
%    			\If{$\U_s \dom \tilde{H}_s^\lambda$}{ Fathom $s$, STOP! \hfill(Fathoming Rule 2a)}
%    			\Else
%%    				\State{Define $\mathscr{D} = \emptyset$.}
%%    				\While{$\mathscr{D} \neq \L_s$}{}
%%    					\State{Generate a defining simplex $\mathcal{S}$ of $\L_s$ where $\mathcal{S} \not\in \mathscr{D}$; add $\mathcal{S}$ to $\mathscr{D}$.}
%%    					\If{$\U_s \not\dom \mathcal{S}$}{ break}
%%%    						\If{No new defining simplices of $\L_s$ remain}{ Fathom $s$, STOP! \hfill(Fathoming Rule 3)}
%%%    						\EndIf
%%    					\EndIf
%%    				\EndWhile
%%    				\If{$\mathscr{D} = \L_s$}{ Fathom $s$, STOP!\hfill(Fathoming Rule 3)}
%%    				\EndIf
%					\If{\textsc{FR\_3}$(s, \cup_{k=1}^{p}y^k_s)$ = 1}{ Fathom $s$, STOP!\hfill(Fathoming Rule 3)}
%    				\EndIf
%    			\EndIf
%    		\EndIf
%    	\Else
%    		\State Select a set $\mathcal{I} \subset \{1,\dots,p,\lambda\}$.
%    		\For{each $k \in \mathcal{I}$}
%    			\State{Solve the MILP $\min\{f_k(x): x\in ({X}_s)_I\}$ to get $({y}^k_s)_I \in ({Y}^k_s)_I$.}
%    		\EndFor
%    		%\State{Generate $D^\mathcal{I}_s=\cup_{k=1}^{p} \left(\left(\mathcal{P}^k_s\setminus \{\cup_{i\in \mathcal{I}}\, {y}^i_s\}\right) \cup \{\cup_{i \in \mathcal{I}}\, ({y}^i_s)_I\}\right)^{ideal}$}
%			\If{$\U_s \dom D^\mathcal{I}_s$}{ Fathom $s$, STOP! \hfill(Fathoming Rule 1b)}
%			\ElsIf{$\lambda \in \mathcal{I}$}
%				\State{Calculate ${H}_s^\lambda$.}
%    			\If{$\U_s \dom {H}_s^\lambda$}{ Fathom $s$, STOP! \hfill(Fathoming Rule 2b)}
%    			\EndIf
%			\EndIf    		
%    	\EndIf
%    \EndFunction
%  \end{algorithmic}
%\end{algorithm} 
%
%Recognize the use of arbitrary functions \textsc{FR\_0} and \textsc{FR\_3} occurring on lines 5 and 13 of Algorithm \ref{alg1}, respectively. The reason we use these functions is that there are many ways to implement Fathoming Rules 0 and 3. In Section (enter section number, currently Appendix) we provide outlines for implementations of each of these rules in Algorithms \ref{alg3} and \ref{alg4}. Also notice the arbitrary selection of the set $\mathcal{I}$ on line 15. Any set can be used here to generate $D^{\mathcal{I}}_s$, the set of strengthened ideal points, and one's strategy for selecting $\mathcal{I}$ may depend on $p$. We discuss such a strategy for $p=2$ in Section \ref{implementation}. Also note that another important task that ought to be carried out while processing node $s$ is updating $\N_s$. Two ways this can be done are: (i) add each integer-feasible simplex discovered while checking Fathoming Rule 0 to $\N_s$, and (ii) for each discovered $x \in X_I$, generate the Pareto set of $\P(x)$ and add each defining simplex of this set to $\N_s$.
%
%%\begin{algorithm}%[H] 
%%%\small
%%  \caption{Update $\N_s$}
%%  \label{alg2}
%%  \begin{algorithmic}[1]
%%    \Function{UpdatePrimal}{$x \in X$} 
%%    	\State Generate Pareto set of $\P(x)$ and add all defining simplices to $\N_s$.
%%    	\If{$y = \objf(x) \in \L_s$}{ Generate $\mathcal{A}^y_s$.}
%%    	
%%    	\EndIf
%%    \EndFunction
%%  \end{algorithmic}
%%\end{algorithm} 

\subsection{Branching}\label{sec:branch}
In general, any rule for selecting a branching variable is permissible. However, it should be noted that for BOMILP several $y \in {Y}$, and consequently several $x\in {X}$, may be discovered while processing a node $s$. In fact, our implementation requires solving at least three LPs at each node. Since the variables may take on different values at each solution, it is possible that an integer variable takes a fractional value at some of these solutions and not at others. Because of this, we use a scoring scheme for branching in which each integer variable is given a score. Of the variables with the highest score, the one with the highest index is selected for branching. The score of $x_i$ is increased if: (i) $x_i$ is fractional at the LP solution associated with objective $f^k$, $k \in \{1,2,\lambda^s\}$, (ii) $x_i$ changes value at a pivoting step of Algorithm \ref{alg4}, or (iii) multiple single objective MILPs are solved to optimality at $s$ and $x_i$ takes different values for at least two of the MILP solutions.

After a branching decision has been made we utilize probing, as introduced in Proposition \ref{probe}, to strengthen bounds on each variable for both of the resulting subproblems. We do this for several reasons: (i) we may find during this process that our branching decision results in an infeasible subproblem, in which case we can discard the infeasible subproblem, enforce that the variable bounds associated with the feasible subproblem be satisfied at any child node of $s$, and choose a new branching variable; (ii) because much work in biobjective BB is dedicated to fathoming, we want to generate the strongest dual bound possible, which probing helps us to do; (iii) since processing a node in biobjective BB is an expensive operation, we seek to limit the number of nodes explored and probing aids in this endeavor by reducing the number of possible future branching decisions. We found during testing that this probing scheme at each node was extremely powerful, both in reducing the number of nodes processed during BB as well as overall running time. See Table \ref{table_presolve} in Section \ref{sec:compute} for evidence of this.


\subsection{Exploiting gaps in $\OS$}\label{sec:osgap}

Due to the noncontinuous, nonconvex nature of the Pareto set of a BOMILP, there are occasionally large gaps between Pareto solutions in $\OS$. If this occurs, the likelihood that $\L_s \subseteq \U_s$ is significantly decreased for each node. Hence, this can result in an extreme amount of computational effort which yields no additional Pareto solutions. One way to combat this issue is to observe the solutions obtained during Preprocessing and record locations in $\OS$ where large gaps exist between discovered solutions. One can then split $\OS$ into a series of subregions based on the locations of these gaps and solve single objective MILPs (using objectives $f_1$ and $f_2$) within each subregion in order to remove locations containing no Pareto solutions. Afterwards BB can be run in each subregion rather than over the entire $\OS$. To aid in understanding this idea, observe Figure \ref{gaps_fig}. Here Pareto solutions are shown in blue and subregions in $\OS$ are indicated by green dashed lines.

\begin{figure}
\begin{subfigure}[h]{.32\textwidth}
\centering
\includegraphics[width=\textwidth]{gaps}
\caption{Gaps}
\label{gaps1}
\end{subfigure} 
\begin{subfigure}[h]{.32\textwidth}
\centering
\includegraphics[width=\textwidth]{gaps9}
\caption{Slitting $\OS$}
\label{gaps2}
\end{subfigure}
\begin{subfigure}[h]{.32\textwidth}
\centering
\includegraphics[width=\textwidth]{gaps10}
\caption{Reducing the subregions}
\label{gaps3}
\end{subfigure}
\caption{Large gaps between solutions in $\OS$}
\label{gaps_fig}
\end{figure}

\subsection{Measuring Performance} \label{sec:gap}

In single objective BB, one can terminate the procedure at any time and obtain a measure of the quality of the best known solution in terms of the gap between this solution and the best known dual bound. %As far as we are aware, 
We propose a similar scheme for biobjective BB. %Hence, we propose one here. %Assume that BB is terminated prematurely and let $s^*$ represent the last node processed prior to termination. 
Let $\mathscr{O}_{s^*}$ represent the set of open nodes after a node $s^*$ has been processed. After processing $s^*$, the global dual bound, denoted $\mathcal{DB}_{s^*}$, is the nondominated subset of $\left(\cup_{s\in\mathscr{O}_{s^*}}\L_s\right)$. Therefore, if BB is terminated after $s^*$ is processed, the performance of BB can be quantified by measuring the distance between $\mathcal{DB}_{s^*}$ and $\U_{s^*}$. One natural metric to use for measuring this distance %between $\mathcal{DB}_{s^*}$ and $(\U_{s^*}+\nonneg)$ 
is the Hausdorff metric: \[d_H(\mathcal{DB}_{s^*},\U_{s^*}) := \max \left\{\sup_{i\in \mathcal{DB}_{s^*}}\inf_{j\in \U_{s^*}} d(i,j),\, \sup_{j\in \U_{s^*}}\inf_{i\in \mathcal{DB}_{s^*}} d(i,j) \right\}.\] % \citep{hausdorff1914grundzuge}.% Under this metric, the distance between two sets $I$ and $J$ is $d_H(I,J) := \max \{\sup_{i\in I}\inf_{j\in J} d(i,j), \sup_{j\in J}\inf_{i\in I} d(i,j) \}$, where $d(\cdot,\cdot)$ represents a distance defined by a metric appropriate for the given sets $I$ and $J$. Here, since we are measuring the distance between two subsets of $\R^2$, we take $d(\cdot,\cdot)$ to be Euclidean distance. 
Unfortunately the nonconvex nature of $\U_s$ makes the Hausdorff metric difficult to use %in this situation 
since it cannot be computed using a linear program. In our implementation $\U_{s^*}$ is stored as the individual line segments and singletons comprising $\N_{s^*}$ using the data structure of \citep{treestructure}. $\mathcal{DB}_{s^*}$ is computed by generating the points and line segments comprising its nondominated subset, which are also stored using the same data structure. Thus, rather than explicitly computing $d_H(\mathcal{DB}_{s^*},\U_{s^*})$, we instead compute 
\begin{equation}
\mathcal{G}_{s^*}:=\max\{d_H(\mathcal{DB}_{s^*},\mathcal{S}+\nonneg): \mathcal{S} \in \N_{s^*}\}
\end{equation}
via pairwise comparison of the points and line segments comprising $\mathcal{DB}_{s^*}$ and $\N_{s^*}$. Clearly, $\mathcal{G}_{s^*}$ is a upper bound on $d_H(\mathcal{DB}_{s^*},\U_{s^*})$. Recognize, though, that $\mathcal{G}_{s^*}$ is an absolute measurement and so it is difficult to use to compare the performance of BB on multiple instances of BOMILP. Thus, in practice we use a percentage calculated as 
\begin{equation}
\overline{\mathcal{G}_{s^*}} := 100\times\frac{\left|\max\{y^2_1-y^1_1,y^1_2-y^2_2\} - \mathcal{G}_{s^*}\right|}{\max\{y^2_1-y^1_1,y^1_2-y^2_2\}}.
\end{equation}

Another method for measuring the distance between $\mathcal{DB}_{s^*}$ and $\U_{s^*}$ is to compute a so called \emph{hypervolume gap}. Let $hv(\cdot)$ denote the area of subset of $\R^2$. Then the hypervolume gap between $\mathcal{DB}_{s^*}$ and $\U_{s^*}$ is
\begin{equation}
\mathcal{HV}_{s^*} := 100\times\frac{hv((\mathcal{DB}_{s^*} + \nonneg) \cap \OS) - hv(\U_{s^*} \cap \OS)}{hv((\mathcal{DB}_{s^*} + \nonneg) \cap \OS)}.
\end{equation} 
See, for example, \citet{zitzler2003performance}). A similar measure is used to assess the quality of approximations to the Pareto sets of BOMILP instances in \citep{boland2015acriterion}. 

Recognize that the Hausdorff and hypervolume gap measurements play significantly different roles. The hypervolume gap provides a measure of the proximity of the dual bound to the primal bound throughout the entirety of $\OS$, while the Hausdorff gap provides a measure of the proximity of the dual and primal bounds in the location at which they are furthest apart. Hence, we can interpret the Hausdorff gap as a worst-case measurement and the hypervolume gap as a sort of average-case measurement. We note that in our initial tests we utilize both the Hausdorff and hypervolume measurements so that our results can be compared with other works, such as \citep{boland2015acriterion}, which use the hypervolume gap. However, since the Hausdorff gap provides a worst-case measure and is therefore more robust, we do not use the hypervolume gap measurement in our final set of experiments.


Finally, we close this section by providing a pseudocode of our \bb{} procedure in Algorithm \ref{alg7}. 

\begin{algorithm}[h!] 
  \caption{\bb{} for BOMILP.\\
  \underline{Input}: An instance $\mathcal{I}$ of BOMILP.\\
  \underline{Output}: The Pareto set of instance $\mathcal{I}$.}
  \label{alg7}
  \begin{algorithmic}[1]
    \Function{BBsolve}{$\mathcal{I}$} 
    	\State Set $\mathscr{L} = \emptyset$.
    	\State Use primal presolve, biobjective duality fixing and exploitation of singleton and dominating columns to simplify $\mathcal{I}$.
    	\For{$k \in \{1,2\}$}{ solve the MILP $\min\{f_k(x): x\in {X}_I\}$ to obtain ${y}^k_I \in {Y}_I$.}
    	\EndFor
    	\State Select $\rho \geq 0$ and run either \textsc{PreprocessingMethod1}$(y^1_I,y^2_I,\rho)$ or \textsc{PreprocessingMethod2}$(y^1_I,y^2_I,\rho)$ to return $\N_0$.
    	\State Perform probing to further simplify $\mathcal{I}$.
    	\State Add the continuous relaxation of $\mathcal{I}$ to $\mathscr{L}$.
    	\While{$\mathscr{L} \neq \emptyset$}{ select $s$ from $\mathscr{L}$.}
    		\State Run \textsc{ProcessNode}$(s)$.
    		\If{$s$ is not fathomed}{ perform $\OS$ fathoming.}
    			\If{the nondominated portion of $\OS$ consists of disjoint regions}{ perform Pareto branching. Add the resulting subproblems to $\mathscr{L}$.}
    			\Else{ select the variable with highest score for branching.}
    				\State Perform probing to simplify each of the subproblems resulting from the current branching decision.
    				\If{probing reveals an infeasible subproblem}{ impose the restrictions of the feasible subproblem and select the variable with the next highest score for branching. Repeat Line 13.}
    				\Else{ branch on the selected variable. Add the resulting subproblems to $\mathscr{L}$.}
    				\EndIf
    			\EndIf 
    		\EndIf
    	\EndWhile
    	\State Return $\N_{s^*}$, where $s^*$ is the last node for which \textsc{ProcessNode} was called.
    \EndFunction
  \end{algorithmic}
\end{algorithm} 

 %In order to tell what values of $\ell(\mathcal{DB}_{s^*})$ and $\overline{\mathcal{G}_{s^*}}$ constitute a ``close'' solution, we used the Type 1 instances from \citet{boland2015acriterion} of size 160 and performed the following experiment. From the experiments ran in Section \ref{experimental_results} we knew the total of number of nodes each of these instances would process during BB. For instance $i$, let us denote this maximum number as $Tn^i$. We ran our BB on each instance a total of three times, but terminated BB after processing $0.9Tn^i$, $0.95Tn^i$, and $0.99Tn^i$ nodes. For each instance and each node limit we recorded both $\ell(\mathcal{DB}_{s^*})$ and $\overline{\mathcal{G}_{s^*}}$. The results of this experiment are shown in Table \ref{table_duality_gap}. These results suggest that values of $\overline{\ell(\mathcal{DB}_{s^*})}$ and $\overline{\mathcal{G}_{s^*}}$ which are less than 1.0 and 10.0, respectively, indicate a relatively ``small'' duality gap.
%
%When using single objective BB it is rare that a MILP is solved until the difference between the best known primal and dual solutions is truly zero. Instead, some $\epsilon > 0$ is typically chosen so that whenever the percent difference between the primal and dual bound at a node $s$ is less than $\epsilon$, the node is fathomed. Our original implementation is an exhaustive method in the sense that no node is fathomed unless $\L_s$ is known to be certainly dominated by $\N_s$. As far as we are aware, all current BOMILP BB techniques are exhaustive in this way. Therefore, we propose a method which will allow a node $s$ to be fathomed whenever $\L_s$ is, in some sense, ``within $\epsilon$ of $\N_s$.'' A natural choice of a metric to use for determining the distance between sets $\L_s$ and $\N_s$ is the Hausdorff metric \citep{hausdorff1914grundzuge}. Under this metric the distance between two sets $I$ and $J$ is $d_H(I,J) := \max \{\sup_{i\in I}\inf_{j\in J} d(i,j), \sup_{j\in J}\inf_{i\in I} d(i,j) \}$, where $d(\cdot,\cdot)$ represents a distance defined by some metric appropriate for the given sets $I$ and $J$. Here, since we are measuring the distance between two subsets of $\R_2$, we take $d(\cdot,\cdot)$ to be Euclidean distance. Unfortunately, though, the Hausdorff metric is quite difficult to use in this situation. The major difficulty comes from the fact that calculating $\sup_{i\in \L_s}\inf_{j\in \N_s} d(i,j)$ and $\sup_{j\in \N_s}\inf_{i\in \L_s} d(i,j)$ may involve the comparison of an infinite number of points in $\R_2$. To begin to deal with this issue, first consider the following fact.
%\begin{itemize}
%\item[\bf{Fact}:] For any $\epsilon > 0$, $d_H(\L_s,\N_s) < \epsilon$ if and only if $d_H(\mathcal{S},\N_s) < \epsilon$ for all singletons and/or segments $\mathcal{S}$ making up $\L_s$.
%\end{itemize}
%This fact shows that rather than calculating $d_H(\L_s,\N_s)$ explicitly at a node $s$, we can instead iteratively generate the segments $\mathcal{S}$ defining $\L_s$ and calculate $d_H(\mathcal{S},\N_s)$ for each. If a $\mathcal{S} \in \L_s$ is discovered for which $d_H(\mathcal{S},\N_s) > \epsilon$, the procedure stops because $s$ cannot be fathomed. Alternatively, if all $\mathcal{S} \in \L_s$ are generated and $d_H(\mathcal{S},\N_s) < \epsilon$ for each, then $s$ can be fathomed. Although this procedure provides a simpler method for fathoming, it is still difficult to implement. Calculating $d_H(\mathcal{S},\N_s)$ for each $\mathcal{S} \in \L_s$ requires the comparison of $\mathcal{S}$ with every element of $\N_s$. This means that for every $\mathcal{S} \in \L_s$, an $O(t)$ procedure needs to be used to calculate $d_H(\mathcal{S},\N_s)$ ($t$ represents the number of solutions stored in $\N_s$). Even for relatively small problems, $t$ can be extremely large, and therefore this method is far too costly. It would be nice if instead of comparing each $\mathcal{S} \in \L_s$ to the entirety of $\N_s$, we could compare it to the individual solutions stored in $\N_s$, one at a time. Unfortunately, it is not true that for any $\epsilon > 0$, $d_H(\L_s,\N_s) < \epsilon$ if and only if $d_H(\mathcal{S},\mathcal{T}) < \epsilon$ for all singletons and/or segments $\mathcal{S}$ making up $\L_s$ and all singletons and/or segments $\mathcal{T}$ making up $\N_s$. For a counter example, see Figure \ref{hausdorff_1}. It is easy to see that there can exist an $\epsilon > 0$ for which $d_H(\L_s,\N_s) < \epsilon$ and $d_H(\mathcal{S}_1,\mathcal{T}_1) > \epsilon$.
%
%\begin{figure}
%
%\begin{subfigure}[h]{.48\textwidth}
%\centering
%\begin{tikzpicture}
%\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.6\textwidth]{hausdorff_1}};
%\node[align=center,yshift=1.0cm,xshift=-2.1cm] at (image.center) { $\mathcal{S}_1$ };
%\node[align=center,yshift=-1.5cm,xshift=-0.7cm] at (image.center) { $\mathcal{S}_2$} ;
%\node[align=center,yshift=.15cm,xshift=.45cm] at (image.center) {$\mathcal{T}_1$};
%\node[align=center,yshift=1.0cm,xshift=-.1cm] at (image.center) {$\mathcal{T}_2$};
%\end{tikzpicture}
%\caption{Counter example}
%\label{hausdorff_1}
%\end{subfigure}
%\begin{subfigure}[h]{.48\textwidth}
%\centering
%\begin{tikzpicture}
%\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.6\textwidth]{hausdorff_2}};
%\node[align=center,yshift=1.0cm,xshift=-2.1cm] at (image.center) { $\mathcal{S}_1$ };
%\node[align=center,yshift=-0.75cm,xshift=-1.5cm] at (image.center) { $\mathcal{S}_1'$ };
%\node[align=center,yshift=-2cm,xshift=-0.7cm] at (image.center) { $\mathcal{T}_1 + \R_{--}$} ;
%\node[align=center,yshift=-1.15cm,xshift=1.75cm] at (image.center) {$\mathcal{T}_1$};
%\node[align=center,yshift=.35cm,xshift=.2cm] at (image.center) {$\mathcal{T}_1'$};
%\node[align=center,yshift=2.0cm,xshift=-.1cm] at (image.center) {$\mathcal{S}_1 + \R_{++}$};
%\end{tikzpicture}
%\caption{Using the proximal Hausdorff distance}
%\label{hausdorff_2}
%\end{subfigure}
%\caption{Troubles with the Hausdorff metric and their remedies}
%\label{hausdorff_fig}
%
%\end{figure}
%
%Due to these difficulties with the Hausdorff metric, we introduce a new modified version of the metric, which we will refer to as the \emph{proximal Hausdorff metric}. Given $\mathcal{S}\in \L_s$ and $\mathcal{T}\in \N_s$, let $\mathcal{S}' = \mathcal{S} \cap (\mathcal{T}+\R_{--})$ and $\mathcal{T}' = \mathcal{T} \cap (\mathcal{S}+\R_{++})$. Then the \emph{proximal Hausdorff distance} between $\mathcal{S}$ and $\mathcal{T}$ is $d_{PH}(\mathcal{S},\mathcal{T}) := d_H(\mathcal{S}',\mathcal{T}')$. To visualize this concept, see Figure \ref{hausdorff_2}. Notice from this figure that use of the proximal Hausdorff distance allows us to compute the distance between only portions of $\mathcal{S}_1$ which may dominate $\mathcal{T}_1$ and portions of $\mathcal{T}_1$ which may be dominated by $\mathcal{S}_1$. Thus, we utilize the the proximal Hausdorff metric to modify the \textsc{CheckDominance}$(\cdot)$ function. %in the following way. Suppose that $\Pi$ represents the tree data structure used to store the points and segments in $\R^2$ which make up $\N_s$. Let an arbitrary node of this tree be represented by $\pi$ and the root node by $\pi_0$. When some $\mathcal{S} \in \L_s$ is determined and \textsc{CheckDominance}$(\mathcal{S})$ is called, $\mathcal{S}$ is first compared with $pi_0$. Immediately, 
%Given a chosen value of $\epsilon > 0$, whenever some $\mathcal{S} \in \L_s$ is determined and \textsc{CheckDominance}$(\mathcal{S})$ is called, \textsc{CheckDominance}$(\cdot)$ returns 1 if either (i) there exists $\mathcal{T}\in \N_s$ for which $d_{PH}(\mathcal{S},\mathcal{T}) > \epsilon$, or (ii) $\mathcal{S} \setminus \left(\cup_{\mathcal{T}\in \N_s} (\mathcal{T}+\R_{--})\right) \neq \emptyset$. If neither of these conditions are met, \textsc{CheckDominance}$(\cdot)$ returns 0. In this way, we are able to modify \textsc{CheckDominance}$(\cdot)$ without increasing its original worst-case complexity of $O(\log(t))$. 
%
%We selected $\epsilon$ to be 0.0001 times the maximum of the differences between the $f_1$ values associated with $\omega^{nw}$ and $\omega^{se}$ and the $f_2$ values associated with the same solutions. Table \ref{} shows the average percent decrease in number of nodes explored and time needed to solve the instances of BOMILP reported in Section \ref{experimental_results_1}.


\section{Computational Analysis} \label{sec:compute}
We implemented our \bb{} scheme using the C programming language and the ILOG CPLEX optimization package \citep{Cplex}.  \citet{boland2015acriterion} graciously shared their code with us and so we were able to compare the performance of our BB with the triangle splitting method, which we recall is a search method in the objective space. In preliminary tests, we also compared with the BB method of \citep{belotti2012biobjective}. However, their implementation was incomplete and so the performance of our \bb{} was far superior to theirs. For this reason, we do not include the results of their \bb{}. All testing was conducted using the Clemson University Palmetto Cluster. Specifically, we used an HP SL250s server node with a single Intel E5-2665 CPU core with 32GB of RAM running Scientific Linux 6.4.%\pagebreak

Our initial test set consisted of the instances examined in \citep{belotti2012biobjective,boland2015acriterion}. The instances from \citep{belotti2012biobjective} contained either 60 variables and 60 constraints (``Belotti60''), or 80 variables and 80 constraints (``Belotti80''). We similarly label the instances used to test the triangle splitting method. The instances in \citep{boland2015acriterion} are ``Boland80,'' ``Boland160,'' and ``Boland320'' (we do not solve instances with less than 60 constraints or variables due to their relative ease). We also utilize some other instances that were considered in a previous paper \citep{boland2013criterion}. To maintain consistency with the way these instances were labelled, we refer to them as ``Boland16,'' ``Boland25,'' and ``Boland50,'' although the respective total number of variables and constraints for each of these instance sets is approximately 800, 1250 and 2500. Figure \ref{pareto_sets} depicts the Pareto set and boundary of $\L_0$ for one instance from each of the two instance classes.

\begin{figure}[h!]
\begin{subfigure}[h]{.55\textwidth}
\centering
\includegraphics[width=.9\textwidth]{belotti_instance_w_legend}
\caption{Instance from the Belotti60 set.}
\label{belotti_instance}
\end{subfigure}%
\begin{subfigure}[h]{.45\textwidth}
\centering
\includegraphics[width=1\textwidth]{boland_instance}
\caption{Instance from the Boland80 set.}
\label{boland_instance}
\end{subfigure}
\caption{Pareto set and boundary of $\L_0$ for the two instance families.}%randomly selected instances from \citep{belotti2012biobjective} and \citep{boland2015acriterion}.}
\label{pareto_sets}
\end{figure}

Due to positive results we obtained when solving the instances from \citep{belotti2012biobjective,boland2015acriterion}, we felt the need to create a more difficult test set. Hence, we also tested on biobjective variants of some instances from MIPLIB 2010 \citep{koch2011miplib} --- we chose only those instances that were marked easy, are mixed-integer and were relatively small in size (up to approximately 200 integer variables). For each instance, we generated six secondary objective functions using a mix of randomized and deterministic procedures (details given in \textsection\ref{sec:MIPLIB}) with the aim of creating some conflict in the two objectives. We discarded instances for which: (i) the Pareto set was a singleton, or (ii) the second objective was unbounded, or (iii) the MILP associated with either $f_1$ or $f_2$ took over 12 hours to solve. We set a maximum solution time of 12 hours for all instances.

We began our tests by turning off all nonessential features of our BB procedure, and then sequentially turning on various features to test their impact on the overall procedure. If a particular feature of our BB procedure was deemed effective in reducing the overall effort required to solve instances of BOMILP, this feature was left on for the remainder of the tests, otherwise it was turned back off. %We first test the utility of the various presolve procedures from \textsection  \ref{sec:presolve}.

\subsection{Presolve Techniques}

Table \ref{table_presolve} contains the results of our first computational experiment. We report the average computation time in seconds to solve instances of each type, the average number of nodes explored, and the average duality gap percentage computed after processing the root node. Note that in for this test we utilized \textsc{PreprocesingMethod2} with $\rho$ set to zero.

 \begin{table}%[H]
\footnotesize
\centering
\caption{Experiment 1 -- Measuring the impact of presolve techniques.}\label{table_presolve}
\begin{tabular}{l|r|rrr|rrr|rrr|rrr}
\toprule
 &  & \multicolumn{3}{c}{All} & \multicolumn{3}{c}{Duality} & \multicolumn{3}{c}{Singleton} & \multicolumn{3}{c}{Dominating}\\ 
  & & \multicolumn{3}{c}{Off} & \multicolumn{3}{c}{Fixing On} & \multicolumn{3}{c}{Columns On} & \multicolumn{3}{c}{Columns On}\\
\cmidrule(r){3-5}
\cmidrule(r){6-8}
\cmidrule(r){9-11}
\cmidrule(r){12-14}
Instance & \# & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$  \\ 
\midrule
Belotti60 & 30 & 7 & 76 & 54 & 7  & 76 & 54 & 7 & 76 & 54 & 7 & 76 & 54\\
Belotti80 & 30 & 16 & 87 & 56 & 17 & 87 & 56 & 17 & 87 & 56 & 16 & 87 & 56\\
\midrule
Boland80 & 5 & 26 & 541 & 38 & 24 & 493 & 33 & 26 & 541 & 38 & 28 & 541 & 38\\
Boland160 & 5 & 899 & 2,873 & 17 & 808 & 2,801 & 15 & 893 & 2,873 & 17 & 895 & 2,873 & 17\\
Boland320$\circledast$ & 5 & 31,822 & 14,262 &9 &32,897 &17,628 &41 &31,841 &14,262 &9 &31,589 &14,262 &9 \\
Boland16 & 4 & 9 & 97 & 18 & 8 & 96 & 18 & 9 & 97 & 18 & 9 & 97 & 18 \\
Boland25 & 4 & 61 & 327 & 15 & 54 & 338 & 15 & 60 & 327 & 15 & 61 & 327 & 15\\
Boland50 & 4 & 2,343 & 2,531 & 19 & 1,461 & 2,084 & 19 & 2,395 & 2,531 & 19 & 2,323 & 2,531 & 19 \\
\bottomrule
\multicolumn{4}{l}{\scriptsize $\circledast$ -- Only 4 of 5 instances completed.\Tstrut}
\end{tabular}
\end{table}

Notice from Table \ref{table_presolve} that the results for duality fixing show the opposite pattern for the Boland320 instances than for all other instances. This is due to the fact that, for an unknown reason, fixing several variables during presolve had a negative impact on preprocessing, causing many fewer solutions to be discovered during this phase and therefore having an overall negative impact on the rest of the BB procedure. We felt, though, that the positive impact duality fixing had on the other instances sets warranted leaving this feature on for the remainder of our tests. Also observe from  Table \ref{table_presolve} that the exploitation of neither singleton nor dominating columns had any impact on the overall BB procedure. We found that this was mainly due to the fact that there were very few occurrences of either of these types of columns. We opted to turn off the exploitation of singleton columns for the remainder of our tests, but we left on the exploitation of dominating columns. Our reasoning here was that singleton columns have no impact on BB that extends beyond presolve, while dominating columns result in disjunctions from which we can generate global cutting planes. Hence, we left on the exploitation of dominating columns in order to test the impact of generating these cuts in later tests. 

\subsection{Preprocessing}\label{preprocess_test}

In our next test we examined the impact of the two preprocessing techniques discussed in Section \ref{sec:presolve}, as well as a hybrid method we derived as a combination of the two presented procedures.  In our initial implementation of this test we used each of these three methods with $rho$ assigned each integer value in $[0,5]$. Recognize from Algorithms \ref{alg5} and \ref{alg6} that each of the proposed preprocessing procedures are designed so that the total number of Pareto solutions computed should have a positive correlation with the value of $\rho$. We determined that \textsc{ProprocesingMethod1} performed poorly for $\rho \leq 1$ and \textsc{ProprocesingMethod2} performed poorly for $\rho \geq 2$. We also discovered that the impact of $\rho$ on overall solution time varied with the size of the instance solved. As a result, we also implemented modified preprocessing procedures in which the value of $\rho$ is automatically computed as a function of the size of an instance. Figures \ref{prof_60} and \ref{prof_80} respectively contain performance profiles of CPU time for instances of size 80 and smaller, and size greater than 80. We note that in the legends for these profiles we use ``e,'' ``w,'' and ``hy'' to denote \textsc{PreprocessingMethod1} (based on the $\epsilon$-constraint method), \textsc{PreprocessingMethod2} (based on the weighted sum approach), and the hybrid method. The subsequent numbers indicate the value of $\rho$. Additionally, the ``term'' vary indicates that $\rho$ was automatically computed as a function of instance size.

%\begin{figure}[H]
%\centering
%\includegraphics[width=\textwidth]{profile_preprocessing}
%\caption{Performance profile of CPU time.}
%\label{prof}
%\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{profile_leq_80_3}
\caption{Performance profile of CPU time for instances of size 80 and less.}
\label{prof_60}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{profile_gr_80_3}
\caption{Performance profile of CPU time for instances of size greater than 80.}
\label{prof_80}
\end{figure}

Observe from Figures \ref{prof_60} and \ref{prof_80} that the hybrid preprocessing approach did not perform well compared to the other approaches. Now consider \textsc{PreprocessingMethod2}. Recognize that although variants of this procedure performed well for smaller instances, the same is not true for larger instances. \textsc{PreprocessingMethod1}, on the other hand, performed quite well on all instances. Notice, however, that values of $\rho$ near two performed quite well for small instances while values near five performed extremely poorly. However, for larger instances values of $\rho$ near five seem to outperform almost every other procedure. Due to the consistent performance of the variant of \textsc{PreprocessingMethod1} in which the value of $\rho$ was computed automatically as a function of instance size, we opted to use this approach for the remainder of our tests.

\subsection{Probing and Pareto Branching}

The next test we performed was designed to examine the utility of the variable probing procedure used directly after preprocessing and at each node prior to branching, and the Pareto branching that we perform when $\OS$ fathoming results in disjoint feasible regions of $\OS$. The results of this experiment are given in Table \ref{table_probing}.

 \begin{table}%[H]
%\scriptsize
\centering
\caption{Experiment 3 -- Measuring the impact of Probing and Pareto branching.}\label{table_probing}
\begin{tabular}{l|r|rrr|rrr|rr|rr}
\toprule
 &  & \multicolumn{3}{c}{All} & \multicolumn{3}{c}{Initial} & \multicolumn{2}{c}{Probing During} & \multicolumn{2}{c}{Pareto}\\ 
  & Total & \multicolumn{3}{c}{Off} & \multicolumn{3}{c}{Probing On} & \multicolumn{2}{c}{Branching On} & \multicolumn{2}{c}{Branching On}\\
\cmidrule(r){3-5}
\cmidrule(r){6-8}
\cmidrule(r){9-10}
\cmidrule(r){11-12}
Instance & Num & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes & Time & Nodes  \\ 
\midrule
Belotti60 & 30 & 7 & 73 & 54 & 8  & 78 & 54 & 5 & 47 & 7 & 72\\
Belotti80 & 30 & 21 & 95 & 46 & 21 & 92 & 45 & 13 & 60 & 18 & 86\\
\midrule
Boland80 & 5 & 19 & 390 & 20 & 19 & 397 & 20 & 10 & 217 & 18 & 407\\
Boland160 & 5 & 667 & 2,497 & 25 & 679 & 2,506 & 25 & 244 & 978 & 534 & 2,569\\
Boland320 & 5 & 19,902 & 10,209 & 6 & 19,160 & 9,971 & 6 & 6,865 & 3,720 & 14,348 & 9,583 \\
Boland16 & 4 & 10 & 83 & 10 & 10 & 84 & 10 & 8 & 62 & 11 & 101 \\
Boland25 & 4 & 52 & 394 & 52 & 56 & 427 & 52 & 34 & 290 & 50 & 381\\
Boland50 & 4 & 1,204 & 1,706 & 16 & 1,556 & 2,008 & 16 & 987 & 1,426 & 1,136 & 2,174  \\
\bottomrule
\end{tabular}
\end{table}

Observe from Table \ref{table_probing} that when utilizing probing directly after preprocessing, in most cases the total CPU time and number of nodes processed increased. Surprisingly, however, performing the same probing procedure prior to branching at each node had an extremely positive impact on the overall performance of BB, significantly lowering total CPU time and the number of explored nodes. We also found that Pareto branching had an overall positive impact on BB performance. For the remainder of our tests we opted to cease probing directly after preprocessing, but to still employ probing during branching as well as Pareto branching. 

\subsection{Local Cuts}

The next test we performed was designed to test the utility of various cut generation procedures that we employed. We divided this test into two parts, (a) and (b). In part (a) we examined the performance of BB while applying the local cut generation procedure we discussed in Section \ref{sec:tight}, the generation of globally valid cutting planes from disjunctions implied by pairs of dominating columns (cf. Proposition~\ref{dominating_col_disjunction}), and the generation of locally valid cuts from $\OS$ space disjunctions discovered during $\OS$ fathoming. For part (b) of the experiment we decided to test the utility of a new procedure for generating globally valid cuts after preprocessing, but prior to processing the root node. In this procedure we preselect a number of of values of $\lambda$, evenly distributed in $(0,1)$, and pass the MILP $\min\{\lambda f_1(x) + (1-\lambda) f_2(x): x \in X_I\}$ to CPLEX. We allow CPLEX to process the root node of this MILP, afterwards we extract the cutting planes discovered by CPLEX and add them to our original BOMILP as global cuts. The motivation behind this approach is that, because the implementation of our biobjective BB procedure is an adaptation of standard CPLEX single objective BB, modified through the use of callbacks, the standard cut generation procedure of CPLEX will only generate cuts based on the objective associated with the single objective problem we pass to CPLEX. This means that the cuts generated by the default CPLEX cut generation procedure are only useful in closing the duality gap in a small subregion of $\OS$. We designed our procedure to combat this issue. The results of parts (a) and (b) of this experiment are given in Tables \ref{table_cuts1} and \ref{table_cuts2}, respectively.

% \begin{table}[H]
%\scriptsize
%\centering
%\caption{CPLEX cut generation off
%}\label{table_presolve}
%\begin{tabular}{l|r|rrr|rrr|rrr|rr}
%\toprule
% &  & \multicolumn{3}{c}{} & \multicolumn{3}{c}{Local Cut} & \multicolumn{3}{c}{Global Cuts From} & \multicolumn{2}{c}{Local Cuts}\\ 
% &  & \multicolumn{3}{c}{All} & \multicolumn{3}{c}{Generation} & \multicolumn{3}{c}{Dominating Column} & \multicolumn{2}{c}{From $\OS$}\\ 
%  & Total & \multicolumn{3}{c}{Off} & \multicolumn{3}{c}{On} & \multicolumn{3}{c}{Disjunctions On} & \multicolumn{2}{c}{Disjunctions On}\\
%\cmidrule(r){3-5}
%\cmidrule(r){6-8}
%\cmidrule(r){9-11}
%\cmidrule(r){12-13}
%Instance & Num & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes  \\ 
%\midrule
%60 $\times$ 60 \citep{belotti2012biobjective} & 30 & 5 & 53 & 48 & 6  & 49 & 47 & 5 & 53 & 48 & 5 & 53 \\
%80 $\times$ 80 \citep{belotti2012biobjective} & 30 & 12 & 65 & 48 & 15 & 60 & 46 & 12 & 65 & 48 & 12 & 64 \\
%\midrule
%C80 \citep{boland2015acriterion} & 5 & 8 & 219 & 20 & 8 & 215 & 20 & 8 & 219 & 20 & 8 & 219 \\
%C160 \citep{boland2015acriterion} & 5 & 185 & 1,015 & 21 & 194 & 1,050 & 21 & 185 & 1,015 & 21 & 186 & 1,007 \\
%C320 \citep{boland2015acriterion} & 5 & 4,433 & 4,030 & 5 & 4,866 & 4,130 & 5 & 4,472 & 3,996 & 5 & 4,531 & 3,979 \\
%C16 \citep{boland2015acriterion} & 4 & 7 & 62 & 10 & 7 & 62 & 10 & 7 & 62 & 10 & 7 & 62 \\
%C25 \citep{boland2015acriterion} & 4 & 28 & 269 & 46 & 30 & 272 & 46 & 28 & 269 & 46 & 31 & 273 \\
%C50 \citep{boland2015acriterion} & 4 & 628 & 1,443 & 16 & 710 & 1,490 & 16 & 655 & 1,476 & 16 & 671 & 1,388   \\
%\bottomrule
%\end{tabular}
%\end{table}
%
\begin{table}%[H]
\small
\centering
\caption{Experiment 4, part (a) -- Measuring the impact of cut generation procedures.}\label{table_cuts1}
\begin{tabular}{l|r|rrr|rrr|rrr|rr}
\toprule
 &  & \multicolumn{3}{c}{} & \multicolumn{3}{c}{Local Cut} & \multicolumn{3}{c}{Global Cuts From} & \multicolumn{2}{c}{Local Cuts}\\ 
 &  & \multicolumn{3}{c}{All} & \multicolumn{3}{c}{Generation} & \multicolumn{3}{c}{Dominating Column} & \multicolumn{2}{c}{From $\OS$}\\ 
  & Total & \multicolumn{3}{c}{Off} & \multicolumn{3}{c}{On} & \multicolumn{3}{c}{Disjunctions On} & \multicolumn{2}{c}{Disjunctions On}\\
\cmidrule(r){3-5}
\cmidrule(r){6-8}
\cmidrule(r){9-11}
\cmidrule(r){12-13}
Instance & Num & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes  \\ 
\midrule
Belotti60 & 30 & 5 & 52 & 44 & 7  & 52 & 44 & 5 & 52 & 44 & 5 & 52 \\
Belotti80 & 30 & 13 & 61 & 45 & 16 & 57 & 45 & 13 & 61 & 45 & 13 & 61 \\
\midrule
Boland80 & 5 & 8 & 219 & 20 & 8 & 215 & 20 & 8 & 219 & 20 & 8 & 219 \\
Boland160 & 5 & 189 & 1,015 & 21 & 198 & 1,050 & 21 & 188 & 1,015 & 21 & 190 & 1,007 \\
Boland320 & 5 & 4,417 & 4,042 & 5 & 5,005 & 4,087 & 5 & 4,545 & 4,103 & 5 & 4,471 & 4,100 \\
Boland16 & 4 & 7 & 60 & 10 & 7 & 60 & 10 & 7 & 60 & 10 & 8 & 61 \\
Boland25 & 4 & 29 & 272 & 46 & 29 & 265 & 46 & 29 & 272 & 46 & 32 & 273 \\
Boland50 & 4 & 658 & 1,437 & 16 & 693 & 1,407 & 16 & 644 & 1,495 & 16 & 709 & 1,528   \\
\bottomrule
\end{tabular}
\end{table}
%
% \begin{table}[H]
%\scriptsize
%\centering
%\caption{CPLEX set to aggressive cut generation
%}\label{table_presolve}
%\begin{tabular}{l|r|rrr|rrr|rrr|rr}
%\toprule
% &  & \multicolumn{3}{c}{} & \multicolumn{3}{c}{Local Cut} & \multicolumn{3}{c}{Global Cuts From} & \multicolumn{2}{c}{Local Cuts}\\ 
% &  & \multicolumn{3}{c}{All} & \multicolumn{3}{c}{Generation} & \multicolumn{3}{c}{Dominating Column} & \multicolumn{2}{c}{From $\OS$}\\ 
%  & Total & \multicolumn{3}{c}{Off} & \multicolumn{3}{c}{On} & \multicolumn{3}{c}{Disjunctions On} & \multicolumn{2}{c}{Disjunctions On}\\
%\cmidrule(r){3-5}
%\cmidrule(r){6-8}
%\cmidrule(r){9-11}
%\cmidrule(r){12-13}
%Instance & Num & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes  \\ 
%\midrule
%60 $\times$ 60 \citep{belotti2012biobjective} & 30 & 5 & 52 & 44 & 6  & 52 & 44 & 5 & 52 & 44 & 5 & 52 \\
%80 $\times$ 80 \citep{belotti2012biobjective} & 30 & 13 & 61 & 45 & 16 & 57 & 45 & 13 & 61 & 45 & 13 & 61 \\
%\midrule
%C80 \citep{boland2015acriterion} & 5 & 8 & 219 & 20 & 8 & 215 & 20 & 8 & 219 & 20 & 8 & 219 \\
%C160 \citep{boland2015acriterion} & 5 & 185 & 1,015 & 21 & 194 & 1,050 & 21 & 185 & 1,015 & 21 & 186 & 1,007 \\
%C320 \citep{boland2015acriterion} & 5 & 4,441 & 4,073 & 5 & 4,089 & 3,741 & 5 & 4,426 & 4,031 & 5 & 4,430 & 4,023 \\
%C16 \citep{boland2015acriterion} & 4 & 7 & 60 & 10 & 7 & 60 & 10 & 7 & 60 & 10 & 7 & 61 \\
%C25 \citep{boland2015acriterion} & 4 & 28 & 272 & 46 & 29 & 265 & 46 & 28 & 272 & 46 & 31 & 273 \\
%C50 \citep{boland2015acriterion} & 4 & 658 & 1,496 & 16 & 693 & 1,459 & 16 & 633 & 1,453 & 16 & 764 & 1,573   \\
%\bottomrule
%\end{tabular}
%\end{table}

Observe from Table \ref{table_cuts1} that utilizing each of the displayed methods for cut generation had a negative impact on the CPU time used during BB. A couple of these methods did aid in reducing the number of nodes explored during BB, but not substantially. As a result, we opted to turn off all of these cut generation schemes for the remainder of our tests. There are a couple of important notes to be made concerning cut generation, though. First, it is important to recognize that the potential impact of generating locally valid cuts for BOMILP is likely not properly displayed by the results of this experiment. The primary reason for this is that CPLEX does not allow for the addition of locally valid cutting planes except during the execution of a user-cut-callback. However, such a callback is only employed intermittently and quite rarely once a certain depth of the BB has been reached. This is unfortunate, since it seems that locally valid cuts may have an increasingly significant impact on the reduction of the duality gap as the depth of the BB tree increases. Another important thing to note concerning these cut generation schemes is that are two ways in which we can pass globally valid cuts to CPLEX, and each is limited in its own way. First, we can pass a global cut to CPLEX specifically as a cut. However, when doing so, CPLEX will only utilize this cut if it detects a solution at which this cut is violated. This is unfortunate though, since as we have discussed, CPLEX is only aware of solutions generated from a single objective. Many of the solutions generated during BB are generated by us, during a callback, and not by CPLEX. Thus, even though solutions may be generated which violate a cut we have passed to CPLEX, the cut still never be utilized. The second way we could pass a cut to CPLEX is by explicitly adding it to the BOMILP model as an additional row. This forces the utilization of this cut, but adding too many cuts in this way causes CPLEX to need to perform a significant amount of additional book-keeping and therefore typically has an overall negative impact on BB. 

\begin{table}%[H]
\small
\centering
\caption{Experiment 4, part (b) -- Measuring the impact of cut generation procedures.}\label{table_cuts2}
\begin{tabular}{l|r|rrr|rrrrrrrrr}
\toprule
 &  & \multicolumn{3}{c}{} & \multicolumn{9}{c}{Add Extra Global Cuts Using Various $\lambda$'s Prior to Start of BB} \\ 
% &  & \multicolumn{3}{c}{} & \multicolumn{15}{c}{$\lambda$'s Prior to Start of BB} \\ 
 \cmidrule(r){6-14}
  &  & \multicolumn{3}{c}{Off} & \multicolumn{3}{l}{\# of $\lambda$'s:~~~ 2} & \multicolumn{3}{c}{3} & \multicolumn{3}{c}{5} \\
\cmidrule(r){3-5}
\cmidrule(r){6-8}
\cmidrule(r){9-11}
\cmidrule(r){12-14}
Instance & \# & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$   \\ 
\midrule
Belotti60 & 30 & 5 & 52 & 48 & 6 & 52 & 49 & 6 & 54 & 48 & 6 & 53 & 43 \\
Belotti80 & 30 & 13 & 61 & 46 & 14 & 62 & 46 & 14 & 62 & 46 & 15 & 60 & 45\\
\midrule
Boland80 & 5 & 8 & 219 & 20 & 8 & 219 & 20 & 8 & 218 & 14 & 8 & 219 & 9 \\
Boland160 & 5 & 185 & 1,015 & 25 & 185 & 1,015 & 23 & 185 & 1,015 & 20 & 185 & 1,011 & 20 \\
Boland320 & 5 & 4,347 & 4,028 & 6 & 4,274 & 3,986 & 6 & 4,257 & 3,976 & 6 & 4,343 & 3,984 & 6 \\
Boland16 & 4 & 7 & 60 & 10 & 7 & 60 & 10 & 7 & 60 & 10 & 7 & 60 & 10 \\
Boland25 & 4 & 29 & 272 & 52 & 28 & 272 & 52 & 29 & 277 & 52 & 26 & 241 & 52  \\
Boland50 & 4 & 639 & 1,470 & 16 & 646 & 1,494 & 16 & 655 & 1,487 & 16 & 666 & 1,505 & 16   \\
\midrule
\midrule
  &  & \multicolumn{3}{c}{}  & \multicolumn{3}{c}{\# of $\lambda$'s:~~~ 9} & \multicolumn{3}{c}{17}\\

\cmidrule(r){6-8}
\cmidrule(r){9-11}

 &  &  &  &  & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$   \\ 
\midrule
Belotti60 & 30 &&&& 7 & 54 & 42 & 9 & 54 & 42\\
Belotti80 & 30 &&&& 17 & 61 & 45 & 21 & 61 & 45\\
\midrule
Boland80 & 5 & &&& 8 & 223 & 6 & 7 & 219 & 6\\
Boland160 & 5 & &&& 181 & 979 & 20 & 181 & 985 & 20\\
Boland320 & 5 & &&& 4,316 & 3,946 & 6 & 4,216 & 3,870 & 6 \\
Boland16 & 4 & &&&7 & 60 & 10 & 7 & 60 & 10\\
Boland25 & 4 & &&&26 & 237 & 52 & 23 & 221 & 52 \\
Boland50 & 4 & &&&707 & 1,544 & 16 & 587 & 1,342 & 16  \\
\bottomrule
\end{tabular}
\end{table}

Observe from Table \ref{table_cuts2} that there is no set of instances which displays an overall decrease in CPU time as the number of utilized values of $\lambda$ increases. We note that for the instances from \citep{belotti2012biobjective} there is an overall increase in running time, while the instances from \citep{boland2015acriterion} display a haphazard pattern, increasing on some occasions and decreasing on others. The reason for the pattern displayed by the instances from \citep{belotti2012biobjective} is that, although several cutting planes were generated for each used value of $\lambda$, as we described in our dicussion of Table \ref{table_cuts1}, in order for these cuts to be utilized by CPLEX we were forced to add them as rows to the BOMILP model, which caused a significant increase in the computational overhead. The reason for the pattern displayed by the instances from \citep{boland2015acriterion} is that for the majority of these instances the single objective MILP associated with each value of $\lambda$ was solved by CPLEX before any cutting planes were generated. Thus, there were rarely cuts to be extracted and copied. The variation in running times and number of nodes processed seems to be due to a difference in the order in which CPLEX processed nodes during the biobjective BB procedure. As this procedure of generating additional cutting planes did not result in a decrease in CPU time spent in BB, we opted to turn off this procedure for the remainder of our tests.

\subsection{Additional Improvements}

For our next experiment we decided to test potential simplifications to Fathoming Rule 3 and the generation of $\nd{\L_s}$. We now describe these two improvements, beginning with that of Fathoming Rule 3. Recognize from Algorithm \ref{alg3} that if we have a node $s$ for which $\U_s \dom \L_s$, but Fathoming Rules 1a and 2a fail, Fathoming Rule 3 does not cease until every defining line segment of $\nd{\L_s}$ is generated. To attempt to reduce the time spent executing Fathoming Rule 3 on these occasions, we implemented the following procedure:
\begin{enumerate}
\item Select $\alpha \in \Z_+$.
\item After $\alpha$ lines segments have been generated during the execution of Algorithm \ref{alg3}, for each newly generated line segment dominated by $U_s$, extend the line segment so that the first component of its left-most point is $(y^1_s)_1$. If this extended line segment is dominated by $\U_s$, then $\L_s$ is also dominated so fathom node $s$.
\end{enumerate}
An example of this procedure is depicted in Figure \ref{fig_improved_fath3}.
\begin{figure}
\centering
%\begin{subfigure}[h]{.33\textwidth}
%\centering
\includegraphics[width=.33\textwidth]{improved_fath3}
\caption{Simplification of Fathoming Rule 3.}
\label{fig_improved_fath3}
%\end{subfigure}\hspace*{1cm}%
%\begin{subfigure}[h]{.32\textwidth}
%\centering
%\includegraphics[width=\textwidth]{gaps9}
%\caption{Slitting $\OS$}
%\label{gaps2}
%\end{subfigure}
%\caption{Improvements to Fathoming Rule 3 and the generation of $\nd{\L_s}$.}
%\label{fig_improved}
\end{figure}
In Figure \ref{fig_improved_fath3} $\N_s$ is shown in blue and $\nd{\L_s}$ is shown in red. We assume the two right-most segments of $\nd{\L_s}$ have already been generated and shown to be dominated by $\U_s$. Hence, we can see that the node being considered can be fathomed after the generation of the ``extended segment'' without needing to generate the final segment of $\nd{\L_s}$. We now consider the simplification of the generation of $\nd{\L_s}$. In order to simplify this procedure we cease generating segments in $\nd{\L_s}$ if any segment in generated which is dominated by $\U_s$. The results we obtained from this experiment are shown in Table \ref{table_improvements}.

 \begin{table}[h!]
%\small
\centering
\caption{Experiment 5 -- Improvements to Fathoming Rule 3 and the generation of $\nd{\L_s}$.}\label{table_improvements}
\begin{tabular}{l|r|rr|rrrrrr|rrrr}
\toprule
 &  & \multicolumn{2}{c}{} & \multicolumn{6}{c}{Check For Early Termination} & \multicolumn{2}{c}{Check For Early}\\ 
 &  & \multicolumn{2}{c}{Both} & \multicolumn{6}{c}{of Fathoming Rule 3} & \multicolumn{2}{c}{Termination of}\\ 
 \cmidrule(r){5-10}
  & Total & \multicolumn{2}{c}{Off} &$\alpha =$\hfill 0 & 5 & 10 & 15 & 20 & 25 & \multicolumn{2}{c}{$\nd{\L_s}$ Generation}\\
\cmidrule(r){3-4}
\cmidrule(r){5-10}
\cmidrule(r){11-12}
Instance & Num & Time & Nodes & Time & Time & Time & Time & Time & Time & Time &  Nodes  \\ 
\midrule
Belotti60 & 30 & 5 & 52 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 52 \\
Belotti80 & 30 & 13 & 61 & 13 & 13 & 13 & 13 & 13 & 13 & 13 & 60 \\
\midrule
Boland80 & 5 & 8 & 218 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 218 \\
Boland160 & 5 & 196 & 1,015 & 195 & 194 & 192 & 194 & 196 & 194 & 199 & 1,045 \\
Boland320 & 5 & 4,384 & 4,094 & 4,487 & 4,383 & 4,382 & 4,469 & 4,395 & 4,399 & 4,380 & 4,117 \\
Boland16 & 4 & 7 & 60 & 7 & 7 & 7 & 7 & 7 & 7 &7 & 61 \\
Boland25 & 4 & 28 & 272 & 28 & 28 & 28 & 28 & 28 & 28 & 28 & 261 \\
Boland50 & 4 & 651 & 1,477 & 645 & 678 & 673 & 638 & 654 & 629 & 638 & 1,468   \\
\bottomrule
\end{tabular}
\end{table}

Note that in Table \ref{table_improvements} we do not report the number of nodes processed when the simplified version of Fathoming Rule 3 is employed because there is no change in the number of nodes processed using this method and the original implementation. Unfortunately, neither of our proposed simplifications resulted in improved CPU times for BB, so we turned off these simplifications for the remainder of our tests.

\subsection{Exploiting $\OS$ Gaps and Comparing with Triangle Splitting}

We have now presented the results of all experiments designed to study the impact of the various aspects of our BB procedure. We now present the results of an experiment designed to test the performance of our BB against that of the triangle splitting method of \citep{boland2015acriterion}. For this experiment we solved all the same instances we used in our previous tests and employed two variants of our BB procedure, one in which we utilized the $\OS$ splitting procedure we discussed in Section \ref{sec:osgap} and one in which we utilized our standard implementation. We compared our results with that of the triangle splitting method of \citep{boland2015acriterion}. The results of this test are given in Table \ref{table_gaps_ts}.

 \begin{table}[h!]
%\small
\centering
\caption{Experiment 6 -- Comparison with the triangle splitting method.}\label{table_gaps_ts}
\begin{tabular}{l|r|rr|rrr|r}
\toprule
 &  & \multicolumn{2}{c}{} & \multicolumn{3}{c}{BB with} & \multicolumn{1}{c}{}\\ 
 &  & \multicolumn{2}{c}{Standard} & \multicolumn{3}{c}{Exploiting} & \multicolumn{1}{c}{Triangle}\\
  & Total & \multicolumn{2}{c}{BB} & \multicolumn{3}{c}{$\OS$ Gaps} & \multicolumn{1}{c}{Splitting}\\
\cmidrule(r){3-4}
\cmidrule(r){5-7}
\cmidrule(r){8-8}
 &  &  &  & \multicolumn{2}{c}{Time} &   \\
\cmidrule(r){5-6}
Instance & Num & Time & Nodes & Total & Parallel$\dagger$ & Nodes & Time  \\ 
\midrule
Belotti60 & 30 & 5 & 52 & 6 & 3 & 33 & 16 \\
Belotti80 & 30 & 13 & 61 & 15 & 7 & 42 & 37  \\
\midrule
Boland80 & 5 & 8 & 218 & 7 & 6 & 203 & 68\\
Boland160 & 5 & 185 & 1,015 & 167 & 162 & 954 & 661  \\
Boland320 & 5 & 4,433 & 4,107 & 4,501 & 4,501 & 4,200 & 8,620 \\
Boland16 & 4 & 7 & 60 & 7 & 7 & 60 & 14 \\
Boland25 & 4 & 28 & 272 & 26 & 26 & 284 & 68  \\
Boland50 & 4 & 664 & 1,488 & 623 & 440 & 1,354 & 631  \\
\bottomrule
\multicolumn{8}{l}{\scriptsize $\dagger$ -- Approximate; calculated as Presolve/Preprocessing time \Tstrut}\\ \multicolumn{8}{l}{\scriptsize ~~~~~plus maximum of BB times over $\OS$ splits.\Tstrut}
\end{tabular}
\end{table}

Observe from Table \ref{table_gaps_ts} that our standard BB procedure outperformed the triangle splitting method on all but one set of instances, while our $\OS$ splitting procedure outperformed the triangle splitting method on all sets of instances. Also recognize that the total CPU times associated with our $\OS$ splitting procedure are always comparable with those of our standard procedure. We point out that there were many more substantial gaps between solutions to exploit after preprocessing for the instances from \citep{belotti2012biobjective} than for the instances from \citep{boland2015acriterion}. This is the reason that there is a drastic reduction in total number of nodes processed when using $\OS$ splitting on the instances from \citep{belotti2012biobjective} but not the instances from \citep{boland2015acriterion}. We also note that the reported approximate CPU times for a parallel implementation of the $\OS$ splitting procedure indicate that even better results can be obtained once we are able to develop a parallel implementation.

\begin{figure}%[H]
\centering
\includegraphics[width=\textwidth]{exp1_prof}
\caption{Performance profile of CPU time for Experiment 6.}
\label{prof}
\end{figure}

\subsection{Approximations of the Pareto Set}

In \citet{boland2015acriterion}, the authors measure the time it takes the Triangle Splitting method to compute an approximate Pareto set having the property that the hypervolume gap between valid primal and dual bounds implied by this approximate set is less than 2\%. We repeat this experiment for our BB procedure, though we note that the primal and dual bounds we utilize are significantly different than those used in \citet{boland2015acriterion}. We measure this gap directly after the completion of our preprocessing procedure, and then each time 25 nodes are processed during BB. We cease the procedure if: (i) BB terminates with the true Pareto set, or (ii) the hypervolume gap is less than 2\%. In this experiment we also report Hausdorff gap measurements, as described in Section \ref{sec:gap}. Additionally, for comparison we include certain results as reported in \citet{boland2015acriterion}. The results of this experiment are displayed in Table \ref{table_appx}.

 \begin{table}[h!]
%\small
\centering
\caption{Experiment 7 -- Obtaining approximate Pareto sets.}\label{table_appx}
\begin{tabular}{l|r|rr|rrrrrr|rrr}
\toprule
 &  & \multicolumn{8}{c}{} & \multicolumn{1}{c}{Triangle}\\
 &  & \multicolumn{8}{c}{Standard BB} & \multicolumn{1}{c}{Splitting}\\
 \cmidrule(r){3-10}
\cmidrule(r){11-11}
  &  & \multicolumn{2}{c}{After} & \multicolumn{6}{c}{Until $\mathcal{HV}_{s^*}$} & \\
  &  & \multicolumn{2}{c}{Preprocessing} & \multicolumn{6}{c}{$\leq$ 2\%} & \\
\cmidrule(r){3-4}
\cmidrule(r){5-10}
 & Total &  & &  & \% Total &  & \% Total &  &  &  \% Total \\ 
Instance & Num & $\mathcal{HV}_0$ & $\overline{\mathcal{G}_{0}}$ & Time & Time & Nodes & Nodes & $\mathcal{HV}_{s^*}$ & $\overline{\mathcal{G}_{s^*}}$ &  Time \\ 
\midrule
Belotti60 & 30 & 21.1 & 61.7 & 4 & 80 & 49 & 94 & 0.1 & 3.3 & --\\
Belotti80 & 30 & 25.8 & 66.6 & 10 & 76 & 59 & 96 & 0.1 & 1.9 & --\\
\midrule
Boland80 & 1 & 1.5 & 12.3 & 2 & 25 & 0 & 0 & 1.5 & 12.3 & 12\\
         & 1 & 1.5 & 10.0 & 1 & 25 & 0 & 0 & 1.5 & 10.0 & 9\\
         & 1 & 2.0 & 18.2 & 3 & 33 & 25 & 8 & 1.5 & 10.8 & 4\\
         & 1 & 49.0 & 67.1 & 9 & 78 & 225 & 77 & 1.1 & 10.6 & 6\\
         & 1 & 1.4 & 14.9 & 1 & 20 & 0 & 0 & 1.4 & 14.9 & 7\\
\cmidrule(r){3-11}
         & Avg & \bf 11.0 & \bf 24.5 & \bf 3 & \bf 36 & \bf 50 & \bf 17 & \bf 1.4 & \bf 11.7 & \bf 7\\
\cmidrule(r){3-11}
Boland160 & 1 & 1.2 & 10.8 & 9 & 8 & 0 & 0 & 1.2 & 10.8 & 2.30\\
         & 1 & 1.0 & 9.4 & 13 & 8 & 0 & 0 & 1.0 & 9.4 & 3.85\\
         & 1 & 0.8 & 10.0 & 8 & 9 & 0 & 0 & 0.8 & 10.0 & 1.50\\
         & 1 & 0.6 & 7.9 & 16 & 5 & 0 & 0 & 0.6 & 7.9 & 0.61\\
         & 1 & 46.9 & 81.5 & 103 & 61 & 550 & 50 & 1.9 & 6.4 & 2.90\\
\cmidrule(r){3-11}
         & Avg & \bf 10.1 & \bf 23.9 & \bf 30 & \bf 18 & \bf 110 & \bf 10 & \bf 1.1 & \bf 8.3 & \bf 1\\
\cmidrule(r){3-11}
Boland320 & 1 & 0.7 & 4.5 & 83 & 2 & 0 & 0 & 0.7 & 4.5 & 0.21\\
         & 1 & 0.4 & 4.8 & 124 & 2 & 0 & 0 & 0.4 & 4.8 & 0.23\\
         & 1 & 0.4 & 5.5 & 123 & 3 & 0 & 0 & 0.4 & 5.5 & 0.26\\
         & 1 & 0.4 & 6.9 & 140 & 2 & 0 & 0 & 0.4 & 6.9 & 0.23\\
         & 1 & 0.4 & 7.4 & 125 & 3 & 0 & 0 & 0.4 & 7.4 & 0.22\\
\cmidrule(r){3-11}
         & Avg & \bf 0.4 & \bf 5.8 & \bf 119 & \bf 2 & \bf 0 & \bf 0 & \bf 0.4 & \bf 5.8 & \bf 0\\
\cmidrule(r){3-11}
Boland16 & 1 & 0.1 & 2.1 & 2 & 59 & 0 & 0 & 0.1 & 2.1 & --\\
         & 1 & 0.6 & 11.4 & 3 & 65 & 0 & 0 & 0.6 & 11.4 & --\\
         & 1 & 0.7 & 18.6 & 4 & 50 & 0 & 0 & 0.7 & 18.6 & --\\
         & 1 & 1.1 & 11.3 & 4 & 40 & 0 & 0 & 1.1 & 11.3 & --\\
\cmidrule(r){3-11}
         & Avg & \bf 0.6 & \bf 10.8 & \bf 3 & \bf 53 & \bf 0 & \bf 0 & \bf 0.6 & \bf 10.8 & --\\
\cmidrule(r){3-11}
Boland25 & 1 & 1.0 & 9.7 & 5 & 47 & 0 & 0 & 1.0 & 9.7 & --\\
         & 1 & 67.2 & 79.3 & 21 & 75 & 175 & 56 & 1.7 & 19.8 & --\\
         & 1 & 83.0 & 87.7 & 9 & 40 & 75 & 27 & 1.4 & 28.5 & --\\
         & 1 & 91.2 & 93.7 & 15 & 32 & 100 & 23 & 1.8 & 12.0 & --\\
\cmidrule(r){3-11}
         & Avg & \bf 60.6 & \bf 67.6 & \bf 13 & \bf 48 & \bf 87 & \bf 26 & \bf 1.5 & \bf 20.0 & --\\
\cmidrule(r){3-11}
Boland50  & 1 & 0.5 & 3.9 & 17 & 9 & 0 & 0 & 0.5 & 3.9 & --\\
         & 1 & 0.6 & 5.7 & 21 & 5 & 0 & 0 & 0.6 & 5.7 & --\\
         & 1 & 0.6 & 5.7 & 23 & 3 & 0 & 0 & 0.6 & 5.7 & --\\
         & 1 & 2.6 & 50.7 & 108 & 7 & 100 & 3 & 1.4 & 28.5 & --\\
\cmidrule(r){3-11}
         & Avg & \bf 1.0 & \bf 16.5 & \bf 42 & \bf 6 & \bf 25 & \bf 0 & \bf 0.8 & \bf 10.9 & --\\
\cmidrule(r){3-11}
\bottomrule
\end{tabular}
\end{table}

There are several things to notice from Table \ref{table_appx}. First, recognize that for the majority of the instances from \citet{boland2013criterion} and \citet{boland2015acriterion}, the hypervolume gap is already less than 2\% after preprocessing, before BB even begins. This is evidence that these instances are relatively easy. Recall Figure \ref{pareto_sets}, and notice that for the instance from \citet{boland2015acriterion} the boundary of the dual bound at the root node is very close to the Pareto set. This is further evidence of the ease of these instances. In contrast to this, notice from Table \ref{table_appx} that for the instances from \citep{belotti2012biobjective}, it takes over 75\% of the total BB time in order to obtain a hypervolume gap of less than 2\%. This indicates that in order to develop a robust solver for BOMILP there is a need for the development of a new set of BOMILP instances that are larger and more challenging than those studied in \citet{boland2015acriterion}. We discuss this topic further in the following section. Before we proceed to this section, though, we also point out that for instances in which the results of our BB can be compared with the triangle splitting method, Table \ref{table_appx} shows that the triangle splitting method is able to determine an approximate solution with a hypervolume gap of less than 2\% in less time, relative to the total solution time. 

\subsection{MIPLIB Instances}\label{sec:MIPLIB}

Due to the successful results we obtained using our BB procedure on instances from the literature, we designed our final set of tests to measure the performance of our procedure on a more realistic set of instances. For this we utilized a set of 39 single objective MILP instances available from the MIPLIB 2010 library \citep{koch2011miplib}. We chose only instances that were marked easy, were mixed-integer and not pure integer, and were relatively small in size (up to approximately 200 integer variables). For each instance, we generated six secondary objective functions according to the following rules:

\begin{enumerate}
\item[(o)] For each $i\in \{1,\dots,m+n\}$ the coefficient $c^2_i$ is randomly generated from the closed interval $\left[-|c^1_i|,|c^1_i|\right]$.
\item[(a)] We solved the LP relaxation associated with $f_1$ to obtain optimal solution $x^*$. Then for each nonbasic variable at this solution, we set $c^2_i = - c_i^1$ if: (i) $c^1_i >0$ and $x^*_i$ was not at its lower bound, or (ii) $c^1_i <0$ and $x^*_i$ was not at its upper bound. Otherwise we set $c^2_i = c_i^1$.
\item[(b)] We set $c^2_i = \dfrac{1}{c^1_i}$.
\item[(c)] Objective 2 is the sum of the continuous variables.
\item[(d)] Objective 2 is the sum of the integer variables, plus one continuous variable.
\item[(e)] We solved the LP relaxation associated with $f_1$ as well as the corresponding MILP. We then repeated strategy (a) for integer variables having the same value at the LP solution as at the MILP solution.
\end{enumerate}

After generation of these instances we did some preliminary testing and discarded instances for which: (i) the Pareto set was a singleton, or (ii) the second objective was unbounded, or (iii) the MILP associated with either $f_1$ or $f_2$ took over 12 hours to solve. We then opted to test the performance of the various preprocessing procedures we tested in Section \ref{preprocess_test}, each set to a maximum execution time of either 5, 10 or 30 minutes. We then calculated the duality gap percentages after exiting preprocessing. The results of this test are displayed in the performance profile found in Figure \ref{prof}. Here ``ev'' represents the implementation of \textsc{PreprocessingMethod1} in which $\rho$ is calculated as a function of instance size, ``w0'' indicates \textsc{PreprocessingMethod2} with $\rho$ set to zero, and ``hy0'' indicates the hybrid preprocessing procedure with $\rho$ set to zero. The numbers following each of these represent the limits on execution time. 

\begin{figure}%[H]
\centering
\includegraphics[width=\textwidth]{miplib_preprocessing_profile}
\caption{Performance profile of duality gap percentage for preprocessing procedures on instances from MIPLIB.}
\label{prof}
\end{figure}

Observe from Figure \ref{prof} that \textsc{PreprocessingMethod1} with an execution time of 30 minutes performed the best. Hence, we utilized this procedure for our final test. In this test we used our original BB procedure, the BB procedure in which we exploit $\OS$ gaps, and the triangle splitting method on each of the instances generated from MIPLIB. We set a maximum time limit of 12 hours for each procedure. Additionally, we set a time limit of 60 seconds for all single objective MILPs solved during the course of our BB procedures, except for: (i) solving the initial MILPs associated with $f_1$ and $f_2$, and (ii) solving the MILPs necessary to appropriately reduce subregions of $\OS$ when employing our $\OS$ splitting procedure. We also note that we do not keep track of a duality gap measurement during the course of BB because doing causes a significant reduction in performance, especially when there are a large number of open nodes. Instead, we calculate this gap after termination of BB, but we only allow an additional 12 hours for this task. The results of this experiment are provided in detail in Table \ref{table_results}. and summarized by the performance profile in Figure \ref{MIPLIB_perf_prof}. Note that when constructing this profile we only included data for instances which were solved by at least one of the three methods for solving BOMILP. Hence, the maximum height of each curve is bounded by the fraction of instances solved in under twelve hours by at least one of the solution procedures.

 \begin{table}[H]
%\tiny
\centering
%\footnotesize
\caption{Experiment 8 -- Comparing with the triangle splitting method for instance from MIPLIB.}\label{table_results}
\begin{tabular}{l|l|rrr|rrrr|r}
\toprule
 &  & \multicolumn{3}{c}{Original BB}  & \multicolumn{4}{c}{BB with Gap Splitting} %& \multicolumn{3}{c}{Solved by one - St. dev.} 
 & \multicolumn{1}{c}{ T.S.} 	\\ 
\cmidrule(r){3-5}
\cmidrule(r){6-9}
\cmidrule(r){10-10}
Instance &  & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & $\dagger$ & Nodes & $\overline{\mathcal{G}_{s^*}}$& Time \\
\midrule
aflow40b          & o   & --    & 901   & *  & 2,181  & 2,181  & 2    & --  & 28,552 \\
                  & a   & --    & 1,577 & *  & --     & --     & 1,658& --  & --\\
		          & d    & 2,230 & 1     & -- & 12,099 & 12,095 & 166  & --  & 7,234 \\
		          & e   & --    & 1,943  & *  & --     & --     & 1,259& *   & -- \\
beasleyC3         & o   & --    & --    & -- & 751    & 749    & 3    & --  & -- \\
                  & a   & --    & 888   & 14 & --     & --     & 9,076& 18  & -- \\
                  & b   & --    & 697   & 24 & --     & --     & 533  & 100 & -- \\
                  & e   & --    & 6,466 & *  & --     & --     &12,250& 13  & -- \\
%berlin\_5\_8\_0 & 1 & 0 & 0 & 0 & -- 	  & --  & -- 	& -- & -- \\
bienst2           & o & 1,596 & 35    & -- & 1,012  & 1,011 & 8     & --   & 2,174 \\
binkar10\_1       & o & --    & 4,586 & *  & --     & --    & 5,461 & *    & -- \\
                  & a & 3,229 & 921   & -- & 3,129  & 3,129 & 938   & --   & -- \\
                  & b & 7,634 & 4,812 & -- & 7,555  & 7,555 & 4,433 & --   & 5,949 \\
                  & c & 5,527 & 2,065 & -- & 8,587  & 3,238 & 8,587 & --   & -- \\
                  & d & 6,892 & 1,313 & -- &  363   & 362   & 2     & --   & 2,360 \\
                  & e & 4,684 & 286   & -- & 5,393  & 2,867 & 522   & --   & 25,293 \\
csched007         & o & --    & 564   & 31 & --     & --    & 596   & 31   & -- \\
                  & c & --    & 2,347 & 2  & --     & --    & 2,631 & 2    & -- \\
csched010         & o & --    & 488   & 35 & --     & --    & 395   & 33   & --\\
                  & a & --    & 515   & 16 & --     & --    & 683   & 16   & --\\
                  & c & --    & 1,044 & 2  & 15,205 & 15,205& 1     & --   & --\\
                  & d & --    & 589   & 100& 4,803  & 4,803 & 9     & --   & 32,811\\
danoint           & o & 42,299& 538   & -- & 7,091  & 7,086 & 6     & --   & 41,880\\
dfn-gwin-UUM      & o & --    & 859   & 67 & --     & --    & 1,953 & 100  & 25,434 \\
gmu-35-40         & o & --    & 1,527 & 68 & --     & --    & 5,759 & 69   & 25,714\\
                  & a & 2,072 & 1     & -- & --     & --    & 2,220 & 44   & --\\
                  & d & --    & 559   & 2  & --     & --    & 719   & 4    & 9,973\\
                  & e & 2,148 & 1     & -- & --     & --    & 2,715 & 44   & --\\
gmu-35-50         & o & 2,804 & 1     & -- & 10,218 & 7,715 & 1,489 & --   & --\\
                  & a & --    & 1,234 & 36 & --     & --    & 1,085 & 49   & --\\
                  & e & --    & 907   & 41 & --     & --    & --    & --   & --\\
ic97\_potential   & o & --    & 391   & 71 & --     & --    & 446   & 72   & --\\
                  & a & --    & --    & -- & --     & --    & 17,248& 1    & 23,911\\
                  & b & --    & 547   & 70 & --     & --    & 1,386 & 99   & --\\
                  & d & --    & 297   & 97 & --     & --    & 340   & 93   & --\\
k16x240           & o & --    & 17,648& 2  & --     & --    & 14,740& 2    & --\\
                  & a & --    & 42,960& 6  & --     & --    & 23,837& *    & --\\
                  & b & --    & 1,022 & 32 & --     & --    &187,846& 86   & --\\
                  & c & --    & 2,418 & *  & --     & --    & 2,479 & *    & --\\
                  & d & --    & 2,264 & *  & --     & --    & 3,542 & *    & --\\
                  & e & --    & 17,251& 8  & --     & --    & 19,401& --   & --\\
\bottomrule
\multicolumn{8}{l}{\scriptsize $\dagger$ -- Approximated parallel time}\\
\multicolumn{8}{l}{\scriptsize* -- duality gap calculation exceeded 12 hours} 
\end{tabular}
\end{table}

%\renewcommand{\thetable}{\arabic{table} (Continued)}  
%\addtocounter{table}{-1}

 \begin{table}[H]
%\tiny
\centering
%\footnotesize
\caption{(Continuation of Table \ref{table_results}.)}\label{table_results2}
\begin{tabular}{l|l|rrr|rrrr|r}
\toprule
 &  & \multicolumn{3}{c}{Original BB}  & \multicolumn{4}{c}{BB with Gap Splitting} %& \multicolumn{3}{c}{Solved by one - St. dev.} 
 & \multicolumn{1}{c}{ T.S.} 	\\ 
\cmidrule(r){3-5}
\cmidrule(r){6-9}
\cmidrule(r){10-10}
Instance &  & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & $\dagger$ & Nodes & $\overline{\mathcal{G}_{s^*}}$& Time \\
\midrule
%gmu-35-50         & o & 2,804 & 1     & -- & 10,218 & 7,715 & 1,489 & --   & --\\
%                  & a & --    & 1,234 & 36 & --     & --    & 1,085 & 49   & --\\
%                  & e & --    & 907   & 41 & --     & --    & --    & --   & --\\
%ic97\_potential   & o & --    & 391   & 71 & --     & --    & 446   & 72   & --\\
%                  & a & --    & --    & -- & --     & --    & 17,248& 1    & 23,911\\
%                  & b & --    & 547   & 70 & --     & --    & 1,386 & 99   & --\\
%                  & d & --    & 297   & 97 & --     & --    & 340   & 93   & --\\
%k16x240           & o & --    & 17,648& 2  & --     & --    & 14,740& 2    & --\\
%                  & a & --    & 42,960& 6  & --     & --    & 23,837& *    & --\\
%                  & b & --    & 1,022 & 32 & --     & --    &187,846& 86   & --\\
%                  & c & --    & 2,418 & *  & --     & --    & 2,479 & *    & --\\
%                  & d & --    & 2,264 & *  & --     & --    & 3,542 & *    & --\\
%                  & e & --    & 17,251& 8  & --     & --    & 19,401& --   & --\\
markshare\_5\_0   & o & --    & 3,905 & 20 & --     & --    & 4,173 & 13   & 32,476\\
                  & c & --    &117,984& 2  & --     & --    &166,057& 2    & 14,270\\
                  & d & --    & 3,937 & 15 & 5,002  & 5,002 & 5     & --   & --\\
mc11              & o & --    & 620   & 77 & --     & --    & 539   & *    & --\\
                  & a & --    & 950   & *  & --     & --    & 6,071 & 13   & --\\
                  & c & --    & 760   & 36 & --     & --    & 275   & *    & --\\
                  & e & --    & 2,041 & *  & --     & --    & 918   & *    & --\\
mcsched           & a & --    &14,578 & *  & --     & --    & 29,378& 20   & --\\
                  & c & --    &14,040 & 0  & --     & --    & 19,954& 3    & --\\
                  & d & --    & --    & -- & --     & --    & 9,588 & 0    & --\\
mik-250-1-100-1   & o & --    & 9,882 & 6  & 404    & 404   & 3     & --   & --\\
                  & a & --    & 17,814& 12 & --     & --    & 21,299& 64   & --\\
                  & c & --    & 14,170& 0  & --     & --    & 14,217& *    & 32,204\\
                  & d & --    & 11,972& *  & --     & --    & 8,889 & *    & --\\
                  & e & --    & 1,434 & 21 & --     & --    & 1,116 & 59   & --\\
neos-1112787      & a & 1,931 & 0     & -- & 1,891  & 1,890 & 3     & --   & -- \\
                  & c & --    & 1,881 & *  & --     & --    & 1,681 & *    & -- \\
neos-1171737      & o & 2,112 & 1     & -- & 2,210  & 2,210 & 1     & --   & --\\
                  & d & --    & 677   & 33 & --     & --    & 673   & 81   & --\\
neos-1225589      & b & --    & 410   & *  & --     & --    & 486   & *    & -- \\
                  & c & --    & 1,260 & *  & --     & --    & 1,291 & *    & -- \\
                  & d & --    & 16,214& *  & --     & --    & 44,389& --   & -- \\
neos13            & o & --    & 1,779 & 16 & --     & --    & 3,466 & 100  & 12,383\\ 
                  & a & --    & 488   & *  & --     & --    & 464   & *    & 25,263\\
                  & c & 496   & 10    & -- & 574    & 559   & 11    & --   & 1,054\\  
                  & d & --    & 12,625& 36 & --     & --    & 170   & 28   & --\\ 
neos-1396125      & a & 824   & 50    & -- & 825    & 825   & 534   & --   & 535\\
                  & b & 5,648 & 53    & -- & 3,713  & 3,590 & 28    & --   & 3,480\\
                  & d & --    & 613   & *  & 2,111  & 2,103 & 4     & --   & 22,695\\
neos-1426635      & d & --    & 2,761 & 30 & 1,203  & 1,203 & 2     & --   & 15,155\\
neos-1426662      & d & 3,689 & 1     & -- & --     & --    & 651   & 27   & --\\
neos-1440460      & d & --    & 999   & 35 & 1,903  & 1,903 & 1     & --   & --\\
neos-1442657      & d & --    & 576   & 15 & 13,097 & 13,097& 1     & --   & --\\
neos15            & a & --    & 15,811& 34 & --     & --    & 18,583& 34   & --\\
                  & b & --    & 2,911 & *  & --     & --    & 2,911 & 42   & --\\
neos-693347       & o & --    & 221   & *  & --     & --    & 2,670 & 56   & 11,480\\ 
                  & c & 674   & 1     & -- & 673    & 673   & 1     & --   & 566\\ 
                  & d & 722   & 1     & -- & 718    & 718   & 1     & --   & 566\\ 
neos-916792       & o & --    & 757   & 51 & --     & --    & 794   & 49   & 3,662\\
                  & a & 985   & 1     & -- & --     & --    & 5,257 & 0    & 426\\
                  & c & --    & 3,874 & 2  & --     & --    & 3,465 & 11   & 6,657\\
                  & d & --    & 672   & 70 & --     & --    & 666   & 79   & --\\
neos-942830       & o & --    & 1,655 & 31 & --     & --    & 5,606 & 64   & --\\
                  & a & 8,690 & 321   & -- & 9,952  & 9,952 & 271   & --   & 5,858\\
                  & c & 3,711 & 1     & -- & 3,980  & 3,980 & 1     & --   & 5,344\\
                  & d & 4,359 & 3     & -- & 3,057  & 3,057 & 3     & --   & --\\
noswot            & o & 7,491 & 30,903& -- & 736    & 732   & 3,141 & --   & 1,069\\ 
                  & a & --    & --    & -- & 754    & 753   & 10    & --   & --\\
                  & c & 32,355& 5,699 & -- & --     & --    &254,933& 54   & 42,748\\
                  & d & 1,942 & 1,092 & -- & 713    & 712   & 6     & --   & 878\\
ns1830653         & o & 23,349& 665   & -- & 19,053 & 19,053& 458   & --   & 25,302\\
                  & a & 19,242& 210   & -- & 18,110 & 18,110& 195   & --   & 10,363\\
p80x400b          & o & --    & 7,024 & 6  & --     & --    & 7,778 & 6    & -- \\
                  & a & --    & 16,439& 15 & --     & --    & 21,374& 17   & -- \\
                  & b & --    & 641   & 58 & --     & --    & 62,581& 70   & -- \\
                  & c & --    & 881   & *  & --     & --    & 2,205 & 0    & -- \\
                  & d & --    & 1,589 & *  & --     & --    & 2,505 & 9    & -- \\
                  & e & --    & 6,803 & 17 & --     & --    & 4,306 & 24   & -- \\
pigeon-10         & o & 1,940 & 562   & -- & 1,656  & 1,656 & 1     & --   & 1,670 \\
pigeon-11         & o & 20,436& 30    & -- & 20,266 & 20,266& 11    & --   & 20,672 \\
qiu               & o & 1,665 & 326   & -- & 1,445  & 1,445 & 280   & --   & 4,100\\
                  & a & 2,726 & 2,367 & -- & 1,911  & 873   & 686   & --   & 1,274\\
                  & b & 19,706& 2,969 & -- & 1,888  & 1,886 & 5     & --   & 5,853\\
                  & c & --    & 956   & 43 & 1,885  & 1,864 & 5     & --   & 4,146\\
                  & d & --    & 41,745& 3  & 1,815  & 1,815 & 3     & --   & --\\
                  & e & 4,809 & 541   & -- & 5,642  & 5,523 & 4,284 & --   & 834\\
\bottomrule
\multicolumn{8}{l}{\scriptsize $\dagger$ -- Approximated parallel time}\\
\multicolumn{8}{l}{\scriptsize* -- duality gap calculation exceeded 12 hours}  
\end{tabular}
\end{table}

 \begin{table}[H]
%\tiny
\centering
%\footnotesize
\caption{(Second continuation of Table \ref{table_results}.)}\label{table_results3}
\begin{tabular}{l|l|rrr|rrrr|r}
\toprule
 &  & \multicolumn{3}{c}{Original BB}  & \multicolumn{4}{c}{BB with Gap Splitting} %& \multicolumn{3}{c}{Solved by one - St. dev.} 
 & \multicolumn{1}{c}{ T.S.} 	\\ 
\cmidrule(r){3-5}
\cmidrule(r){6-9}
\cmidrule(r){10-10}
Instance &  & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & $\dagger$ & Nodes & $\overline{\mathcal{G}_{s^*}}$& Time \\
\midrule
%neos-1396125      & a & 824   & 50    & -- & 825    & 825   & 534   & --   & 535\\
%                  & b & 5,648 & 53    & -- & 3,713  & 3,590 & 28    & --   & 3,480\\
%                  & d & --    & 613   & *  & 2,111  & 2,103 & 4     & --   & 22,695\\
%neos-1426635      & d & --    & 2,761 & 30 & 1,203  & 1,203 & 2     & --   & 15,155\\
%neos-1426662      & d & 3,689 & 1     & -- & --     & --    & 651   & 27   & --\\
%neos-1440460      & d & --    & 999   & 35 & 1,903  & 1,903 & 1     & --   & --\\
%neos-1442657      & d & --    & 576   & 15 & 13,097 & 13,097& 1     & --   & --\\
%neos15            & a & --    & 15,811& 34 & --     & --    & 18,583& 34   & --\\
%                  & b & --    & 2,911 & *  & --     & --    & 2,911 & 42   & --\\
%neos-693347       & o & --    & 221   & *  & --     & --    & 2,670 & 56   & 11,480\\ 
%                  & c & 674   & 1     & -- & 673    & 673   & 1     & --   & 566\\ 
%                  & d & 722   & 1     & -- & 718    & 718   & 1     & --   & 566\\ 
%neos-916792       & o & --    & 757   & 51 & --     & --    & 794   & 49   & 3,662\\
%                  & a & 985   & 1     & -- & --     & --    & 5,257 & 0    & 426\\
%                  & c & --    & 3,874 & 2  & --     & --    & 3,465 & 11   & 6,657\\
%                  & d & --    & 672   & 70 & --     & --    & 666   & 79   & --\\
%neos-942830       & o & --    & 1,655 & 31 & --     & --    & 5,606 & 64   & --\\
%                  & a & 8,690 & 321   & -- & 9,952  & 9,952 & 271   & --   & 5,858\\
%                  & c & 3,711 & 1     & -- & 3,980  & 3,980 & 1     & --   & 5,344\\
%                  & d & 4,359 & 3     & -- & 3,057  & 3,057 & 3     & --   & --\\
%noswot            & o & 7,491 & 30,903& -- & 736    & 732   & 3,141 & --   & 1,069\\ 
%                  & a & --    & --    & -- & 754    & 753   & 10    & --   & --\\
%                  & c & 32,355& 5,699 & -- & --     & --    &254,933& 54   & 42,748\\
%                  & d & 1,942 & 1,092 & -- & 713    & 712   & 6     & --   & 878\\
%ns1830653         & o & 23,349& 665   & -- & 19,053 & 19,053& 458   & --   & 25,302\\
%                  & a & 19,242& 210   & -- & 18,110 & 18,110& 195   & --   & 10,363\\
%p80x400b          & o & --    & 7,024 & 6  & --     & --    & 7,778 & 6    & -- \\
%                  & a & --    & 16,439& 15 & --     & --    & 21,374& 17   & -- \\
%                  & b & --    & 641   & 58 & --     & --    & 62,581& 70   & -- \\
%                  & c & --    & 881   & *  & --     & --    & 2,205 & 0    & -- \\
%                  & d & --    & 1,589 & *  & --     & --    & 2,505 & 9    & -- \\
%                  & e & --    & 6,803 & 17 & --     & --    & 4,306 & 24   & -- \\
%pigeon-10         & o & 1,940 & 562   & -- & 1,656  & 1,656 & 1     & --   & 1,670 \\
%pigeon-11         & o & 20,436& 30    & -- & 20,266 & 20,266& 11    & --   & 20,672 \\
%qiu               & o & 1,665 & 326   & -- & 1,445  & 1,445 & 280   & --   & 4,100\\
%                  & a & 2,726 & 2,367 & -- & 1,911  & 873   & 686   & --   & 1,274\\
%                  & b & 19,706& 2,969 & -- & 1,888  & 1,886 & 5     & --   & 5,853\\
%                  & c & --    & 956   & 43 & 1,885  & 1,864 & 5     & --   & 4,146\\
%                  & d & --    & 41,745& 3  & 1,815  & 1,815 & 3     & --   & --\\
%                  & e & 4,809 & 541   & -- & 5,642  & 5,523 & 4,284 & --   & 834\\
ran14x18          & o & --    & 2,610 & 8  & --     & --    & 4,424 & 8    & -- \\
                  & a & --    & 40,881& 11 & --     & --    & 56,981& 12   & -- \\
                  & b & --    & 875   & 33 & 3,074  & 3,073 & 4     & --   & -- \\
                  & e & --    & 46,951& 14 & --     & --    & 40,132& 12   & 38,624 \\
ran14x18-disj-8   & o & --    & 6,826 & 10 & --     & --    & 9,769 & 8    & -- \\
                  & a & --    & 45,528& 27 & --     & --    & 68,137& 28   & 26,029 \\
                  & b & --    & 922   & 30 & --     & --    & 648   & 1    & -- \\
                  & e & --    & 10,727& 9  & --     & --    & 15,977& 11   & -- \\
ran16x16          & o & --    & 12,180& 6  & --     & --    & 12,175& 6    & -- \\
                  & a & --    & 23,189& *  & --     & --    & 25,738& *    & -- \\
                  & e & --    & 19,916& 11 & --     & --    & 9,477 & *    & -- \\
timtab1           & o & --    & 1,136 & 50 & --     & --    & 1,167 & 100  & -- \\
                  & a & --    & 513   & 100& --     & --    & 535   & 100  & -- \\
                  & b & --    & 575   & 100& --     & --    & 477   & 100  & -- \\
                  & d & --    & 789   & 63 & --     & --    & 798   & 48   & -- \\
%ran14x18          & o & --    & 2,610 & 8  & --     & --    & 4,424 & 8    & -- \\
%                  & a & --    & 40,881& 11 & --     & --    & 56,981& 12   & -- \\
%                  & b & --    & 875   & 33 & 3,074  & 3,073 & 4     & --   & -- \\
%                  & e & --    & 46,951& 14 & --     & --    & 40,132& 12   & 38,624 \\
%ran14x18-disj-8   & o & --    & 6,826 & 10 & --     & --    & 9,769 & 8    & -- \\
%                  & a & --    & 45,528& 27 & --     & --    & 68,137& 28   & 26,029 \\
%                  & b & --    & 922   & 30 & --     & --    & 648   & 1    & -- \\
%                  & e & --    & 10,727& 9  & --     & --    & 15,977& 11   & -- \\
%ran16x16          & o & --    & 12,180& 6  & --     & --    & 12,175& 6    & -- \\
%                  & a & --    & 23,189& *  & --     & --    & 25,738& *    & -- \\
%                  & e & --    & 19,916& 11 & --     & --    & 9,477 & *    & -- \\
%timtab1           & o & --    & 1,136 & 50 & --     & --    & 1,167 & 100  & -- \\
%                  & a & --    & 513   & 100& --     & --    & 535   & 100  & -- \\
%                  & b & --    & 575   & 100& --     & --    & 477   & 100  & -- \\
%                  & d & --    & 789   & 63 & --     & --    & 798   & 48   & -- \\
\bottomrule
\multicolumn{8}{l}{\scriptsize $\dagger$ -- Approximated parallel time}\\
\multicolumn{8}{l}{\scriptsize* -- duality gap calculation exceeded 12 hours}  
\end{tabular}
\end{table}

% \begin{table}[H]
%%\tiny
%\centering
%%\footnotesize
%\caption{(Third continuation of Table \ref{table_results}.)}\label{table_results4}
%\begin{tabular}{l|l|rrr|rrrr|r}
%\toprule
% &  & \multicolumn{3}{c}{Original BB}  & \multicolumn{4}{c}{BB with Gap Splitting} %& \multicolumn{3}{c}{Solved by one - St. dev.} 
% & \multicolumn{1}{c}{ T.S.} 	\\ 
%\cmidrule(r){3-5}
%\cmidrule(r){6-9}
%\cmidrule(r){10-10}
%Instance &  & Time & Nodes & $\overline{\mathcal{G}_{s^*}}$ & Time & $\dagger$ & Nodes & $\overline{\mathcal{G}_{s^*}}$& Time \\
%\midrule
%%neos-1396125      & a & 824   & 50    & -- & 825    & 825   & 534   & --   & 535\\
%%                  & b & 5,648 & 53    & -- & 3,713  & 3,590 & 28    & --   & 3,480\\
%%                  & d & --    & 613   & *  & 2,111  & 2,103 & 4     & --   & 22,695\\
%%neos-1426635      & d & --    & 2,761 & 30 & 1,203  & 1,203 & 2     & --   & 15,155\\
%%neos-1426662      & d & 3,689 & 1     & -- & --     & --    & 651   & 27   & --\\
%%neos-1440460      & d & --    & 999   & 35 & 1,903  & 1,903 & 1     & --   & --\\
%%neos-1442657      & d & --    & 576   & 15 & 13,097 & 13,097& 1     & --   & --\\
%%neos15            & a & --    & 15,811& 34 & --     & --    & 18,583& 34   & --\\
%%                  & b & --    & 2,911 & *  & --     & --    & 2,911 & 42   & --\\
%%neos-693347       & o & --    & --    & -- & --     & --    & 2,670 & 56   & --\\ 
%%                  & c & 674   & 1     & -- & 673    & 673   & 1     & --   & 566\\ 
%%                  & d & 722   & 1     & -- & 718    & 718   & 1     & --   & 566\\ 
%%neos-916792       & o & --    & 757   & 51 & --     & --    & 794   & 49   & 3,662\\
%%                  & a & 985   & 1     & -- & --     & --    & 5,257 & 0    & 426\\
%%                  & c & --    & 3,874 & 2  & --     & --    & 3,465 & 11   & 6,657\\
%%                  & d & --    & 672   & 70 & --     & --    & 666   & 79   & --\\
%%neos-942830       & o & --    & 1,655 & 31 & --     & --    & 5,606 & 64   & --\\
%%                  & a & 8,690 & 321   & -- & 9,952  & 9,952 & 271   & --   & 5,858\\
%%                  & c & 3,711 & 1     & -- & 3,980  & 3,980 & 1     & --   & 5,344\\
%%                  & d & 4,359 & 3     & -- & 3,057  & 3,057 & 3     & --   & --\\
%%noswot            & o & 7,491 & 30,903& -- & 736    & 732   & 3,141 & --   & 1,069\\ 
%%                  & a & --    & --    & -- & 754    & 753   & 10    & --   & --\\
%%                  & c & 32,355& 5,699 & -- & --     & --    &254,933& 54   & 42,748\\
%%                  & d & 1,942 & 1,092 & -- & 713    & 712   & 6     & --   & 878\\
%%ns1830653         & o & 23,349& 665   & -- & 19,053 & 19,053& 458   & --   & 25,302\\
%%                  & a & 19,242& 210   & -- & 18,110 & 18,110& 195   & --   & 10,363\\
%%p80x400b          & o & --    & 7,024 & 6  & --     & --    & 7,778 & 6    & -- \\
%%                  & a & --    & 16,439& 15 & --     & --    & 21,374& 17   & -- \\
%%                  & b & --    & 641   & 58 & --     & --    & 62,581& 70   & -- \\
%%                  & c & --    & 881   & *  & --     & --    & 2,205 & 0    & -- \\
%%                  & d & --    & 1,589 & *  & --     & --    & 2,505 & 9    & -- \\
%%                  & e & --    & 6,803 & 17 & --     & --    & 4,306 & 24   & -- \\
%%pigeon-10         & o & 1,940 & 562   & -- & 1,656  & 1,656 & 1     & --   & 1,670 \\
%%pigeon-11         & o & 20,436& 30    & -- & 20,266 & 20,266& 11    & --   & 20,672 \\
%%qiu               & o & 1,665 & 326   & -- & 1,445  & 1,445 & 280   & --   & 4,100\\
%%                  & a & 2,726 & 2,367 & -- & 1,911  & 873   & 686   & --   & 1,274\\
%%                  & b & 19,706& 2,969 & -- & 1,888  & 1,886 & 5     & --   & 5,853\\
%%                  & c & --    & 956   & 43 & 1,885  & 1,864 & 5     & --   & 4,146\\
%%                  & d & --    & 41,745& 3  & 1,815  & 1,815 & 3     & --   & --\\
%%                  & e & 4,809 & 541   & -- & 5,642  & 5,523 & 4,284 & --   & 834\\
%%ran14x18          & o & --    & 2,610 & 8  & --     & --    & 4,424 & 8    & -- \\
%%                  & a & --    & 40,881& 11 & --     & --    & 56,981& 12   & -- \\
%%                  & b & --    & 875   & 33 & 3,074  & 3,073 & 4     & --   & -- \\
%%                  & e & --    & 46,951& 14 & --     & --    & 40,132& 12   & 38,624 \\
%%ran14x18-disj-8   & o & --    & 6,826 & 10 & --     & --    & 9,769 & 8    & -- \\
%%                  & a & --    & 45,528& 27 & --     & --    & 68,137& 28   & 26,029 \\
%%                  & b & --    & 922   & 30 & --     & --    & 648   & 1    & -- \\
%%                  & e & --    & 10,727& 9  & --     & --    & 15,977& 11   & -- \\
%%ran16x16          & o & --    & 12,180& 6  & --     & --    & 12,175& 6    & -- \\
%%                  & a & --    & 23,189& *  & --     & --    & 25,738& *    & -- \\
%%                  & e & --    & 19,916& 11 & --     & --    & 9,477 & *    & -- \\
%%timtab1           & o & --    & 1,136 & 50 & --     & --    & 1,167 & 100  & -- \\
%%                  & a & --    & 513   & 100& --     & --    & 535   & 100  & -- \\
%%                  & b & --    & 575   & 100& --     & --    & 477   & 100  & -- \\
%%                  & d & --    & 789   & 63 & --     & --    & 798   & 48   & -- \\
%\bottomrule
%\multicolumn{8}{l}{\scriptsize $\dagger$ -- Approximated parallel time}\\
%\multicolumn{8}{l}{\scriptsize* -- duality gap calculation exceeded 12 hours}  
%\end{tabular}
%\end{table}

%\renewcommand{\thetable}{\arabic{table}}  

There are a couple of important pieces of information to recognize from Tables \ref{table_results}--\ref{table_results3}. First, notice that of the 115 instance considered, 34 were solved in under 12 hours by the original BB implementation, 43 by the $\OS$ splitting BB variant, and 40 by the triangle splitting method. Additionally, there were 17 instances which were solved in under 12 hours by one version of BB, but not by the triangle splitting method, and 10 instances solved in under 12 hours by the triangle splitting method, but not by a BB procedure. In all, the results display comparable performance between the BB approaches and the triangle splitting method. We also point out that there are a small number of instances for which one of the BB procedures terminated after processing a very small number of nodes. There are two situations in which this occurred: (i) when all Pareto solutions on a BOMILP instance lie on a single line segment in $\OS$, and (ii) when there are an extremely low number of Pareto points or line segments. The former case seems to happen far less frequently than the latter, but it should be noted that in this case numerical issues can cause BB to terminate before all Pareto solutions are found if a cutting plane is generated which lies on the same segment in $\OS$ on which all Pareto solutions lie.

In order to test the impact of the 60 second time limit placed on solving single objective MILPs, we repeated this experiment an additional two times, once with this time limit changed to 30 seconds, and once with it changed to 300 seconds. The results are summarized by the performance profile in Figure \ref{MIPLIB_perf_prof}. Note that when constructing this profile we only included data for instances which were solved by at least one of the utilized methods for solving BOMILP. Hence, the maximum height of each curve is bounded by the fraction of instances solved in under twelve hours by at least one of the solution procedures.
\begin{figure}[h!]
\centering
\includegraphics[width=0.97\textwidth]{miplib_profile3}
\caption{Performance profile of relative CPU time for MIPLIB instances.}
\label{MIPLIB_perf_prof}
\end{figure}
As with the results in Tables \ref{table_results}--\ref{table_results3}, this profile also displays comparable performance between the BB approaches and the triangle splitting method. On average our BB implementation in which objective space gaps are exploited and single objective MILPs are processed for a maximum of 300 seconds performs the best, while, interestingly, our standard BB with a MILP processing time of 300 seconds seems to perform the worst. 

\section{Concluding Remarks} \label{sec:conclude}
In this paper, we have introduced a new \bb{} method for solving BOMILP with general integers. For each component of single objective \bb{}, we presented procedure(s) for extending this component to the  biobjective setting. We have also conducted numerous computational experiments. The first several experiments provide insight into the usefulness of each of the algorithms we proposed. The final few experiments compare the performance of our BB procedure and the triangle splitting method \citep{boland2015acriterion}. Our BB procedure outperforms the triangle splitting method on  instances from literature, and performs comparably on large, challenging instances that were developed in this paper. 

Most of the algorithms proposed by us have, in theory, straightforward generalizations to the multiobjective case (MOMILPs). However, having an implementable correct \bb{} for MOMILPs is far from a trivial extension of this work. We point out some important questions that need to be answered in this regard.

%\paragraph{Generation of strong cuts}% in the objective space}
%The local cuts proposed in Section \ref{sec:tight} were shown to have minimal computational impact in Table \ref{table_cuts1}. 
%
%Obtaining good cuts for BOMILP is extremely challenging for at least two reasons: (1) Local cuts generate valid cuts only in a localized area of the feasible set, (2) Unlike MILP, the solutions to MOMILP do not lie on the boundary of the convex hull of the feasible region. 
%
%Cutting planes are known to have 
%
%The main goal here is to exploit the structure of Pareto-optimal solutions to obtain strong cuts that eliminate only those fractional vertices that cannot be Pareto-optimal. The \bb{} of *** used only local cuts provided by the MILP solver since obtaining good cuts for MOMILP is extremely challenging for at least two reasons: (1) Current cut generation techniques generate valid cuts in a localized area of the feasible set, (2) Unlike MILP, the solutions to MOMILP do not lie on the boundary of the convex hull of the feasible region. Consider (1) first. In MOMILPs, Pareto solutions may exist through a large subset of the feasible region and thus generating valid cuts in a localized region of the feasible set is only helpful for revealing Pareto solutions within a particular region of the objective space. To counter this, one can use standard cut generation procedures using a weighted-sum scalarization of the multiobjective problem, for several values of the weights. This allows for the generation of valid cuts which aid in revealing supported Pareto solutions throughout the objective space. However there are implementation challenges related to number of times to generate cuts, weight selection, removing redundant cuts from different weights between rounds of cuts, storage of cuts and making the MILP solver aware of cuts generated with respect to a different objective. Now consider (2). It is clear that only \emph{supported} Pareto solutions lie on the boundary of the convex hull of the feasible region. Hence, globally valid cutting planes are not sufficiently helpful in obtaining \emph{unsupported} Pareto solutions. This is particularly important for instances in which there are far more unsupported Pareto solutions than supported ones, and when there exist unsupported solutions which are far away (in terms of Euclidean distance) from the boundary of the convex hull. There are a couple of ways in which this can be dealt with, though with each comes a set of challenges: generating locally valid cuts at each node of the \bb{} tree; breaking the objective space into small subregions and carrying out \bb{} with cutting planes in each region individually. Both of these approaches will have to be computationally tested to ensure that they do not become prohibitively expensive and slow down the \bb{}. 
%%Using this approach can reduce the impact of many of the difficulties we have mentioned, and it also leads to an easily implemented parallel procedure. However, the main difficulty with this approach is determining the size, number, and orientation of the objective space subregions.
%Finally, we will also theoretically investigate the map between the feasible region and the Pareto points in the objective space to see if this map provides information for cutting off a fractional point in the $x$-space that is verified to be non-Pareto.
%
\paragraph{Extension to multiobjective MILP}
Correct node fathoming is what makes a \bb{} algorithm a correct and exact method. Fathoming by bound dominance is how fathoming mostly occurs in \bb{}. For BOMILP, the bound sets are two-dimensional polyhedra. This greatly simplifies checking bound dominance for BOMILPs since given two line segments, or piecewise linear curves in general, in $\R^{2}$, one can easily identify the dominated portion through pairwise comparisons. The data structure \citep{treestructure} of the authors stores nondomimated line segments and efficiently checks if a new line segment is dominated by what is currently stored. This enabled the node processing step in this paper to perform fathoming efficiently. Bound sets for MOMILP are higher-dimensional polyhedra and hence one will require an even more sophisticated data structure to store these sets. Since the local dual bound set at each node is a polyhedron and the global primal bound is a finite union of polyhedra, checking dominance requires checking containment of polyhedra, whose complexity depends on their respective representations (see \citep{freund1985complexity,gritzmann1994complexity}), and also computing the set difference between the primal and dual bound sets. The set resulting from this set difference would be nonconvex, in general, which begs the question: is there a straightforward way to represent this nonconvex set as a union of polyhedra whose relative interiors are disjoint?  All in all, fathoming and storing nondominated regions for a MOMILP is even more nontrivial. Once these obstacles are overcome, the \bb{} proposed in this paper should extend to a implementable \bb{} for MOMILPs.

%In terms of  storage, we will choose to store the facets that generate the bound set. Thus, in summary, this task hinges on developing a sophisticated data structure, much more advanced than what \citep{treestructure} achieved for the biobjective case, so that fathoming can be carried out correctly and efficiently.

%\section*{Acknowledgements}
%Part of this research was carried out under support from ONR grant N00014-16-1-2725.


\bibliographystyle{abbrvnat}%{spmpscinat}
\bibliography{biobjBB.bib}

%\newrefcontext[sorting=nyt]
%\printbibliography

\appendix
\section{Proofs for Dual Presolving}\label{app:proofs}
\begin{proof}[Proof of Proposition~\ref{duality_fixing}] 
It is well known (see Theorem 4.5 of \citep{ehrgott2005multicriteria}) that $x^*$ is efficient for the given BOMILP if and only if there exists $\epsilon$ such that $x^*$ is optimal to the problem: 
\begin{equation}\label{prop1_eqn}
\min_{x} \{f_1(x)\colon x \in X_I,\, f_k(x) \leq \epsilon_k \text{ for all } k \neq 1\}
\end{equation}
Hence, every efficient solution to the given BOMILP can be obtained by solving \eqref{prop1_eqn} for some $\epsilon$. If the stated assumptions hold, then single objective duality fixing can be applied to \eqref{prop1_eqn}. This shows that every efficient solution to the given BOMILP can be obtained by solving the modified version of \eqref{prop1_eqn} in which variable fixing has been performed. \qed
\end{proof}

\begin{proof}[Proof of Proposition~\ref{singleton_columns}]
\renewcommand{\ell}{l}
Let $x$ be an efficient solution with $x_s  < u_s$. If $x_j = \ell_j$ for all $j\in J (r )\setminus\{s\}$,
then a new solution $x'$ constructed from $x$ by setting $x_s'$ to $u_s$ is feasible because
\[\su[j]{}a_{rj}x_j'=\su[j\neq s]{}a_{rj}x_j' + a_{rs}u_s \leq U_r + a_{rs}(u_s-\ell_s) \leq b_r.\]
Additionally, the value of every objective function improves because $c_s^k < 0$ for all $k$. This contradicts
our assumption of $x$ being efficient. Hence, there exists a $j \in J (r )\setminus\{s\}$ with $x_j > \ell_j$. In this case we can construct a new solution ${x}^*$ from $x$ by decreasing the value of $x_j$
to $x_j'$ while at the same time increasing the value of $x_s$ so that $A_{r \bul} {x}^* = A_{r\bul} x$. In particular, $a_{r s} ({x}_s^* - x_s) = a_{r j} (x_j - {x}_j^* )$ holds. The change of objective $k$ can be estimated by
\begin{align*}
c_s^k {x}_s^* + c^k_j {x}_j^* &= c^k_s x_s + c^k_j x_j + c^k_s ({x}_s^* - x_s) - c^k_j (x_j - {x}_j^* )\\
&= c^k_s x_s + c^k_j x_j + c^k_s \frac{a_{rs}}{a_{rs}} ({x}_s^* - x_s) - c^k_j \frac{a_{rj}}{a_{rj}} (x_j - {x}_j^* )\\
&\leq c^k_s x_s + c^k_j x_j + c^k_s \frac{a_{rs}}{a_{rs}} ({x}_s^* - x_s) - c^k_s \frac{a_{rj}}{a_{rs}} (x_j - {x}_j^* )\\
&= c^k_s x_s + c^k_j x_j + \frac{ c^k_s}{a_{rs}}\left( a_{rs}({x}_s^* - x_s) - a_{rj} (x_j - {x}_j^* )\right)\\
&= c^k_s x_s + c^k_j x_j.\\
\end{align*}
If ${x}_s^* = u_s$, the result of the proposition holds. Otherwise, ${x}_j^* = \ell_j$ holds. Applying this argument iteratively results in an optimal solution with ${x}_s^* = u_s$ or ${x}_j^* = j$ for all $j \in J (r )\setminus\{s\}$. But as shown before, the latter case contradicts the efficiency of $x^*$.\qed
\end{proof}

\begin{lemma}\label{lemma1}
Let $x$ be a feasible solution for an instance of BOMILP and $x_j \dom x_i$. Given $0 < \alpha \in \R$, we define $x^*$ so that 
\[x^*_\gamma = \left\{\begin{array}{cc}
x_\gamma + \alpha & \gamma = i\\
x_\gamma - \alpha & \gamma = j\\
x_\gamma & \text{otherwise.}
\end{array} \right. \] 
If $x^*_j = x_j + \alpha \leq u_j$ and $x^*_i = x_i - \alpha \geq \ell_i$, then $x^*$ is feasible and $f_k( x^*) \leq f_k(x)$ for all $k$.
\end{lemma}

\begin{proof}[Proof of Proposition~\ref{dominating_col_disjunction}] 
Let $x\in X_{E}$ with $x_j < u_j$ and $x_i > l_i$. We construct a feasible solution $x^*$ by defining $\alpha = \min\{ x_i - l_i, u_j - x_j \}$ and applying Lemma~\ref{lemma1}. Since $x$ is efficient and $f_k(x^*) \leq f_k(x)$ for all $k$, $x^*$ is also efficient. By definition of $\alpha$, we also have $x^*_j = u_j$ or $x_i^* = l_i$.\qed
\end{proof}

\end{document}
